{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from torch import nn;\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "MIN_WORD_FREQUENCY = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "\n",
    "def read_lines(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads all the lines form all the texts in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scan for all input files\n",
    "    inDirectoryName = os.path.join(DATA_DIR, 'input', dataset)\n",
    "    inFileNames = [os.path.join(inDirectoryName, f) for f in os.listdir(inDirectoryName)]\n",
    "\n",
    "    # Read all the lines from all the files\n",
    "    lines = []\n",
    "    for inFileName in inFileNames:\n",
    "        with open(inFileName, 'r') as file:\n",
    "            lines += file.readlines()\n",
    "\n",
    "    print(f\"Read {len(lines)} lines from {dataset}\")\n",
    "    return lines\n",
    "\n",
    "def create_tokens(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Creates tokens for all the words in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'words.{dataset}.pt')\n",
    "    \n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded tokenized words for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    tokens = []\n",
    "    for line in read_lines(dataset):\n",
    "        tokens += TOKENIZER(line)\n",
    "\n",
    "    # Save tokens so we dont have to do this again\n",
    "    torch.save(tokens, outFileName)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_vocabulary(\n",
    "    dataset: str\n",
    ") -> Vocab:\n",
    "    \"\"\"\n",
    "    Creates a vocabulary for the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'vocabulary.pt')\n",
    "\n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded vocabulary for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    def read_sanitize_tokenize():\n",
    "\n",
    "        for line in read_lines(dataset):\n",
    "\n",
    "            line = re.sub('\\\\w*[0-9]+\\\\w*', ' ', line) # Remove numbers\n",
    "            line = re.sub('\\\\w*[A-Z]+\\\\w*', ' ', line) # Remove uppercase names\n",
    "            line = re.sub('\\\\s+', ' ', line) # Remove double spaces\n",
    "\n",
    "            yield TOKENIZER(line)\n",
    "\n",
    "    vocabulary = build_vocab_from_iterator(read_sanitize_tokenize(), min_freq=MIN_WORD_FREQUENCY, specials=['<unk>'])\n",
    "\n",
    "    vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "    # We removed all uppercase names, this includes 'I'\n",
    "    vocabulary.append_token('i') \n",
    "\n",
    "    # Save vocabulary so we dont have to do this again\n",
    "    torch.save(vocabulary, outFileName)\n",
    "\n",
    "    return vocabulary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized words for train (./data/words.train.pt)\n",
      "Loaded tokenized words for val (./data/words.val.pt)\n",
      "Loaded tokenized words for test (./data/words.test.pt)\n",
      "Loaded vocabulary for train (./data/vocabulary.pt)\n"
     ]
    }
   ],
   "source": [
    "words_train = create_tokens('train')\n",
    "words_val = create_tokens('val')\n",
    "words_test = create_tokens('test')\n",
    "\n",
    "vocabulary = create_vocabulary('train')\n",
    "VOCABULARY_SIZE = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in 'train' dataset ........: 2684706\n",
      "Words in 'val' dataset ..........: 49526\n",
      "Words in 'test' dataset .........: 124152\n",
      "Distinct words in 'train' dataset: 52105\n",
      "Words in vocabulary .............: 1880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Words in 'train' dataset ........:\", len(words_train))\n",
    "print(\"Words in 'val' dataset ..........:\", len(words_val))\n",
    "print(\"Words in 'test' dataset .........:\", len(words_test))\n",
    "print(\"Distinct words in 'train' dataset:\", len(set(words_train)))\n",
    "print(\"Words in vocabulary .............:\", VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIM = 32\n",
    "CONTEXT_SIZE = 5\n",
    "EMBEDDINGS_BATCH_SIZE = 128\n",
    "EMBEDDINGS_EPOCHS = 100\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(VOCABULARY_SIZE, EMBEDDINGS_DIM, sparse=True)\n",
    "        self.linear = nn.Linear(EMBEDDINGS_DIM*CONTEXT_SIZE, VOCABULARY_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "def cbow_create_dataset(\n",
    "    words: list[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "    words_idx = [vocabulary[word] for word in words]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(len(words) - CONTEXT_SIZE):\n",
    "        context = words_idx[i:i+CONTEXT_SIZE]\n",
    "        target = words_idx[i+CONTEXT_SIZE]\n",
    "\n",
    "        contexts.append(torch.tensor(context))\n",
    "        targets.append(target)\n",
    "\n",
    "    contexts = torch.stack(contexts).to(device)\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "def model_nameof(\n",
    "    model: nn.Module, \n",
    "    criterion: object, \n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Creates a good name for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    name = f'{model.__class__.__name__}_{criterion.__class__.__name__}_{optimizer.__class__.__name__}'\n",
    "    options = optimizer.param_groups[0]\n",
    "\n",
    "    if 'lr' in options:\n",
    "        name += f'-lr{options[\"lr\"]:.3f}'\n",
    "\n",
    "    if 'momentum' in options and options['momentum'] != 0.0:\n",
    "        name += f'-m{options[\"momentum\"]:.3f}'\n",
    "\n",
    "    if 'weight_decay' in options and options['weight_decay'] != 0.0:\n",
    "        name += f'-wd{options[\"weight_decay\"]:.3f}'\n",
    "\n",
    "    return name\n",
    "\n",
    "def model_save(model: nn.Module, folder: str | None = None):\n",
    "    \"\"\"\n",
    "    Save the given model to a file.\n",
    "    \"\"\"\n",
    "\n",
    "    folder = '' if folder is None else folder + '/'\n",
    "    filename = DATA_DIR + f'{folder}{model.name}.pt'\n",
    "\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f'Saved {model.name} ({filename})')\n",
    "\n",
    "def model_load(model: nn.Module, folder: str | None = None) -> bool:\n",
    "    \"\"\"\n",
    "    Save the given model to a file.\n",
    "\n",
    "    Returns `True` if the model was loaded, `False` otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    folder = '' if folder is None else folder + '/'\n",
    "    filename = DATA_DIR + f'{folder}{model.name}.pt'\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        return False\n",
    "    \n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    print(f'Loaded {model.name} ({filename}')\n",
    "    return True\n",
    "    \n",
    "def cbow_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    force_retrain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the given model on the given dataset.\n",
    "\n",
    "    Returns the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    model.name = model_nameof(model, criterion, optimizer)\n",
    "\n",
    "    if model_load(model, 'embeddings') and force_retrain is False:\n",
    "        return\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=EMBEDDINGS_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f'Training {model.name}...')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(EMBEDDINGS_EPOCHS):\n",
    "\n",
    "        total_loss = torch.tensor([0.0]).to(device)\n",
    "        total_size = 0\n",
    "\n",
    "        for contexts, targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts)\n",
    "            targets = torch.nn.functional.one_hot(targets, num_classes=VOCABULARY_SIZE).float().to(device)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_size += len(targets)\n",
    "\n",
    "        total_loss = total_loss.item() / total_size\n",
    "        losses.append(total_loss)\n",
    "        print(f'Training | {model.name} | Epoch {epoch} | Loss {total_loss}')\n",
    "\n",
    "    model_save(model, 'embeddings')\n",
    "\n",
    "def cbow_eval(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the given model on the given dataset.\n",
    "\n",
    "    Returns the accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=EMBEDDINGS_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for contexts, targets in data_loader:\n",
    "        outputs = model(contexts)\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (outputs == targets).sum().item()\n",
    "\n",
    "    print(f'Validation | {model.name} | Accuracy {correct/total:.4f}')\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def cbow_eval_and_pick_best(\n",
    "    dataset: TensorDataset,\n",
    "    models: list[nn.Module]\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models and pick the best one.\n",
    "    \"\"\"\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for model in models:\n",
    "        accuracy = cbow_eval(dataset, model)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "\n",
    "    print(f'Best model: {best_model.name} | Accuracy {best_accuracy}')\n",
    "    return best_model\n",
    "\n",
    "def cbow_create_embeddings() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create multiple embeddings models and pick the best one.  \n",
    "\n",
    "    Returns the embeddings of the best model.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset = cbow_create_dataset(words_train)\n",
    "\n",
    "    m1 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m1,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m1.parameters(), lr=0.02)\n",
    "    )\n",
    "\n",
    "    m2 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m2,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m2.parameters(), lr=0.01)\n",
    "    )\n",
    "    \n",
    "    m3 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m3,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m3.parameters(), lr=0.001)\n",
    "    )\n",
    "\n",
    "    val_dataset = cbow_create_dataset(words_val)\n",
    "    best_model = cbow_eval_and_pick_best(\n",
    "        val_dataset,\n",
    "        [m1, m2, m3]\n",
    "    )\n",
    "\n",
    "    return best_model.embeddings.weight.detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW_CrossEntropyLoss_SGD-lr0.020...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 0 | Loss 0.03887515080822781\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 1 | Loss 0.03571249065538397\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 2 | Loss 0.034859221171184424\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 3 | Loss 0.03433660710727191\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 4 | Loss 0.03396170793321118\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 5 | Loss 0.033673378571766464\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 6 | Loss 0.03343976608102727\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 7 | Loss 0.03324561911177446\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 8 | Loss 0.0330800309559612\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 9 | Loss 0.032937679209900844\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 10 | Loss 0.032812252067362435\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 11 | Loss 0.03270045539987507\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 12 | Loss 0.03260034241243252\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 13 | Loss 0.032509809169624475\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 14 | Loss 0.03242668771587599\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 15 | Loss 0.032351207941778246\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 16 | Loss 0.03228093999108281\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 17 | Loss 0.032216151584478124\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 18 | Loss 0.032155728189098155\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 19 | Loss 0.032098715322488425\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 20 | Loss 0.03204536615529997\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 21 | Loss 0.03199562539739062\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 22 | Loss 0.031948186455400436\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 23 | Loss 0.031903672070930804\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 24 | Loss 0.0318607756506218\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 25 | Loss 0.031820367286710886\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 26 | Loss 0.031781949367918436\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 27 | Loss 0.03174558591440909\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 28 | Loss 0.03171030207367599\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 29 | Loss 0.03167726766872735\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 30 | Loss 0.03164466394488623\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 31 | Loss 0.031613878975722064\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 32 | Loss 0.03158408631910965\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 33 | Loss 0.03155520158483198\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 34 | Loss 0.0315279930148646\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 35 | Loss 0.031501293696206766\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 36 | Loss 0.031475732190474844\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 37 | Loss 0.031450473325521165\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 38 | Loss 0.031425802282079085\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 39 | Loss 0.03140291798323165\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 40 | Loss 0.031380871766539364\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 41 | Loss 0.0313585287290838\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 42 | Loss 0.0313371576341276\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 43 | Loss 0.031316156110121764\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 44 | Loss 0.03129663286991736\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 45 | Loss 0.03127662656847075\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 46 | Loss 0.031257359408924865\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 47 | Loss 0.031239291172462037\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 48 | Loss 0.031220844635026397\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 49 | Loss 0.031203917121496956\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 50 | Loss 0.03118577031483208\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 51 | Loss 0.031168988301676796\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 52 | Loss 0.031152991990541962\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 53 | Loss 0.031136591188366973\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 54 | Loss 0.03112123507887843\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 55 | Loss 0.031105320247953126\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 56 | Loss 0.03109095063100137\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 57 | Loss 0.03107586806221624\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 58 | Loss 0.031061521725324347\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 59 | Loss 0.031046767987384816\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 60 | Loss 0.031032968731899754\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 61 | Loss 0.031020257819213387\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 62 | Loss 0.031006793214588886\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 63 | Loss 0.03099359342064535\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 64 | Loss 0.03098060896725557\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 65 | Loss 0.03096795625471887\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 66 | Loss 0.030955635283035243\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 67 | Loss 0.030942906910303977\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 68 | Loss 0.030930810009196556\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 69 | Loss 0.030919041938934726\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 70 | Loss 0.03090739608898719\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 71 | Loss 0.03089654176107507\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 72 | Loss 0.030885169451830948\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 73 | Loss 0.03087427147380658\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 74 | Loss 0.030863551006238683\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 75 | Loss 0.03085258318803472\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 76 | Loss 0.030842115891117856\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 77 | Loss 0.03083182319464998\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 78 | Loss 0.030821102727082085\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 79 | Loss 0.03081193038349522\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 80 | Loss 0.030801591126907616\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 81 | Loss 0.03079202884231801\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 82 | Loss 0.030782088256755595\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 83 | Loss 0.03077301194341567\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 84 | Loss 0.03076311791797299\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 85 | Loss 0.030754050334655517\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 86 | Loss 0.03074472958068701\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 87 | Loss 0.030736139238596773\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 88 | Loss 0.030727208425631012\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 89 | Loss 0.030718478403181582\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 90 | Loss 0.030708985958771574\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 91 | Loss 0.030700674977399717\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 92 | Loss 0.030692893617389794\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 93 | Loss 0.030684541895913176\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 94 | Loss 0.030675835153523612\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 95 | Loss 0.03066796067327423\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 96 | Loss 0.03066044994396024\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 97 | Loss 0.030652360123157103\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 98 | Loss 0.030643897821396125\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 99 | Loss 0.03063647439230663\n",
      "Saved CBOW_CrossEntropyLoss_SGD-lr0.020 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.020.pt)\n",
      "Training CBOW_CrossEntropyLoss_SGD-lr0.010...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 0 | Loss 0.04075043820056684\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 1 | Loss 0.037000459967422815\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 2 | Loss 0.03600157661877431\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 3 | Loss 0.03541110991037736\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 4 | Loss 0.03499257808411439\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 5 | Loss 0.03466798711942224\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 6 | Loss 0.03440553354451762\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 7 | Loss 0.03418547877864239\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 8 | Loss 0.03399722748455042\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 9 | Loss 0.033833236922845414\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 10 | Loss 0.033688592090888335\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 11 | Loss 0.03355949833892117\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 12 | Loss 0.033443461790530864\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 13 | Loss 0.033338128249663555\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 14 | Loss 0.033241126060220484\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 15 | Loss 0.03315264146268057\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 16 | Loss 0.033070800412224675\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 17 | Loss 0.03299452911609151\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 18 | Loss 0.03292336779309875\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 19 | Loss 0.03285720586296202\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 20 | Loss 0.032795004453009854\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 21 | Loss 0.03273601569131907\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 22 | Loss 0.03268090305959583\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 23 | Loss 0.0326278798132455\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 24 | Loss 0.032578927667364074\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 25 | Loss 0.032531017304161615\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 26 | Loss 0.03248599657838992\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 27 | Loss 0.03244351337914352\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 28 | Loss 0.03240258994390809\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 29 | Loss 0.032362862521748234\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 30 | Loss 0.03232510517465446\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 31 | Loss 0.03228931208261181\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 32 | Loss 0.03225402242186374\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 33 | Loss 0.03222054278577018\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 34 | Loss 0.03218877423407672\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 35 | Loss 0.03215736943331864\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 36 | Loss 0.03212744582636949\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 37 | Loss 0.032098176971104043\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 38 | Loss 0.032070255449303295\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 39 | Loss 0.03204364052086247\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 40 | Loss 0.032017307863147514\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 41 | Loss 0.031991376786465235\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 42 | Loss 0.03196615575160884\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 43 | Loss 0.03194220057000761\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 44 | Loss 0.03191937156130236\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 45 | Loss 0.031896475622424994\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 46 | Loss 0.031874106394902076\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 47 | Loss 0.03185241810913022\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 48 | Loss 0.03183148351529649\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 49 | Loss 0.03181120367314647\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 50 | Loss 0.03179055717005357\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 51 | Loss 0.0317713569406798\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 52 | Loss 0.0317526106724734\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 53 | Loss 0.031733526843398946\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 54 | Loss 0.031715569187220474\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 55 | Loss 0.0316976813712216\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 56 | Loss 0.0316804192068316\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 57 | Loss 0.03166335492295045\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 58 | Loss 0.03164668640008701\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 59 | Loss 0.03163035252808413\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 60 | Loss 0.03161442023711393\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 61 | Loss 0.03159872947676482\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 62 | Loss 0.031583629447934795\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 63 | Loss 0.03156792704755576\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 64 | Loss 0.03155318494964616\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 65 | Loss 0.03153884734277672\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 66 | Loss 0.03152462322619912\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 67 | Loss 0.03151053878998071\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 68 | Loss 0.031497097465416075\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 69 | Loss 0.0314834204302453\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 70 | Loss 0.03147070369754397\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 71 | Loss 0.03145750681360792\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 72 | Loss 0.03144449326014331\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 73 | Loss 0.031431898747756266\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 74 | Loss 0.031419312965391674\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 75 | Loss 0.03140667480289239\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 76 | Loss 0.03139524720350609\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 77 | Loss 0.03138360717357352\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 78 | Loss 0.03137140260218922\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 79 | Loss 0.031359884792570944\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 80 | Loss 0.03134848338325199\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 81 | Loss 0.03133767852546708\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 82 | Loss 0.03132722868859512\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 83 | Loss 0.03131625505037619\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 84 | Loss 0.031305400722464065\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 85 | Loss 0.03129516622614585\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 86 | Loss 0.03128455633886232\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 87 | Loss 0.03127423454231961\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 88 | Loss 0.03126447437722115\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 89 | Loss 0.03125400126028932\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 90 | Loss 0.03124475907652286\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 91 | Loss 0.031234643890511456\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 92 | Loss 0.031225555937141602\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 93 | Loss 0.031216680414318018\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 94 | Loss 0.031206396447872594\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 95 | Loss 0.031197320134532674\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 96 | Loss 0.03118866868228529\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 97 | Loss 0.03117927808813719\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 98 | Loss 0.031170539335665312\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 99 | Loss 0.031161366992078447\n",
      "Saved CBOW_CrossEntropyLoss_SGD-lr0.010 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.010.pt)\n",
      "Training CBOW_CrossEntropyLoss_SGD-lr0.001...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 0 | Loss 0.04853175747038497\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 1 | Loss 0.0432956384156001\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 2 | Loss 0.041458477941305194\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 3 | Loss 0.04040833772084861\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 4 | Loss 0.039697975794138717\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 5 | Loss 0.039167036198816924\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 6 | Loss 0.03874876918323493\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 7 | Loss 0.03840704700448951\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 8 | Loss 0.03812126971960751\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 9 | Loss 0.03787633438975141\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 10 | Loss 0.037662926080967676\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 11 | Loss 0.03747367374430151\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 12 | Loss 0.0373048845802568\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 13 | Loss 0.037151428245640765\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 14 | Loss 0.0370118730167717\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 15 | Loss 0.036883797767423634\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 16 | Loss 0.036765130572268566\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 17 | Loss 0.03665471906834318\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 18 | Loss 0.03655187649388144\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 19 | Loss 0.03645535736568057\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 20 | Loss 0.03636436434169019\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 21 | Loss 0.03627831833042115\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 22 | Loss 0.036197149491693864\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 23 | Loss 0.036119778212732066\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 24 | Loss 0.036046061903169106\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 25 | Loss 0.03597540983148589\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 26 | Loss 0.03590823521874503\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 27 | Loss 0.03584380474306077\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 28 | Loss 0.03578164407321337\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 29 | Loss 0.03572237013078924\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 30 | Loss 0.03566523504386522\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 31 | Loss 0.03560954914066781\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 32 | Loss 0.035555952622843286\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 33 | Loss 0.0355041806797107\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 34 | Loss 0.03545434971156937\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 35 | Loss 0.035405930097057364\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 36 | Loss 0.03535899749636924\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 37 | Loss 0.03531316778851723\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 38 | Loss 0.03526879017439931\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 39 | Loss 0.03522613819471889\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 40 | Loss 0.03518397800630312\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 41 | Loss 0.03514287124059625\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 42 | Loss 0.03510334460895273\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 43 | Loss 0.03506451928911264\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 44 | Loss 0.0350268259621835\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 45 | Loss 0.03499013076782107\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 46 | Loss 0.03495417471535937\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 47 | Loss 0.034918838494491566\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 48 | Loss 0.03488459061642246\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 49 | Loss 0.03485094801990985\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 50 | Loss 0.034817945625043534\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 51 | Loss 0.034786174163342586\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 52 | Loss 0.034754632592232805\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 53 | Loss 0.03472380979297136\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 54 | Loss 0.034693429314847354\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 55 | Loss 0.03466391019893836\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 56 | Loss 0.03463473155390488\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 57 | Loss 0.0346062804107422\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 58 | Loss 0.034578440369150976\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 59 | Loss 0.034551266719273395\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 60 | Loss 0.03452461396073529\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 61 | Loss 0.03449822310287067\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 62 | Loss 0.03447216398585913\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 63 | Loss 0.03444668396033674\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 64 | Loss 0.03442178011629601\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 65 | Loss 0.034397499013856664\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 66 | Loss 0.03437345362202346\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 67 | Loss 0.034349373310100455\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 68 | Loss 0.03432623584060199\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 69 | Loss 0.03430352614220355\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 70 | Loss 0.0342811103545609\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 71 | Loss 0.034258860437344794\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 72 | Loss 0.03423687533080965\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 73 | Loss 0.03421538492554664\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 74 | Loss 0.03419453472192993\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 75 | Loss 0.03417320436707849\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 76 | Loss 0.034153093305362495\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 77 | Loss 0.03413276108307778\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 78 | Loss 0.03411310689253664\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 79 | Loss 0.03409334212171113\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 80 | Loss 0.034074138982329874\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 81 | Loss 0.03405527631382415\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 82 | Loss 0.03403608190446534\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 83 | Loss 0.034017690657171876\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 84 | Loss 0.03399976210106824\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 85 | Loss 0.03398139704384213\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 86 | Loss 0.03396376821850925\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 87 | Loss 0.03394614812319882\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 88 | Loss 0.03392897907904828\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 89 | Loss 0.033911722734673244\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 90 | Loss 0.03389499601166014\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 91 | Loss 0.03387826928864704\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 92 | Loss 0.03386202562687614\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 93 | Loss 0.033845796515142655\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 94 | Loss 0.03382978274396292\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 95 | Loss 0.03381392611318728\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 96 | Loss 0.03379840704327968\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 97 | Loss 0.03378288797337208\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 98 | Loss 0.0337675726039883\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 99 | Loss 0.03375278685596646\n",
      "Saved CBOW_CrossEntropyLoss_SGD-lr0.001 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.001.pt)\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.020 | Accuracy 0.2166\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.010 | Accuracy 0.2084\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.001 | Accuracy 0.1869\n",
      "Best model: CBOW_CrossEntropyLoss_SGD-lr0.020 | Accuracy 0.21659497990751397\n"
     ]
    }
   ],
   "source": [
    "embeddings = cbow_create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_similarity_cosine(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return torch.dot(word_a, word_b) / (word_a.norm() * word_b.norm())\n",
    "\n",
    "def word_vector_similarity_euclidian(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return (word_a - word_b).norm()\n",
    "\n",
    "def word_similarity_cosine(word_a:str, word_b:str):\n",
    "    word_a_idx = vocabulary[word_a]\n",
    "    word_b_idx = vocabulary[word_b]\n",
    "\n",
    "    word_a_embedding = embeddings[word_a_idx]\n",
    "    word_b_embedding = embeddings[word_b_idx]\n",
    "\n",
    "    return word_vector_similarity_cosine(word_a_embedding, word_b_embedding)\n",
    "\n",
    "def word_find_top_closest(\n",
    "    word: str,\n",
    "    top: int\n",
    "):\n",
    "    similarities = []\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        similarity = word_similarity_cosine(word, other).item()\n",
    "        similarities.append((other, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    similarities = similarities[1:top+1]\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def word_find_closest(\n",
    "    word_vector:torch.Tensor,\n",
    "):\n",
    "    closest_word = None\n",
    "    closest_distance = 1_000_000\n",
    "\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        other_idx = vocabulary[other]\n",
    "        other_embedding = embeddings[other_idx]\n",
    "\n",
    "        distance = word_vector_similarity_euclidian(word_vector, other_embedding)\n",
    "\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_word = other\n",
    "    \n",
    "    return closest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar words\n",
      "king : ['earl', 'prince', 'father', 'spirit', 'building', 'aloud', 'rest', 'wrong', 'bishop', 'feel']\n",
      "queen : ['fixed', 'can', 'sprang', 'desired', 'trust', 'wound', 'large', 'walls', 'spring', 'crossed']\n",
      "man : ['wood', 'social', 'together', 'doctor', 'chamber', 'glancing', 'party', 'sun', 'harm', 'ye']\n",
      "woman : ['fatigue', 'soul', 'paid', 'empty', 'size', 'heat', 'man', 'thick', 'singing', 'le']\n",
      "he : ['who', 'she', 'growth', 'fully', 'face', 'wrong', 'count', 'they', 'example', 'i']\n",
      "she : ['he', 'himself', 'everything', 'never', 'excellent', 'brothers', 'key', 'child', 'her', 'i']\n",
      "doctor : ['smile', 'report', 'torn', 'glancing', 'man', 'really', 'cases', 'otherwise', 'uttered', 'dream']\n",
      "'nurse' is not in vocabulary\n",
      "black : ['poor', 'skald', 'words', 'white', 'turned', 'o', 'minutes', 'called', 'years', 'human']\n",
      "white : ['single', 'poor', 'excellent', 'rain', 'nose', 'personal', 'whose', 'black', 'horrible', 'gaze']\n",
      "'slave' is not in vocabulary\n",
      "master : ['begged', 'bring', 'rage', 'troubled', 'life', 'stout', 'approaching', 'eastward', 'autumn', 'court']\n",
      "poor : ['personal', 'black', 'white', 'curious', 'doctor', 'la', 'increased', 'dangerous', 'interesting', 'good']\n",
      "rich : ['-', 'built', 'march', 'others', 'manure', 'shut', 'peasants', 'erect', 'rest', 'bishop']\n",
      "'smart' is not in vocabulary\n",
      "'dumb' is not in vocabulary\n",
      "strong : ['huge', 'early', 'direct', 'driver', 'the', 'de', 'enemy', 'chief', 'wise', 'quick']\n",
      "weak : ['remember', 'trench', 'ladies', 'kind', 'fingers', 'military', 'shouting', 'anxiety', 'tea', 'rose']\n",
      "good : ['curious', 'popular', 'treated', 'become', 'frost', 'loud', 'sick', 'all', 'confidence', 'broad']\n",
      "bad : ['delicate', 'often', 'mine', 'tender', 'level', 'gardener', 'equal', 'curious', 'stay', 'also']\n"
     ]
    }
   ],
   "source": [
    "def print_most_similar_words(words, top = 10):\n",
    "    print(f\"Top {top} most similar words\")\n",
    "    for word in words:\n",
    "        if vocabulary[word] == vocabulary['<unk>']:\n",
    "            print(word, ':'. \"Not in vocabulary\")\n",
    "        else:\n",
    "            print(word, ':', [x[0] for x in word_find_top_closest(word, top)])\n",
    "\n",
    "print_most_similar_words([\n",
    "    'king', 'queen', 'man', 'woman', 'he', 'she', 'doctor', 'nurse',\n",
    "    'black', 'white', 'slave', 'master',\n",
    "    'poor', 'rich', \n",
    "    'smart', 'dumb', \n",
    "    'strong', 'weak',\n",
    "    'good', 'bad',\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
