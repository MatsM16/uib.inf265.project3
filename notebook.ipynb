{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from torch import nn;\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "DOCS_DIR = './docs/'\n",
    "MIN_WORD_FREQUENCY = 100\n",
    "FORCE_RETRAIN = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "\n",
    "def read_lines(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads all the lines form all the texts in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scan for all input files\n",
    "    inDirectoryName = os.path.join(DATA_DIR, 'input', dataset)\n",
    "    inFileNames = [os.path.join(inDirectoryName, f) for f in os.listdir(inDirectoryName)]\n",
    "\n",
    "    # Read all the lines from all the files\n",
    "    lines = []\n",
    "    for inFileName in inFileNames:\n",
    "        with open(inFileName, 'r') as file:\n",
    "            lines += file.readlines()\n",
    "\n",
    "    print(f\"Read {len(lines)} lines from {dataset}\")\n",
    "    return lines\n",
    "\n",
    "def create_tokens(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Creates tokens for all the words in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'words.{dataset}.pt')\n",
    "    \n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded tokenized words for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    tokens = []\n",
    "    for line in read_lines(dataset):\n",
    "        tokens += TOKENIZER(line)\n",
    "\n",
    "    # Save tokens so we dont have to do this again\n",
    "    torch.save(tokens, outFileName)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_vocabulary(\n",
    "    dataset: str\n",
    ") -> Vocab:\n",
    "    \"\"\"\n",
    "    Creates a vocabulary for the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'vocabulary.pt')\n",
    "\n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded vocabulary for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    def read_sanitize_tokenize():\n",
    "\n",
    "        for line in read_lines(dataset):\n",
    "\n",
    "            line = re.sub('\\\\w*[0-9]+\\\\w*', ' ', line) # Remove numbers\n",
    "            line = re.sub('\\\\w*[A-Z]+\\\\w*', ' ', line) # Remove uppercase names\n",
    "            line = re.sub('\\\\s+', ' ', line) # Remove double spaces\n",
    "\n",
    "            yield TOKENIZER(line)\n",
    "\n",
    "    vocabulary = build_vocab_from_iterator(read_sanitize_tokenize(), min_freq=MIN_WORD_FREQUENCY, specials=['<unk>'])\n",
    "\n",
    "    vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "    # We removed all uppercase names, this includes 'I'\n",
    "    vocabulary.append_token('i') \n",
    "\n",
    "    # Save vocabulary so we dont have to do this again\n",
    "    torch.save(vocabulary, outFileName)\n",
    "\n",
    "    return vocabulary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train = create_tokens('train')\n",
    "words_val = create_tokens('val')\n",
    "words_test = create_tokens('test')\n",
    "\n",
    "vocabulary = create_vocabulary('train')\n",
    "VOCABULARY_SIZE = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in 'train' dataset ........: 2684706\n",
      "Words in 'val' dataset ..........: 49526\n",
      "Words in 'test' dataset .........: 124152\n",
      "Distinct words in 'train' dataset: 52105\n",
      "Words in vocabulary .............: 1880\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAHHCAYAAAAiSltoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6yklEQVR4nO3deXwN9+L/8feJyEISEUKSiiQoYt9VtKWlYi1KuUXRxlI3SpVb1VtbtbS3rbZarS6XhKu1VC3XviXEVopYSi2xE3Vba0Ityfz+8Mt8e5ogCMHn9Xw8zqOdz3zmM58ZJ3PeZ+YzcxyWZVkCAADGcsntDgAAgNxFGAAAwHCEAQAADEcYAADAcIQBAAAMRxgAAMBwhAEAAAxHGAAAwHCEAQAADEcYAB4w8fHxcjgcio+Pz+2uALhPEAaAWzBt2jQ5HA7NnDkz07zKlSvL4XAoLi4u07zixYsrIiLibnTxvnfs2DENGzZMiYmJud0V4IFHGABuwaOPPipJWrVqlVP52bNntX37drm6umr16tVO8w4fPqzDhw/by+L6jh07puHDhxMGgLuAMADcgqCgIIWFhWUKA2vXrpVlWXr22WczzcuYvt0wYFmWLly4cFttAMCfEQaAW/Too49q8+bNTh/Mq1evVvny5dWkSROtW7dO6enpTvMcDofq1q0rSbpy5YpGjBihkiVLyt3dXaGhoXrjjTd08eJFp/WEhoaqefPmWrRokWrUqCFPT099+eWXkqQjR46oVatWyp8/v4oUKaJ+/fplWv56jh49qqioKAUFBcnd3V1hYWHq1auXLl26ZNfZt2+fnn32Wfn5+Slfvnx65JFHNG/ePKd2YmJi5HA4dODAAafyrMYv1K9fXxUqVNCOHTv0xBNPKF++fHrooYf0r3/9y2m5mjVrSpJeeOEFORwOORwOxcTESJL27NmjNm3aKCAgQB4eHipWrJj+9re/6cyZM9nedgD/xzW3OwDcrx599FFNmjRJP/74o+rXry/p6gd+RESEIiIidObMGW3fvl2VKlWy55UtW1aFChWSJHXr1k2xsbFq27at+vfvrx9//FGjRo3Szp07M41F2LVrl5577jn17NlT3bt3V5kyZXThwgU1aNBAhw4dUp8+fRQUFKRJkyZp+fLl2er/sWPHVKtWLZ0+fVo9evRQ2bJldfToUX3//fc6f/683Nzc9OuvvyoiIkLnz59Xnz59VKhQIcXGxurpp5/W999/r9atW9/Svjt16pQaN26sZ555Ru3atdP333+vgQMHqmLFimrSpInCw8P11ltvaciQIerRo4cee+wxSVJERIQuXbqkyMhIXbx4US+//LICAgJ09OhRzZ07V6dPn1aBAgVuqU+A0SwAt+Tnn3+2JFkjRoywLMuyLl++bOXPn9+KjY21LMuyihYtao0dO9ayLMs6e/aslSdPHqt79+6WZVlWYmKiJcnq1q2bU5sDBgywJFnLly+3y0JCQixJ1sKFC53qfvzxx5Yka9q0aXZZamqqVapUKUuSFRcXd93+d+7c2XJxcbE2bNiQaV56erplWZb1yiuvWJKshIQEe965c+essLAwKzQ01EpLS7Msy7ImTJhgSbL279/v1E5cXFymvtSrV8+SZE2cONEuu3jxohUQEGC1adPGLtuwYYMlyZowYYJTm5s3b7YkWdOnT7/u9gHIPi4TALcoPDxchQoVsscCbNmyRampqfbdAhEREfYgwrVr1yotLc0eLzB//nxJ0quvvurUZv/+/SUp02n4sLAwRUZGOpXNnz9fgYGBatu2rV2WL18+9ejR44Z9T09P16xZs9SiRQvVqFEj03yHw2Gvo1atWk7jHLy8vNSjRw8dOHBAO3bsuOG6suLl5aVOnTrZ025ubqpVq5b27dt3w2UzvvkvWrRI58+fv6X1A3BGGABukcPhUEREhD02YPXq1SpSpIhKlSolyTkMZPw340P14MGDcnFxsetmCAgIkK+vrw4ePOhUHhYWlmn9Bw8eVKlSpewP7gxlypS5Yd//97//6ezZs6pQocJ16x08eDDL9sLDw+35t6JYsWKZ+l2wYEGdOnXqhsuGhYXp1Vdf1TfffKPChQsrMjJSY8eOZbwAcBsIA8BtePTRR3XmzBlt27bNHi+QISIiQgcPHtTRo0e1atUqBQUFqUSJEk7L//UD8Vo8PT1ztN857VrbkZaWlmV5njx5siy3LCtb6/vwww+1detWvfHGG7pw4YL69Omj8uXL68iRI9nrMAAnhAHgNvz5eQOrV6+27xSQpOrVq8vd3V3x8fH68ccfneaFhIQoPT1de/bscWrv119/1enTpxUSEnLDdYeEhCgpKSnTB+iuXbtuuKy/v798fHy0ffv2G64jq/Z++eUXe7509Vu9JJ0+fdqp3q2eOZBuHJQqVqyoN998UytXrlRCQoKOHj2qcePG3fL6AJMRBoDbUKNGDXl4eGjy5Mk6evSo05kBd3d3VatWTWPHjlVqaqrTdfemTZtKkj7++GOn9kaPHi1Jatas2Q3X3bRpUx07dkzff/+9XXb+/Hl99dVXN1zWxcVFrVq10n//+1/99NNPmeZnBIymTZtq/fr1Wrt2rT0vNTVVX331lUJDQ1WuXDlJUsmSJSVJK1eutOulpaVlqy/Xkj9/fkmZA8bZs2d15coVp7KKFSvKxcXlpm6rBPB/uLUQuA1ubm6qWbOmEhIS5O7ururVqzvNj4iI0IcffijJ+WFDlStXVpcuXfTVV1/p9OnTqlevntavX6/Y2Fi1atVKTzzxxA3X3b17d3322Wfq3LmzNm7cqMDAQE2aNEn58uXLVt9HjhypxYsXq169eurRo4fCw8OVnJys6dOna9WqVfL19dXrr7+u7777Tk2aNFGfPn3k5+en2NhY7d+/XzNmzJCLy9XvE+XLl9cjjzyiQYMG6eTJk/Lz89OUKVMyfWjfjJIlS8rX11fjxo2Tt7e38ufPr9q1a2vLli3q3bu3nn32WZUuXVpXrlzRpEmTlCdPHrVp0+aW1wcYLZfvZgDue4MGDbIkWREREZnm/fDDD5Yky9vb27py5YrTvMuXL1vDhw+3wsLCrLx581rBwcHWoEGDrD/++MOpXkhIiNWsWbMs133w4EHr6aeftvLly2cVLlzY6tu3r7Vw4cJs3VqYsXznzp0tf39/y93d3SpRooQVHR1tXbx40a6TlJRktW3b1vL19bU8PDysWrVqWXPnzs3UVlJSktWwYUPL3d3dKlq0qPXGG29YS5YsyfLWwvLly2davkuXLlZISIhT2ezZs61y5cpZrq6u9m2G+/bts1588UWrZMmSloeHh+Xn52c98cQT1tKlS2+4vQCy5rCsbI7YAQAADyTGDAAAYDjCAAAAhiMMAABgOMIAAACGIwwAAGA4wgAAAIbL1kOH0tPTdezYMXl7e2f7WeoAACB3WZalc+fOKSgoyH5IWFayFQaOHTum4ODgHOscAAC4ew4fPqxixYpdc362woC3t7fdmI+PT870DAAA3FFnz55VcHCw/Tl+LdkKAxmXBnx8fAgDAADcZ250iZ8BhAAAGI4wAACA4e75MDBq1CjVrFlT3t7eKlKkiFq1aqVdu3Zlqrd27Vo9+eSTyp8/v3x8fPT444/rwoUL9vzdu3erZcuWKly4sHx8fPToo48qLi4uy3X+/vvvKlasmBwOh9NvqXft2lUOhyPTq3z58k7Ljx07VqGhofLw8FDt2rW1fv36m+4vAAB3yz0fBlasWKHo6GitW7dOS5Ys0eXLl9WoUSOlpqbaddauXavGjRurUaNGWr9+vTZs2KDevXs73UbRvHlzXblyRcuXL9fGjRtVuXJlNW/eXMePH8+0zqioKFWqVClT+SeffKLk5GT7dfjwYfn5+enZZ5+160ydOlWvvvqqhg4dqk2bNqly5cqKjIzUiRMnbqq/AADcNdn5neMzZ85YkqwzZ87c0d9Tzo4TJ05YkqwVK1bYZbVr17befPPNay7zv//9z5JkrVy50i47e/asJclasmSJU93PP//cqlevnrVs2TJLknXq1Klrtjtz5kzL4XBYBw4csMtq1aplRUdH29NpaWlWUFCQNWrUqGz3FwCAnJDdz+/77qvomTNnJEl+fn6SpBMnTujHH39UkSJFFBERoaJFi6pevXpatWqVvUyhQoVUpkwZTZw4Uampqbpy5Yq+/PJLFSlSRNWrV7fr7dixQ2+99ZYmTpyYrW/p//73v9WwYUOFhIRIki5duqSNGzeqYcOGdh0XFxc1bNhQa9euzXZ/AQC4m+6rMJCenq5XXnlFdevWVYUKFSRJ+/btkyQNGzZM3bt318KFC1WtWjU1aNBAe/bskXT1loqlS5dq8+bN8vb2loeHh0aPHq2FCxeqYMGCkqSLFy/queee0/vvv6/ixYvfsC/Hjh3TggUL1K1bN7vst99+U1pamooWLepUt2jRovbliOz0FwCAu+m+CgPR0dHavn27pkyZYpelp6dLknr27KkXXnhBVatW1UcffaQyZcpo/Pjxkq4+jjE6OlpFihRRQkKC1q9fr1atWqlFixZKTk6WJA0aNEjh4eHq1KlTtvoSGxsrX19ftWrV6qa2ITv9BQDgbrpvwkDv3r01d+5cxcXFOT1SMTAwUJJUrlw5p/rh4eE6dOiQJGn58uWaO3eupkyZorp166patWr6/PPP5enpqdjYWLvO9OnT5erqKldXVzVo0ECSVLhwYQ0dOtSpbcuyNH78eD3//PNyc3OzywsXLqw8efLo119/dar/66+/KiAgINv9BQDgbrrnw4BlWerdu7dmzpyp5cuXKywszGl+aGiogoKCMt1uuHv3bvta/vnz5yUp0zgAFxcX+5v6jBkztGXLFiUmJioxMVHffPONJCkhIUHR0dFOy61YsUJ79+5VVFSUU7mbm5uqV6+uZcuW2WXp6elatmyZ6tSpk+3+AgBwV+XkaMQ7oVevXlaBAgWs+Ph4Kzk52X6dP3/ervPRRx9ZPj4+1vTp0609e/ZYb775puXh4WHt3bvXsqyrdxMUKlTIeuaZZ6zExERr165d1oABA6y8efNaiYmJWa43Li7umncTdOrUyapdu3aWy02ZMsVyd3e3YmJirB07dlg9evSwfH19rePHj2e7vwAA5ITsfn7f82FAUpavCRMmONUbNWqUVaxYMStfvnxWnTp1rISEBKf5GzZssBo1amT5+flZ3t7e1iOPPGLNnz//muu9Vhg4ffq05enpaX311VfXXPbTTz+1ihcvbrm5uVm1atWy1q1bl6nOjfoLAMDtyu7nt8OyLOtGZw/Onj2rAgUK6MyZM/xQEQAA94nsfn7f82MGAADAnUUYAADAcK653QFJCn19Xm534b5x4N1mud0FAMADhjMDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIAwAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMAAAgOEIA8A9buXKlWrRooWCgoLkcDg0a9Yse97ly5c1cOBAVaxYUfnz51dQUJA6d+6sY8eOZdnWxYsXVaVKFTkcDiUmJtrlf/zxh7p27aqKFSvK1dVVrVq1yrRsfHy8HA5Hptfx48ftOqNGjVLNmjXl7e2tIkWKqFWrVtq1a1dO7QoAdwhhALjHpaamqnLlyho7dmymeefPn9emTZs0ePBgbdq0ST/88IN27dqlp59+Osu2XnvtNQUFBWUqT0tLk6enp/r06aOGDRtetz+7du1ScnKy/SpSpIg9b8WKFYqOjta6deu0ZMkSXb58WY0aNVJqaupNbjWAu8k1tzsA4PqaNGmiJk2aZDmvQIECWrJkiVPZZ599plq1aunQoUMqXry4Xb5gwQItXrxYM2bM0IIFC5yWyZ8/v7744gtJ0urVq3X69Olr9qdIkSLy9fXNct7ChQudpmNiYlSkSBFt3LhRjz/++DXbBJC7ODMAPGDOnDkjh8Ph9IH966+/qnv37po0aZLy5ct3W+1XqVJFgYGBeuqpp7R69eob9kWS/Pz8bmudAO4swgDwAPnjjz80cOBAPffcc/Lx8ZEkWZalrl276qWXXlKNGjVuue3AwECNGzdOM2bM0IwZMxQcHKz69etr06ZNWdZPT0/XK6+8orp166pChQq3vF4Adx6XCYAHxOXLl9WuXTtZlmWf8pekTz/9VOfOndOgQYNuq/0yZcqoTJky9nRERISSkpL00UcfadKkSZnqR0dHa/v27Vq1atVtrRfAnceZAeABkBEEDh48qCVLlthnBSRp+fLlWrt2rdzd3eXq6qpSpUpJkmrUqKEuXbrc1npr1aqlvXv3Zirv3bu35s6dq7i4OBUrVuy21gHgzuPMAHCfywgCe/bsUVxcnAoVKuQ0f8yYMXr77bft6WPHjikyMlJTp05V7dq1b2vdiYmJCgwMtKcty9LLL7+smTNnKj4+XmFhYbfVPoC7gzAA3ONSUlKcvn3v379fiYmJ8vPzU2BgoNq2batNmzZp7ty5SktLs+/79/Pzk5ubm9MdBZLk5eUlSSpZsqTTt/YdO3bo0qVLOnnypM6dO2c/h6BKlSqSpI8//lhhYWEqX768/vjjD33zzTdavny5Fi9ebLcRHR2tb7/9VrNnz5a3t7fdlwIFCsjT0zPH9w2AnMFlAuSYd999Vw6HQ6+88opdlpSUpNatW8vf318+Pj5q166dfv31V6flQkNDMz3I5t1337Xnx8fHq2XLlgoMDFT+/PlVpUoVTZ48OdP6T58+rejoaAUGBsrd3V2lS5fW/Pnz79j23i0//fSTqlatqqpVq0qSXn31VVWtWlVDhgzR0aNHNWfOHB05csQe5Z/xWrNmzU2tp2nTpqpatar++9//Kj4+3mmdknTp0iX1799fFStWVL169bRlyxYtXbpUDRo0sOt88cUXOnPmjOrXr+/Ul6lTp+bMzshFw4YNy/Q+LVu2rCTp5MmTevnll1WmTBl5enqqePHi6tOnj303RYY+ffqoevXqcnd3t0PWX23dulWPPfaYPDw8FBwcrH/9619O82NiYjL1w8PD445sM8zBmQHkiA0bNujLL79UpUqV7LLU1FQ1atRIlStX1vLlyyVJgwcPVosWLbRu3Tq5uPxfFn3rrbfUvXt3e9rb29v+/zVr1qhSpUoaOHCgihYtqrlz56pz584qUKCAmjdvLunqB9VTTz2lIkWK6Pvvv9dDDz2kgwcPXvN++PtJ/fr1ZVnWNedfb15WQkNDs1zmwIED113utdde02uvvXbdOjfbl/tN+fLltXTpUnva1fXqIfTYsWM6duyYPvjgA5UrV04HDx7USy+9pGPHjun77793auPFF1/Ujz/+qK1bt2Zq/+zZs2rUqJEaNmyocePGadu2bXrxxRfl6+urHj162PV8fHycnuzocDhyelNhGMIAbltKSoo6duyor7/+2una9OrVq3XgwAFt3rzZHtAWGxurggULavny5U5PuvP29lZAQECW7b/xxhtO03379tXixYv1ww8/2GFg/PjxOnnypNasWaO8efNKuvqhB+QkV1fXLN+nFSpU0IwZM+zpkiVL6p133lGnTp105coVOzSMGTNGkvS///0vyzAwefJkXbp0SePHj5ebm5vKly+vxMREjR492ikMOByOa/69ALeCMIDbFh0drWbNmqlhw4ZOYeDixYtyOBxyd3e3yzw8POTi4qJVq1Y5hYF3331XI0aMUPHixdWhQwf169fPPoBm5cyZMwoPD7en58yZozp16ig6OlqzZ8+Wv7+/OnTooIEDBypPnjw5vMW3LvT1ebndhfvKgXeb5XYXnOzZs0dBQUHy8PBQnTp1NGrUqExjMjKcOXNGPj4+130f/9XatWv1+OOPy83NzS6LjIzUe++9p1OnTqlgwYKSrgbwkJAQpaenq1q1aho5cqTKly9/exsHozFmALdlypQp2rRpk0aNGpVp3iOPPKL8+fNr4MCBOn/+vFJTUzVgwAClpaUpOTnZrtenTx9NmTJFcXFx6tmzp0aOHHnd09HTpk3Thg0b9MILL9hl+/bt0/fff6+0tDTNnz9fgwcP1ocffugUToDbUbt2bcXExGjhwoX64osvtH//fj322GM6d+5cprq//fabRowY4fRtPjuOHz+uokWLOpVlTGcMxixTpozGjx+v2bNn6z//+Y/S09MVERGhI0eO3OKWAZwZwG04fPiw+vbtqyVLlmQ5gMnf31/Tp09Xr169NGbMGLm4uOi5555TtWrVnMYLvPrqq/b/V6pUSW5uburZs6dGjRrldFZBkuLi4vTCCy/o66+/dvomlJ6eriJFiuirr75Snjx5VL16dR09elTvv/++hg4dege2Hqb58+9DVKpUSbVr11ZISIimTZumqKgoe97Zs2fVrFkzlStXTsOGDcvxftSpU0d16tSxpyMiIhQeHq4vv/xSI0aMyPH1wQyEAdyyjRs36sSJE6pWrZpdlpaWppUrV+qzzz7TxYsX1ahRIyUlJem3336Tq6urfH19FRAQoBIlSlyz3dq1a+vKlSs6cOCA0xPvVqxYoRYtWuijjz5S586dnZYJDAxU3rx5nS4JhIeH6/jx47p06ZLTaVcgJ/j6+qp06dJOt32eO3dOjRs3lre3t2bOnGmPX8mugICATHfbZExfa4xA3rx5VbVq1Swf/gRkF5cJcMsaNGigbdu2KTEx0X7VqFFDHTt2VGJiotMHc+HCheXr66vly5frxIkT1/yJXenqg2xcXFycfho3Pj5ezZo103vvvZflqde6detq7969Sk9Pt8t2796twMBAggDuiJSUFCUlJdkPXcq4E8DNzU1z5sy5pdv96tSpo5UrV+ry5ct22ZIlS1SmTBl7vMBfpaWladu2bU4Pf3rQrVy5Ui1atFBQUJAcDodmzZrlND8lJUW9e/dWsWLF5OnpqXLlymncuHFOdY4fP67nn39eAQEByp8/v6pVq+Y0CFS6estox44d5ePjI19fX0VFRSklJeVOb16uIAzglnl7e6tChQpOr/z586tQoUL2D9NMmDBB69atU1JSkv7zn//o2WefVb9+/exv/GvXrtXHH3+sLVu2aN++fZo8ebL69eunTp062Qe/uLg4NWvWTH369FGbNm10/PhxHT9+XCdPnrT70qtXL508eVJ9+/bV7t27NW/ePI0cOVLR0dF3f8fggTRgwACtWLFCBw4c0Jo1a9S6dWvlyZNHzz33nB0EUlNT9e9//1tnz56136dpaWl2G3v37lViYqKOHz+uCxcu2CH60qVLkqQOHTrIzc1NUVFR+vnnnzV16lR98sknTpfS3nrrLS1evFj79u3Tpk2b1KlTJx08eFDdunW76/skt6Smpqpy5coaO3ZslvNfffVVLVy4UP/5z3+0c+dOvfLKK+rdu7fmzJlj1+ncubN27dqlOXPmaNu2bXrmmWfUrl07bd682a7TsWNH/fzzz1qyZInmzp2rlStX3vQ4kPsFlwlwR+3atUuDBg3SyZMnFRoaqn/+85/q16+fPd/d3V1TpkzRsGHDdPHiRYWFhalfv35OB7/Y2FidP39eo0aNchqoWK9ePcXHx0uSgoODtWjRIvXr10+VKlXSQw89pL59+2rgwIF3bVvxYDty5Iiee+45/f777/L399ejjz6qdevWyd/fX/Hx8frxxx8lyf7thwz79++3b3Pt1q2bVqxYYc/LeKhTRp0CBQpo8eLFio6OVvXq1VW4cGENGTLE6QPo1KlT6t69u44fP66CBQuqevXqWrNmjcqVK3eH98C9o0mTJk5jOP5qzZo16tKli+rXry9J6tGjh7788kutX7/ePiu5Zs0affHFF6pVq5Yk6c0339RHH32kjRs3qmrVqtq5c6cWLlyoDRs22L/2+emnn6pp06b64IMPFBQUdGc38i5zWNl4SsjZs2dVoEAB+1aZnMbtVtl3r91qhZvDe/3m8H7HjTgcDs2cOVOtWrWyy3r06KHNmzdr1qxZCgoKUnx8vJ5++mnNmzdPjz/+uCTZl3QmTpwoX19feyDoli1bVKpUKY0fP179+/fXqVOn7HavXLkiDw8PTZ8+Xa1bt77bm3pLsvv5zZkBAMAD5dNPP1WPHj1UrFgxubq6ysXFRV9//bUdBKSrtyi3b99ehQoVkqurq/Lly6eZM2faZ3aOHz/uNG5JuvrQKT8/P/s2zwcJYQAA8ED59NNPtW7dOs2ZM0chISFauXKloqOjFRQUZD/sbPDgwTp9+rSWLl2qwoULa9asWWrXrp0SEhJUsWLFXN6Cu48wYChOV98cTlff33i/35z7+f1+4cIFvfHGG5o5c6aaNbu6HZUqVVJiYqI++OADNWzYUElJSfrss8+0fft2+3kllStXVkJCgsaOHatx48YpICBAJ06ccGr7ypUrOnny5AP5KGjuJgAAPDAuX76sy5cvOz3YTJLy5Mlj33p8/vx5SbpunTp16uj06dPauHGjPX/58uVKT09X7dq17+Qm5ArODAAA7ispKSlOD1nav3+/EhMT5efnp+LFi6tevXr6xz/+IU9PT4WEhGjFihWaOHGiRo8eLUkqW7asSpUqpZ49e+qDDz5QoUKFNGvWLPsWQunqQ8saN26s7t27a9y4cbp8+bJ69+6tv/3tbw/cnQQSYQAAcJ/56aef9MQTT9jTGbcid+nSRTExMZoyZYoGDRqkjh076uTJkwoJCdE777yjl156SdLVpzbOnz9fr7/+ulq0aKGUlBSVKlVKsbGxatq0qd3u5MmT1bt3bzVo0EAuLi5q06aN/cuTDxrCAADgvlK/fn1d7674gIAATZgw4bptPPzww5meOPhXfn5++vbbb2+pj/cbxgwAAGC4bJ0ZyEhgZ8+evSOdSL94/o60+yDKqX8D9vnNYb/nDvZ77sip/V5h6KIcaccE24dH3pF2M/4tb/R8wWw9gfDIkSMKDg7OmZ4BAIC76vDhwypWrNg152crDKSnp+vYsWPy9vaWw+HI0Q7ei86ePavg4GAdPnz4jjx+GVljv+cO9nvuYL/nDtP2u2VZOnfunIKCgjLdSvln2bpM4OLict1E8aDy8fEx4s1yr2G/5w72e+5gv+cOk/Z7gQIFbliHAYQAABiOMAAAgOEIA1lwd3fX0KFD5e7unttdMQr7PXew33MH+z13sN+zlq0BhAAA4MHFmQEAAAxHGAAAwHCEAQAADEcYuAmhoaH6+OOPc7sb96z69evrlVdesaezs78cDodmzZp12+vOqXaAO4Xjx93XtWtXtWrV6rbbMeH48kCGAYfDcd3XsGHDbqndDRs2qEePHjnb2XtEixYt1Lhx4yznJSQkyOFwaOvWrTfV5p3YX8OGDVOVKlUylScnJ6tJkyY5uq770Z1672e0/aAfELPy15CbISYmRr6+vtlu50E+fmTlThxTcOc8kD9hnJycbP//1KlTNWTIEO3atcsu8/Lysv/fsiylpaXJ1fXGu8Lf3z9nO3oPiYqKUps2bXTkyJFMT5ucMGGCatSooUqVKt1Um3dzfwUEBNy1dd3Lbua9j7vrQT5+ZOVOHFPuV5cuXZKbm1tud+O6HsgzAwEBAfarQIECcjgc9vQvv/wib29vLViwQNWrV5e7u7tWrVqlpKQktWzZUkWLFpWXl5dq1qyppUuXOrX719N8DodD33zzjVq3bq18+fLp4Ycf1pw5c+7y1uaM5s2by9/fXzExMU7lKSkpmj59ulq1aqXnnntODz30kPLly6eKFSvqu+++u26bf91fe/bs0eOPPy4PDw+VK1dOS5YsybTMwIEDVbp0aeXLl08lSpTQ4MGDdfnyZUlXv4kNHz5cW7Zssb/pZvT3r99at23bpieffFKenp4qVKiQevTooZSUFHt+xunDDz74QIGBgSpUqJCio6Ptdd2vrvfeDwgI0JQpUxQeHi4PDw+VLVtWn3/+ub3spUuX1Lt3bwUGBsrDw0MhISEaNWqUpKv/lpLUunVrORwOexpXZef9lJ2/hz+/j+Pj4+VwOHT69Gl7mcTERDkcDh04cMAuW7VqlR577DF5enoqODhYffr0UWpq6h3e4hu70TElKipKM2bMUPny5eXu7q7Q0FB9+OGHTnUvXryogQMHKjg4WO7u7ipVqpT+/e9/S5LS0tIUFRWlsLAweXp6qkyZMvrkk0+y7Mvw4cPl7+8vHx8fvfTSS7p06ZI9L6vLN1WqVLnuWbTrHaek/zuD+c033ygsLEweHh6aOHGiChUqpIsXLzq11apVKz3//PPXXNfd8kCGgex4/fXX9e6772rnzp2qVKmSUlJS1LRpUy1btkybN29W48aN1aJFCx06dOi67QwfPlzt2rXT1q1b1bRpU3Xs2FEnT568S1uRc1xdXdW5c2fFxMQ4/dTl9OnTlZaWpk6dOql69eqaN2+etm/frh49euj555/X+vXrs9V+enq6nnnmGbm5uenHH3/UuHHjNHDgwEz1vL29FRMTox07duiTTz7R119/rY8++kiS1L59e/Xv31/ly5dXcnKykpOT1b59+0xtpKamKjIyUgULFtSGDRs0ffp0LV26VL1793aqFxcXp6SkJMXFxSk2NlYxMTGZDlwPksmTJ2vIkCF65513tHPnTo0cOVKDBw9WbGysJGnMmDGaM2eOpk2bpl27dmny5Mn2h/6GDRskXf1Gl5ycbE/j/9zM+ym7fw83kpSUpMaNG6tNmzbaunWrpk6dqlWrVmV6r+eGGx1TwsPD1a5dO/3tb3/Ttm3bNGzYMA0ePNhpn3Xu3FnfffedxowZo507d+rLL7+0z26lp6erWLFimj59unbs2KEhQ4bojTfe0LRp05z6sWzZMu3cuVPx8fH67rvv9MMPP2j48OG3tW3XO05l2Lt3r2bMmKEffvhBiYmJevbZZ5WWlub0hfHEiROaN2+eXnzxxdvqT46wHnATJkywChQoYE/HxcVZkqxZs2bdcNny5ctbn376qT0dEhJiffTRR/a0JOvNN9+0p1NSUixJ1oIFC3Kk73fbzp07LUlWXFycXfbYY49ZnTp1yrJ+s2bNrP79+9vT9erVs/r27WtP/3l/LVq0yHJ1dbWOHj1qz1+wYIElyZo5c+Y1+/T+++9b1atXt6eHDh1qVa5cOVO9P7fz1VdfWQULFrRSUlLs+fPmzbNcXFys48ePW5ZlWV26dLFCQkKsK1eu2HWeffZZq3379tfsy/3mr+/9kiVLWt9++61TnREjRlh16tSxLMuyXn75ZevJJ5+00tPTs2zvRv9WD6q/vq8z/Hn/Zuf9dLN/DxnHqlOnTtl1Nm/ebEmy9u/fb1mWZUVFRVk9evRw6ldCQoLl4uJiXbhw4dY3Oodc75jSoUMH66mnnnKq/49//MMqV66cZVmWtWvXLkuStWTJkmyvLzo62mrTpo093aVLF8vPz89KTU21y7744gvLy8vLSktLsywr83HdsiyrcuXK1tChQ+3pWzlO5c2b1zpx4oRTvV69ellNmjSxpz/88EOrRIkS1/ybu5uMPTNQo0YNp+mUlBQNGDBA4eHh8vX1lZeXl3bu3HnDMwN/vuaVP39++fj46MSJE3ekz3da2bJlFRERofHjx0u6mmwTEhIUFRWltLQ0jRgxQhUrVpSfn5+8vLy0aNGiG+6fDDt37lRwcLCCgoLssjp16mSqN3XqVNWtW1cBAQHy8vLSm2++me11/HldlStXVv78+e2yunXrKj093en6efny5ZUnTx57OjAw8L79t7uR1NRUJSUlKSoqSl5eXvbr7bffVlJSkqSrp7oTExNVpkwZ9enTR4sXL87lXt9fbub9lN2/hxvZsmWLYmJinP5NIyMjlZ6erv3799/8RuSw6x1Tdu7cqbp16zrVr1u3rvbs2aO0tDQlJiYqT548qlev3jXbHzt2rKpXry5/f395eXnpq6++ynS8qFy5svLly2dP16lTRykpKTp8+PAtb1d2jlMhISGZxol0795dixcv1tGjRyVdvfTZtWtXORyOW+5LTjE2DPz5g0KSBgwYoJkzZ2rkyJFKSEhQYmKiKlas6HRtKSt58+Z1mnY4HEpPT8/x/t4tGdfxzp07pwkTJqhkyZKqV6+e3n//fX3yyScaOHCg4uLilJiYqMjIyBvun5uxdu1adezYUU2bNtXcuXO1efNm/fOf/8zRdfzZg/Zvdz0Z4yW+/vprJSYm2q/t27dr3bp1kqRq1app//79GjFihC5cuKB27dqpbdu2udnte4KPj4/OnDmTqfz06dNOPw2b0++njN+et/50iv2vY1pSUlLUs2dPp3/TLVu2aM+ePSpZsuQtrzsnXeuYciOenp7XnT9lyhQNGDBAUVFRWrx4sRITE/XCCy/c9PHCxcXFaR9Lmffzn2X3OPXXzxhJqlq1qipXrqyJEydq48aN+vnnn9W1a9eb6u+d8kDeTXArVq9era5du6p169aSrv6R/XmQjinatWunvn376ttvv9XEiRPVq1cvORwOrV69Wi1btlSnTp0kXb1et3v3bpUrVy5b7YaHh+vw4cNKTk5WYGCgJNkfQhnWrFmjkJAQ/fOf/7TLDh486FTHzc1NaWlpN1xXTEyMUlNT7T/I1atXy8XFRWXKlMlWfx80RYsWVVBQkPbt26eOHTtes56Pj4/at2+v9u3bq23btmrcuLFOnjwpPz8/5c2b94b7/kFUpkyZLM+SbNq0SaVLl76lNrPz95DxrTI5OVkFCxaUdHUA4Z9Vq1ZNO3bsUKlSpW6pH3fDtY4p4eHhWr16tVPd1atXq3Tp0sqTJ48qVqyo9PR0rVixQg0bNszU7urVqxUREaG///3vdlnGWa4/27Jliy5cuGCHi3Xr1snLy0vBwcGSru7nP9+Fc/bs2eueVcnOcep6unXrpo8//lhHjx5Vw4YN7X7kNmPPDPzVww8/bA/02LJlizp06PDAfku8Hi8vL7Vv316DBg1ScnKynVoffvhhLVmyRGvWrNHOnTvVs2dP/frrr9lut2HDhipdurS6dOmiLVu2KCEhwemPKWMdhw4d0pQpU5SUlKQxY8Zo5syZTnVCQ0O1f/9+JSYm6rfffss0MleSOnbsKA8PD3Xp0kXbt29XXFycXn75ZT3//PMqWrToze+UB8Tw4cM1atQojRkzRrt379a2bds0YcIEjR49WpI0evRofffdd/rll1+0e/duTZ8+XQEBAfa99KGhoVq2bJmOHz+uU6dO5eKW3F29evXS7t271adPH23dulW7du2y91X//v1vqc3s/D2UKlVKwcHBGjZsmPbs2aN58+ZlGm0/cOBArVmzRr1791ZiYqL27Nmj2bNn3xMDCDNc65jSv39/LVu2TCNGjNDu3bsVGxurzz77TAMGDJB09f3WpUsXvfjii5o1a5b279+v+Ph4e4Dgww8/rJ9++kmLFi3S7t27NXjw4CwHtl66dElRUVHasWOH5s+fr6FDh6p37972mZcnn3xSkyZNUkJCgrZt26YuXbo4Xe75q+wcp66nQ4cOOnLkiL7++ut7Y+Dg/0cY+P9Gjx6tggULKiIiQi1atFBkZKSqVauW293KFVFRUTp16pQiIyPta5pvvvmmqlWrpsjISNWvX18BAQE39WQvFxcXzZw5UxcuXFCtWrXUrVs3vfPOO051nn76afXr10+9e/dWlSpVtGbNGg0ePNipTps2bdS4cWM98cQT8vf3z/L2xnz58mnRokU6efKkatasqbZt26pBgwb67LPPbn5nPEC6deumb775RhMmTFDFihVVr149xcTEKCwsTNLVEdL/+te/VKNGDdWsWVMHDhzQ/Pnz7YPmhx9+qCVLlig4OFhVq1bNzU25q0qUKKGVK1fql19+UcOGDVW7dm1NmzZN06dPv+ZDdW4kO38PefPmtcNZpUqV9N577+ntt992qlOpUiWtWLFCu3fv1mOPPaaqVatqyJAhTmMR7gVZHVOqVaumadOmacqUKapQoYKGDBmit956y+m0+RdffKG2bdvq73//u8qWLavu3bvbt0327NlTzzzzjNq3b6/atWvr999/dzpLkKFBgwZ6+OGH9fjjj6t9+/Z6+umnnW4bHDRokOrVq6fmzZurWbNmatWq1XUvsWTnOHU9BQoUUJs2beTl5ZUjT0fMKfyEMQDcIxwOh2bOnHlPfUgg5zVo0EDly5fXmDFjcrsrNsYMAABwF5w6dUrx8fGKj493euDXvYAwAADAXVC1alWdOnVK77333j03mJnLBAAAGI4BhAAAGI4wAACA4QgDAAAYjjAAAIDhCAMAABiOMADcwzJ+0czhcChv3rwqWrSonnrqKY0fP/6mHpcdExNjP1b4buratSsP0AHuA4QB4B7XuHFjJScn68CBA1qwYIGeeOIJ9e3bV82bN9eVK1dyu3sAHgCEAeAe5+7uroCAAD300EOqVq2a3njjDc2ePVsLFixQTEyMpKu/rVGxYkXlz59fwcHB+vvf/27/bHF8fLxeeOEFnTlzxj7LkPFs9kmTJqlGjRry9vZWQECAOnTooBMnTtjrPnXqlDp27Ch/f395enrq4Ycf1oQJE+z5hw8fVrt27eTr6ys/Pz+1bNnS/rXPYcOGKTY2VrNnz7bXGx8ffzd2GYCbRBgA7kNPPvmkKleurB9++EHS1R++GTNmjH7++WfFxsZq+fLleu211yRJERER+vjjj+Xj46Pk5GQlJyfbvwx3+fJljRgxQlu2bNGsWbN04MABpx+KGTx4sHbs2KEFCxZo586d+uKLL1S4cGF72cjISHl7eyshIUGrV6+Wl5eXGjdurEuXLmnAgAFq166dfWYjOTlZERERd3dHAcgWHkcM3KfKli2rrVu3SpJeeeUVuzw0NFRvv/22XnrpJX3++edyc3NTgQIF5HA4FBAQ4NTGn39CtUSJEhozZoxq1qyplJQUeXl56dChQ6patapq1Khht51h6tSpSk9P1zfffCOHwyFJmjBhgnx9fRUfH69GjRrJ09NTFy9ezLReAPcWzgwA9ynLsuwP4aVLl6pBgwZ66KGH5O3treeff16///67zp8/f902Nm7cqBYtWqh48eLy9vZWvXr1JEmHDh2SJPXq1UtTpkxRlSpV9Nprr2nNmjX2slu2bNHevXvl7e0tLy8veXl5yc/PT3/88YeSkpLu0FYDuBMIA8B9aufOnQoLC9OBAwfUvHlzVapUSTNmzNDGjRs1duxYSdKlS5euuXxqaqoiIyPl4+OjyZMna8OGDZo5c6bTck2aNNHBgwfVr18/HTt2TA0aNLAvMaSkpKh69epKTEx0eu3evVsdOnS4w1sPICdxmQC4Dy1fvlzbtm1Tv379tHHjRqWnp+vDDz+Ui8vVfD9t2jSn+m5ubkpLS3Mq++WXX/T777/r3XffVXBwsCTpp59+yrQuf39/denSRV26dNFjjz2mf/zjH/rggw9UrVo1TZ06VUWKFJGPj0+W/cxqvQDuPZwZAO5xFy9e1PHjx3X06FFt2rRJI0eOVMuWLdW8eXN17txZpUqV0uXLl/Xpp59q3759mjRpksaNG+fURmhoqFJSUrRs2TL99ttvOn/+vIoXLy43Nzd7uTlz5mjEiBFOyw0ZMkSzZ8/W3r179fPPP2vu3LkKDw+XJHXs2FGFCxdWy5YtlZCQoP379ys+Pl59+vTRkSNH7PVu3bpVu3bt0m+//abLly/fnZ0G4OZYAO5ZXbp0sSRZkixXV1fL39/fatiwoTV+/HgrLS3Nrjd69GgrMDDQ8vT0tCIjI62JEydakqxTp07ZdV566SWrUKFCliRr6NChlmVZ1rfffmuFhoZa7u7uVp06daw5c+ZYkqzNmzdblmVZI0aMsMLDwy1PT0/Lz8/PatmypbVv3z67zeTkZKtz585W4cKFLXd3d6tEiRJW9+7drTNnzliWZVknTpywnnrqKcvLy8uSZMXFxd3pXQbgFjgsy7JyM4wAAIDcxWUCAAAMRxgAAMBwhAEAAAxHGAAAwHCEAQAADEcYAADAcIQBAAAMRxgAAMBwhAEAAAxHGAAAwHCEAQAADEcYAADAcP8PUa2cIAj8AFYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def print_dataset_info():\n",
    "    print(\"Words in 'train' dataset ........:\", len(words_train))\n",
    "    print(\"Words in 'val' dataset ..........:\", len(words_val))\n",
    "    print(\"Words in 'test' dataset .........:\", len(words_test))\n",
    "    print(\"Distinct words in 'train' dataset:\", len(set(words_train)))\n",
    "    print(\"Words in vocabulary .............:\", VOCABULARY_SIZE)\n",
    "\n",
    "    bars = plt.bar(['Train', 'Validation', 'Test', 'Unique', 'Vocabulary'], [len(words_train), len(words_val), len(words_test), len(set(words_train)), VOCABULARY_SIZE])\n",
    "    plt.title('Word counts')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Word count')\n",
    "    plt.gca().yaxis.set_visible(False)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, yval, ha='center', va='bottom')\n",
    "    plt.savefig(DOCS_DIR + 'word_counts.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "print_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "This section contains som utilites which come in handy for all the next assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nameof(\n",
    "    model: nn.Module, \n",
    "    criterion: object, \n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Creates a good name for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    name = f'{model.__class__.__name__}_{criterion.__class__.__name__}_{optimizer.__class__.__name__}'\n",
    "    options = optimizer.param_groups[0]\n",
    "\n",
    "    if 'lr' in options:\n",
    "        name += f'-lr{options[\"lr\"]:.4f}'\n",
    "\n",
    "    if 'momentum' in options and options['momentum'] != 0.0:\n",
    "        name += f'-m{options[\"momentum\"]:.4f}'\n",
    "\n",
    "    if 'weight_decay' in options and options['weight_decay'] != 0.0:\n",
    "        name += f'-wd{options[\"weight_decay\"]:.4f}'\n",
    "\n",
    "    return name\n",
    "\n",
    "def model_save(model: nn.Module, folder: str | None = None):\n",
    "    \"\"\"\n",
    "    Save the given model to a file.\n",
    "    \"\"\"\n",
    "\n",
    "    folder = '' if folder is None else folder + '/'\n",
    "    filename = DATA_DIR + f'{folder}{model.name}.pt'\n",
    "\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f'Saved {model.name} ({filename})')\n",
    "\n",
    "def model_load(model: nn.Module, folder: str | None = None) -> bool:\n",
    "    \"\"\"\n",
    "    Save the given model to a file.\n",
    "\n",
    "    Returns `True` if the model was loaded, `False` otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    if FORCE_RETRAIN:\n",
    "        return False\n",
    "\n",
    "    folder = '' if folder is None else folder + '/'\n",
    "    filename = DATA_DIR + f'{folder}{model.name}.pt'\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        return False\n",
    "    \n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    print(f'Loaded {model.name} ({filename}')\n",
    "    return True\n",
    "\n",
    "def dataset_create(\n",
    "    words: list[str],\n",
    "    context_size_before: int = 0,\n",
    "    context_size_after: int = 0,\n",
    "    vocabulary_index_to_target: dict[int, int] = {},\n",
    "    dataset_name: str | None = None\n",
    ") -> TensorDataset:\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert in case they are floats\n",
    "    context_size_before = int(context_size_before)\n",
    "    context_size_after = int(context_size_after)\n",
    "\n",
    "    filename = DATA_DIR + f'dataset/{dataset_name}.pt'\n",
    "    if os.path.exists(filename) and dataset_name is not None and not FORCE_RETRAIN:\n",
    "        return torch.load(filename)\n",
    "\n",
    "    word_idx = [vocabulary[word] for word in words]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(context_size_before, len(words) - context_size_after - 1):\n",
    "\n",
    "        context_before = word_idx[i-context_size_before:i]\n",
    "        context_after = word_idx[i+1:i+1+context_size_after]\n",
    "        context = context_before + context_after\n",
    "        target = word_idx[i+context_size_before]\n",
    "        target = vocabulary_index_to_target.get(target, None)\n",
    "\n",
    "        if target is not None:\n",
    "            contexts.append(torch.tensor(context))\n",
    "            targets.append(target)\n",
    "\n",
    "    contexts = torch.stack(contexts).to(device)\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "\n",
    "    dataset = TensorDataset(contexts, targets)\n",
    "    torch.save(dataset, filename)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def model_train(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    criterion: typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    tranform_targets: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    tranform_contexts: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    model_category: str = \"models\",\n",
    "    retrain: bool = False,\n",
    "    figure_tag: str = \"\"\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Trains the given `model` with the given `dataset`.  \n",
    "\n",
    "    dataset: The dataset to train the model with.\n",
    "    model: The model to train.\n",
    "    criterion: The loss function to use.\n",
    "    optimizer: The optimizer to use.\n",
    "    batch_size: The batch size to use.\n",
    "    epochs: The number of epochs to train.\n",
    "    force_retrain: If `True`, the model will be trained even if it has been trained before.\n",
    "    tranform_targets: A function to transform the targets before they are passed to `criterion` along with the `model` output.\n",
    "    tranform_contexts: A function to transform the contexts before they are passed to `model`.\n",
    "    model_category: The category of the model. If `None`, the model's class name will be used.\n",
    "    \"\"\"\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Name the model for easier referencing\n",
    "    model.name = model_nameof(model, criterion, optimizer)\n",
    "\n",
    "    # If the model has already been trained,\n",
    "    # and we are not forcing a retrain:\n",
    "    #     Load the trained model and return\n",
    "    if (not retrain) and model_load(model, model_category):\n",
    "        return []\n",
    "    \n",
    "    # Prepare a data loader for the given dataset.\n",
    "    # Ensure the data is shuffled.\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(f'Training {model.name}...')\n",
    "\n",
    "    losses = []\n",
    "    dataset_size = dataset.tensors[0].size(0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = torch.tensor([0.0]).to(device)\n",
    "\n",
    "        for contexts, targets in data_loader:\n",
    "\n",
    "            # Perform transformations\n",
    "            contexts = tranform_contexts(contexts).to(device)\n",
    "            targets = tranform_targets(targets).to(device)\n",
    "\n",
    "            # Perform a training step\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts).to(device)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss\n",
    "\n",
    "        loss_sum = loss_sum.item() / dataset_size\n",
    "        losses.append(loss_sum)\n",
    "        print(f'Training | {model.name} | Epoch {epoch} | Loss {loss_sum}')\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(losses)\n",
    "    plt.title(f'{model.name} | Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(DOCS_DIR + f'loss_{model.name}{figure_tag}.png')\n",
    "\n",
    "    # Save the model so we can skip training every time.\n",
    "    model_save(model, model_category)\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def model_accuracy(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation',\n",
    "    transform_contexts: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    transform_outputs: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the given model on the given dataset.\n",
    "\n",
    "    Returns the accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_loader = DataLoader(dataset, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for contexts, targets in data_loader:\n",
    "        contexts = transform_contexts(contexts).to(device)\n",
    "        outputs = model(contexts)\n",
    "        outputs = transform_outputs(outputs)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (outputs == targets).sum().item()\n",
    "\n",
    "    print(f'{dataset_name} | {model.name} | Accuracy {correct/total:.4f}')\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def model_pick_best(\n",
    "    models: list[nn.Module],\n",
    "    dataset: TensorDataset,\n",
    "    performance_measure: typing.Callable[[nn.Module, TensorDataset], float],\n",
    "    figure_tag: str = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Pick the best model from the given list of `models` on a given `dataset`.\n",
    "    \"\"\"\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    accuracies: dict[str, float] = {}\n",
    "\n",
    "    for model in models:\n",
    "        accuracy = performance_measure(model, dataset)\n",
    "        accuracies[model.name] = accuracy\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "\n",
    "    print(f'Best model: {best_model.name} | Accuracy {best_accuracy:.4f}')\n",
    "\n",
    "    colors = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.BASE_COLORS.values()) + list(mcolors.CSS4_COLORS.values())\n",
    "    bars = plt.bar(accuracies.keys(), accuracies.values(), color=colors[:len(accuracies)])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.gca().xaxis.set_visible(False)\n",
    "    plt.legend(bars, accuracies.keys())\n",
    "    plt.gca().yaxis.set_visible(False)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.4f}', ha='center', va='bottom')\n",
    "    plt.savefig(DOCS_DIR + f'accuracy{figure_tag}.png')\n",
    "\n",
    "    return best_model, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "This section contains the training and selecting of the best performing embeddings using `CBOW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIM = 32\n",
    "EMBEDDINGS_CONTEXT_SIZE = 5\n",
    "EMBEDDINGS_BATCH_SIZE = 128\n",
    "EMBEDDINGS_EPOCHS = 100\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(VOCABULARY_SIZE, EMBEDDINGS_DIM, sparse=True)\n",
    "        self.linear = nn.Linear(EMBEDDINGS_DIM*EMBEDDINGS_CONTEXT_SIZE, VOCABULARY_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "def cbow_create_dataset(\n",
    "    words: list[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "    return dataset_create(words, EMBEDDINGS_CONTEXT_SIZE, dataset_name='cbow.' + dataset_name)\n",
    "\n",
    "def cbow_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "):\n",
    "    return model_train(\n",
    "        model, dataset, criterion, optimizer, \n",
    "        model_category='embeddings',\n",
    "        epochs=EMBEDDINGS_EPOCHS, batch_size=EMBEDDINGS_BATCH_SIZE,\n",
    "        tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=VOCABULARY_SIZE).float()\n",
    "    )\n",
    "\n",
    "def cbow_performance(\n",
    "    model: CBOW,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation'\n",
    "):\n",
    "    return model_accuracy(\n",
    "        model, dataset, dataset_name,\n",
    "        transform_outputs=lambda x: torch.argmax(x, dim=1)\n",
    "    )\n",
    "\n",
    "def cbow_create_embeddings() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create multiple embeddings models and pick the best one.  \n",
    "\n",
    "    Returns the embeddings of the best model.\n",
    "    \"\"\"\n",
    "    training_data = cbow_create_dataset(words_train, 'train')\n",
    "\n",
    "    m1 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m1,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m1.parameters(), lr=0.02)\n",
    "    )\n",
    "\n",
    "    m2 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m2,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m2.parameters(), lr=0.01)\n",
    "    )\n",
    "    \n",
    "    m3 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m3,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m3.parameters(), lr=0.001)\n",
    "    )\n",
    "    \n",
    "    m4 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m4,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.Adam(m4.parameters(), lr=0.02)\n",
    "    )\n",
    "    \n",
    "    m5 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m5,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.Adam(m5.parameters(), lr=0.002)\n",
    "    )\n",
    "\n",
    "    validation_data = cbow_create_dataset(words_val, 'val')\n",
    "    best_model, best_model_accuracy = model_pick_best(\n",
    "        [m1, m2, m3, m4, m5],\n",
    "        dataset = validation_data,\n",
    "        performance_measure=cbow_performance,\n",
    "        figure_tag='cbow'\n",
    "    )\n",
    "\n",
    "    return best_model.embeddings.weight.detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = cbow_create_embeddings()\n",
    "embeddings = torch.load(DATA_DIR + 'embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insepecting the embeddings\n",
    "In this section we try to understand the embeddings we created in the previous section.  \n",
    "We will identify which words the model believes are similar and take a look at the embeddings using the `Tensorflow Projector` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_similarity_cosine(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return torch.dot(word_a, word_b) / (word_a.norm() * word_b.norm())\n",
    "\n",
    "def word_vector_similarity_euclidian(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return (word_a - word_b).norm()\n",
    "\n",
    "def word_similarity_cosine(word_a:str, word_b:str):\n",
    "    word_a_idx = vocabulary[word_a]\n",
    "    word_b_idx = vocabulary[word_b]\n",
    "\n",
    "    word_a_embedding = embeddings[word_a_idx]\n",
    "    word_b_embedding = embeddings[word_b_idx]\n",
    "\n",
    "    return word_vector_similarity_cosine(word_a_embedding, word_b_embedding)\n",
    "\n",
    "def word_find_top_closest(\n",
    "    word: str,\n",
    "    top: int\n",
    "):\n",
    "    similarities = []\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        similarity = word_similarity_cosine(word, other).item()\n",
    "        similarities.append((other, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    similarities = similarities[1:top+1]\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def word_find_closest(\n",
    "    word_vector:torch.Tensor,\n",
    "):\n",
    "    closest_word = None\n",
    "    closest_distance = 1_000_000\n",
    "\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        other_idx = vocabulary[other]\n",
    "        other_embedding = embeddings[other_idx]\n",
    "\n",
    "        distance = word_vector_similarity_euclidian(word_vector, other_embedding)\n",
    "\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_word = other\n",
    "    \n",
    "    return closest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_words(words, top = 10):\n",
    "    print(f\"Top {top} most similar words\")\n",
    "    for word in words:\n",
    "        if vocabulary[word] == vocabulary['<unk>']:\n",
    "            print(word, ':', \"Not in vocabulary\")\n",
    "        else:\n",
    "            print(word, ':', [x[0] for x in word_find_top_closest(word, top)])\n",
    "\n",
    "print_most_similar_words([\n",
    "    'king', 'queen', 'man', 'woman', 'he', 'she', 'doctor', 'nurse',\n",
    "    'black', 'white', 'slave', 'master',\n",
    "    'poor', 'rich', \n",
    "    'smart', 'dumb', \n",
    "    'strong', 'weak',\n",
    "    'good', 'bad',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_projector_create_data():\n",
    "    e = embeddings.cpu().numpy()\n",
    "    e = pd.DataFrame(e)\n",
    "    e.to_csv(DATA_DIR + 'tensorflow_projector/embeddings.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "    v = vocabulary.lookup_tokens(range(len(vocabulary)))\n",
    "    v = pd.DataFrame(v)\n",
    "    v.to_csv(DATA_DIR + 'tensorflow_projector/vocabulary.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "tensorflow_projector_create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugating _be_ and _have_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training be/have models\n",
      "Context size: 0 torch.Size([124029, 20])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00030469634901256714\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.000259993937102632\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.0002048152633888562\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00014921204240221636\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00010215866555996827\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 1 torch.Size([124029, 18])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00028144739173745404\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0002505287489231979\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00023220736653515917\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00021899531453888494\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.0002097055921610004\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 2 torch.Size([124029, 16])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.0002869646762732594\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00026841315841456435\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.0002605329446700564\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0002556791546134671\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00025094719103188883\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 3 torch.Size([124029, 14])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.0002766813960946287\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00026981697709644453\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.0002662291089783091\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0002647274231306917\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.0002636223115241225\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 4 torch.Size([124029, 12])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00026811515873862856\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00026725711405297916\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.0002654555216267623\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0002647405561538752\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.0002643024298886563\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 5 torch.Size([124029, 10])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00026656121760672844\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0002662499311251363\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.0002656987439858603\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0002652783949745057\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00026510526666654384\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 6 torch.Size([124029, 8])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00026753275375735906\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00026738985908356425\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00026715438736109917\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0002670798028617551\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00026673551466148415\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 7 torch.Size([124029, 6])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00026945528072024424\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0002689477678032645\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.0002691443940730816\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0002688152995811306\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00026872078487330205\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 8 torch.Size([124029, 4])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00027249448373165876\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0002718654519046138\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.0002713803912286737\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00027142166644439315\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.0002715615777382142\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Validating be/have models\n",
      "Validation | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.6447\n",
      "Best model: BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.6447\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BEHAVE_CONTEXT_SIZE = 20\n",
    "BEHAVE_BATCH_SIZE = 8192\n",
    "BEHAVE_EPOCHS = 5\n",
    "BEHAVE_WORDS = ['<unk>', 'be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']\n",
    "BEHAVE_WORDS_SIZE = len(BEHAVE_WORDS)\n",
    "\n",
    "class BeHaveRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BeHaveRNN, self).__init__()\n",
    "\n",
    "        # self.rnn = nn.RNN(EMBEDDINGS_DIM*BEHAVE_CONTEXT_SIZE, EMBEDDINGS_DIM, batch_first=True).to(device)\n",
    "        self.rnn = nn.RNN(EMBEDDINGS_DIM, EMBEDDINGS_DIM, batch_first=True).to(device)\n",
    "        self.fc1 = nn.Linear(EMBEDDINGS_DIM, BEHAVE_WORDS_SIZE * 4).to(device)\n",
    "        self.fc2 = nn.Linear(BEHAVE_WORDS_SIZE * 4, BEHAVE_WORDS_SIZE).to(device)\n",
    "        self.hidden = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = embeddings[x]\n",
    "\n",
    "        # Reset the state if its incompatible with current input\n",
    "        if self.hidden is not None and self.hidden.size(1) != x.size(0):\n",
    "            self.reset()\n",
    "\n",
    "        x, hidden = self.rnn(x, self.hidden)\n",
    "        x = x[:, -1, :] # Keep only the last output\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        self.hidden = hidden.data\n",
    "\n",
    "        return x\n",
    "\n",
    "class BeHaveAlways(nn.Module):\n",
    "    def __init__(self, word):\n",
    "        super(BeHaveAlways, self).__init__()\n",
    "        self.name = \"Always_\" + word\n",
    "        self.label = BEHAVE_WORDS.index(word)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.zeros(x.size(0), BEHAVE_WORDS_SIZE)\n",
    "        x[:, self.label] = 1.0\n",
    "        return x.to(device)\n",
    "\n",
    "class BeHaveMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BeHaveMLP, self).__init__()\n",
    "        FC2_SCALE = 16\n",
    "        FC3_SCALE = 4\n",
    "\n",
    "        self.fc1 = nn.Linear(EMBEDDINGS_DIM*BEHAVE_CONTEXT_SIZE, EMBEDDINGS_DIM).to(device)\n",
    "        self.fc2 = nn.Linear(EMBEDDINGS_DIM, BEHAVE_WORDS_SIZE * FC2_SCALE).to(device)\n",
    "        self.fc3 = nn.Linear(BEHAVE_WORDS_SIZE * FC2_SCALE, BEHAVE_WORDS_SIZE * FC3_SCALE).to(device)\n",
    "        self.fc4 = nn.Linear(BEHAVE_WORDS_SIZE * FC3_SCALE, BEHAVE_WORDS_SIZE).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = embeddings[x]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = torch.log_softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "\n",
    "def behave_create_dataset(\n",
    "    words: list[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.  \n",
    "    \"\"\"\n",
    "    label_to_vocabulary:dict[int, int] = {}\n",
    "\n",
    "    vocabulary_to_label = { vocabulary[word]: None for word in vocabulary.lookup_tokens(range(VOCABULARY_SIZE)) }\n",
    "\n",
    "    for label, word in enumerate(BEHAVE_WORDS):\n",
    "\n",
    "        # Skip '<unk>' to avoid bias to just guess <unk>\n",
    "        if word == '<unk>': continue\n",
    "\n",
    "        vocabulary_index = vocabulary[word]\n",
    "\n",
    "        vocabulary_to_label[vocabulary_index] = label\n",
    "        label_to_vocabulary[label] = vocabulary_index\n",
    "\n",
    "    return dataset_create(\n",
    "        words, \n",
    "        context_size_before=BEHAVE_CONTEXT_SIZE / 2, \n",
    "        context_size_after=BEHAVE_CONTEXT_SIZE / 2, \n",
    "        vocabulary_index_to_target=vocabulary_to_label, \n",
    "        dataset_name='behave.'+dataset_name\n",
    "    ), label_to_vocabulary\n",
    "\n",
    "def behave_rnn_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: BeHaveRNN,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> list[tuple[int, float]]: \n",
    "\n",
    "    model.name = model_nameof(model, criterion, optimizer)\n",
    "\n",
    "    if model_load(model, 'behave'):\n",
    "        return\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    # Train with different sequence lengths\n",
    "    for context_size in range(0, (BEHAVE_CONTEXT_SIZE // 2) - 1):\n",
    "        contexts = dataset.tensors[0][:,context_size:BEHAVE_CONTEXT_SIZE - context_size]\n",
    "        targets = dataset.tensors[1]\n",
    "\n",
    "        print(\"Context size:\", context_size, contexts.shape)\n",
    "        \n",
    "        losses.append(model_train(\n",
    "            model, TensorDataset(contexts, targets), criterion, optimizer, \n",
    "            model_category='behave',\n",
    "            epochs=BEHAVE_EPOCHS, \n",
    "            batch_size=BEHAVE_BATCH_SIZE,\n",
    "            tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=BEHAVE_WORDS_SIZE).float(),\n",
    "            retrain=True,\n",
    "            figure_tag=f'_context-{context_size}'\n",
    "        ))\n",
    "\n",
    "        model.reset()\n",
    "\n",
    "    plt.clf()\n",
    "    for i, loss in enumerate(losses):\n",
    "        plt.plot(loss, label=f'Context size {i+1}')\n",
    "    plt.title(f'{model.name} | Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(DOCS_DIR + f'loss_{model.name}.png')\n",
    "\n",
    "    model_save(model, 'behave')\n",
    "\n",
    "def behave_mlp_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: BeHaveMLP,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> list[tuple[int, float]]: \n",
    "    \n",
    "    model_train(\n",
    "        model, dataset, \n",
    "        criterion, \n",
    "        optimizer,\n",
    "        model_category='behave',\n",
    "        epochs=BEHAVE_EPOCHS, batch_size=BEHAVE_BATCH_SIZE,\n",
    "        tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=BEHAVE_WORDS_SIZE).float()\n",
    "    )\n",
    "\n",
    "def behave_performance(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation'\n",
    "):\n",
    "    return model_accuracy(\n",
    "        model, dataset, dataset_name,\n",
    "        transform_outputs=lambda x: torch.argmax(x, dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def behave_create_model():\n",
    "\n",
    "    print(\"Training be/have models\")\n",
    "    training_data, label_to_vocabulary = behave_create_dataset(words_train, 'train')\n",
    "\n",
    "    rnn1 = BeHaveRNN()\n",
    "    behave_rnn_train(\n",
    "        training_data, rnn1, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(rnn1.parameters(), lr=0.001)\n",
    "    )\n",
    "    \n",
    "    # rnn2 = BeHaveRNN()\n",
    "    # behave_rnn_train(\n",
    "    #     training_data, rnn2, \n",
    "    #     nn.CrossEntropyLoss(), \n",
    "    #     torch.optim.Adam(rnn2.parameters(), lr=0.002)\n",
    "    # )\n",
    "\n",
    "    # rnn3 = BeHaveRNN()\n",
    "    # behave_rnn_train(\n",
    "    #     training_data, rnn3, \n",
    "    #     nn.CrossEntropyLoss(), \n",
    "    #     torch.optim.Adam(rnn3.parameters(), lr=0.003)\n",
    "    # )\n",
    "\n",
    "    # rnn4 = BeHaveRNN()\n",
    "    # behave_rnn_train(\n",
    "    #     training_data, rnn4, \n",
    "    #     nn.CrossEntropyLoss(), \n",
    "    #     torch.optim.SGD(rnn4.parameters(), lr=0.001)\n",
    "    # )\n",
    "\n",
    "    # rnn5 = BeHaveRNN()\n",
    "    # behave_rnn_train(\n",
    "    #     training_data, rnn5, \n",
    "    #     nn.CrossEntropyLoss(), \n",
    "    #     torch.optim.SGD(rnn5.parameters(), lr=0.01)\n",
    "    # )\n",
    "\n",
    "    # rnn6 = BeHaveRNN()\n",
    "    # behave_rnn_train(\n",
    "    #     training_data, rnn6, \n",
    "    #     nn.CrossEntropyLoss(), \n",
    "    #     torch.optim.Adam(rnn6.parameters(), lr=0.0001)\n",
    "    # )\n",
    "\n",
    "    # mlp1 = BeHaveMLP()\n",
    "    # behave_mlp_train(\n",
    "    #     training_data, mlp1,\n",
    "    #     nn.CrossEntropyLoss(), \n",
    "    #     torch.optim.Adam(mlp1.parameters(), lr=0.001)\n",
    "    # )\n",
    "\n",
    "    # mlp2 = BeHaveMLP()\n",
    "    # behave_mlp_train(\n",
    "    #     training_data, mlp2,\n",
    "    #     nn.CrossEntropyLoss(), \n",
    "    #     torch.optim.Adam(mlp2.parameters(), lr=0.0001)\n",
    "    # )\n",
    "\n",
    "    # mlp3 = BeHaveMLP()\n",
    "    # behave_mlp_train(\n",
    "    #     training_data, mlp3,\n",
    "    #     nn.CrossEntropyLoss(), \n",
    "    #     torch.optim.SGD(mlp3.parameters(), lr=0.0001)\n",
    "    # )\n",
    "\n",
    "    alwaysUnknown = BeHaveAlways('<unk>')\n",
    "\n",
    "    print(\"Validating be/have models\")\n",
    "\n",
    "    validation_data, _ = behave_create_dataset(words_val, 'val')\n",
    "\n",
    "    best_model, validation_accuracy = model_pick_best(\n",
    "        # [rnn1, rnn2, rnn3, rnn4, rnn5, rnn6, mlp1, mlp2, mlp3, alwaysUnknown], \n",
    "        [rnn1], \n",
    "        validation_data, \n",
    "        behave_performance,\n",
    "        figure_tag='_behave'\n",
    "    )\n",
    "\n",
    "    # test_data, _ = behave_create_dataset(words_test, 'test')\n",
    "    # test_accuracy = behave_performance(best_model, test_data, 'Test')\n",
    "    # train_accuracy = behave_performance(best_model, training_data, 'Train')\n",
    "\n",
    "    # plt.clf()\n",
    "    # plt.bar(['Train', 'Validation', 'Test'], [train_accuracy, validation_accuracy, test_accuracy])\n",
    "    # plt.title('Be/Have Model Accuracy')\n",
    "    # plt.xlabel('Dataset')\n",
    "    # plt.ylabel('Accuracy')\n",
    "    # plt.ylim(0.0, 1.0)\n",
    "    # plt.savefig(DOCS_DIR + 'accuracy_behave_best.png')\n",
    "    # plt.show()\n",
    "\n",
    "    return best_model\n",
    "     \n",
    "\n",
    "behave_model = behave_create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.5502, -2.0161, -3.8378, -1.7402, -1.6109, -2.4095, -2.5988, -3.1170,\n",
      "         -4.3467, -2.0176, -3.4821, -2.5694, -4.7973]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0.6709, 0.1416, 0.2696, 0.1223, 0.1132, 0.1693, 0.1826, 0.2190, 0.3054,\n",
      "         0.1417, 0.2446, 0.1805, 0.3370]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward0>)\n",
      "When we | is | younger we\n"
     ]
    }
   ],
   "source": [
    "def behave_try(before: str, after:str):\n",
    "    behave_model.to(device)\n",
    "    behave_model.eval()\n",
    "    behave_model.reset()\n",
    "    \n",
    "    words_before = TOKENIZER(before)\n",
    "    words_after = TOKENIZER(after)\n",
    "\n",
    "    if len(words_before) != len(words_after):\n",
    "        print(\"Contexts must have the same length\")\n",
    "        return\n",
    "\n",
    "    context = torch.zeros(1, len(words_before) + len(words_after)).long().to(device)\n",
    "    for i, word in enumerate(words_before):\n",
    "        context[0, i] = vocabulary[word]\n",
    "    for i, word in enumerate(words_after):\n",
    "        context[0, i + len(words_before)] = vocabulary[word]\n",
    "\n",
    "    output = behave_model(context)\n",
    "    print(output)\n",
    "\n",
    "    print((output / output.norm()).abs())\n",
    "\n",
    "    output = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    word = BEHAVE_WORDS[output]\n",
    "    \n",
    "    print(f'{before} | {word} | {after}')\n",
    "\n",
    "\n",
    "# Not great\n",
    "behave_try(\"When we\", \"younger we\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
