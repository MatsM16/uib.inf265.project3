{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from torch import nn;\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "MIN_WORD_FREQUENCY = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "\n",
    "def read_lines(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads all the lines form all the texts in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scan for all input files\n",
    "    inDirectoryName = os.path.join(DATA_DIR, 'input', dataset)\n",
    "    inFileNames = [os.path.join(inDirectoryName, f) for f in os.listdir(inDirectoryName)]\n",
    "\n",
    "    # Read all the lines from all the files\n",
    "    lines = []\n",
    "    for inFileName in inFileNames:\n",
    "        with open(inFileName, 'r') as file:\n",
    "            lines += file.readlines()\n",
    "\n",
    "    print(f\"Read {len(lines)} lines from {dataset}\")\n",
    "    return lines\n",
    "\n",
    "def create_tokens(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Creates tokens for all the words in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'words.{dataset}.pt')\n",
    "    \n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded tokenized words for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    tokens = []\n",
    "    for line in read_lines(dataset):\n",
    "        tokens += TOKENIZER(line)\n",
    "\n",
    "    # Save tokens so we dont have to do this again\n",
    "    torch.save(tokens, outFileName)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_vocabulary(\n",
    "    dataset: str\n",
    ") -> Vocab:\n",
    "    \"\"\"\n",
    "    Creates a vocabulary for the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'vocabulary.pt')\n",
    "\n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded vocabulary for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    def read_sanitize_tokenize():\n",
    "\n",
    "        for line in read_lines(dataset):\n",
    "\n",
    "            line = re.sub('\\\\w*[0-9]+\\\\w*', ' ', line) # Remove numbers\n",
    "            line = re.sub('\\\\w*[A-Z]+\\\\w*', ' ', line) # Remove uppercase names\n",
    "            line = re.sub('\\\\s+', ' ', line) # Remove double spaces\n",
    "\n",
    "            yield TOKENIZER(line)\n",
    "\n",
    "    vocabulary = build_vocab_from_iterator(read_sanitize_tokenize(), min_freq=MIN_WORD_FREQUENCY, specials=['<unk>'])\n",
    "\n",
    "    vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "    # We removed all uppercase names, this includes 'I'\n",
    "    vocabulary.append_token('i') \n",
    "\n",
    "    # Save vocabulary so we dont have to do this again\n",
    "    torch.save(vocabulary, outFileName)\n",
    "\n",
    "    return vocabulary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized words for train (./data/words.train.pt)\n",
      "Loaded tokenized words for val (./data/words.val.pt)\n",
      "Loaded tokenized words for test (./data/words.test.pt)\n",
      "Loaded vocabulary for train (./data/vocabulary.pt)\n"
     ]
    }
   ],
   "source": [
    "words_train = create_tokens('train')\n",
    "words_val = create_tokens('val')\n",
    "words_test = create_tokens('test')\n",
    "\n",
    "vocabulary = create_vocabulary('train')\n",
    "VOCABULARY_SIZE = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in 'train' dataset ........: 2684706\n",
      "Words in 'val' dataset ..........: 49526\n",
      "Words in 'test' dataset .........: 124152\n",
      "Distinct words in 'train' dataset: 52105\n",
      "Words in vocabulary .............: 1880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Words in 'train' dataset ........:\", len(words_train))\n",
    "print(\"Words in 'val' dataset ..........:\", len(words_val))\n",
    "print(\"Words in 'test' dataset .........:\", len(words_test))\n",
    "print(\"Distinct words in 'train' dataset:\", len(set(words_train)))\n",
    "print(\"Words in vocabulary .............:\", VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CBOW_CrossEntropyLoss_SGD-lr0.020 loaded from file (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.020.pt)\n",
      "Model CBOW_CrossEntropyLoss_SGD-lr0.010 loaded from file (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.010.pt)\n",
      "Model CBOW_CrossEntropyLoss_SGD-lr0.001 loaded from file (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.001.pt)\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.020 | Accuracy 0.000\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.010 | Accuracy 0.001\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.001 | Accuracy 0.001\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_DIM = 10\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDINGS_BATCH_SIZE = 128\n",
    "EMBEDDINGS_EPOCHS = 3\n",
    "# TODO: Increase embeddings_dim, context_size and epochs\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.EmbeddingBag(VOCABULARY_SIZE, EMBEDDINGS_DIM, sparse=True)\n",
    "        self.linear = nn.Linear(EMBEDDINGS_DIM, VOCABULARY_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "def cbow_create_dataset(\n",
    "    words: list[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "    words_idx = [vocabulary[word] for word in words]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(len(words) - CONTEXT_SIZE):\n",
    "        context = words_idx[i:i+CONTEXT_SIZE]\n",
    "        target = words_idx[i+CONTEXT_SIZE]\n",
    "\n",
    "        contexts.append(torch.tensor(context))\n",
    "        targets.append(target)\n",
    "\n",
    "    contexts = torch.stack(contexts).to(device)\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "def model_nameof(model: nn.Module, criterion: object, optimizer: torch.optim.Optimizer) -> str:\n",
    "    \"\"\"\n",
    "    Creates a good name for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    name = f'{model.__class__.__name__}_{criterion.__class__.__name__}_{optimizer.__class__.__name__}'\n",
    "    options = optimizer.param_groups[0]\n",
    "\n",
    "    if 'lr' in options:\n",
    "        name += f'-lr{options[\"lr\"]:.3f}'\n",
    "\n",
    "    if 'momentum' in options and options['momentum'] != 0.0:\n",
    "        name += f'-m{options[\"momentum\"]:.3f}'\n",
    "\n",
    "    if 'weight_decay' in options and options['weight_decay'] != 0.0:\n",
    "        name += f'-wd{options[\"weight_decay\"]:.3f}'\n",
    "\n",
    "    return name\n",
    "    \n",
    "def cbow_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> CBOW:\n",
    "    \"\"\"\n",
    "    Trains the given model on the given dataset.\n",
    "\n",
    "    Returns the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    model.name = model_nameof(model, criterion, optimizer)\n",
    "    filename = DATA_DIR + f'embeddings/{model.name}.pt'\n",
    "\n",
    "    if (os.path.exists(filename)):\n",
    "        print(f'Model {model.name} loaded from file ({filename})')\n",
    "        return torch.load(filename).to(device)\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=EMBEDDINGS_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f'Training {model.name}...')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(EMBEDDINGS_EPOCHS):\n",
    "\n",
    "        total_loss = torch.tensor([0.0]).to(device)\n",
    "        total_size = 0\n",
    "\n",
    "        for contexts, targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_size += len(targets)\n",
    "\n",
    "        total_loss = total_loss.item() / total_size\n",
    "        losses.append(total_loss)\n",
    "        print(f'Training | {model.name} | Epoch {epoch} | Loss {total_loss}')\n",
    "\n",
    "    torch.save(model, filename)\n",
    "    return model\n",
    "\n",
    "def cbow_eval(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the given model on the given dataset.\n",
    "\n",
    "    Returns the accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=EMBEDDINGS_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for contexts, targets in data_loader:\n",
    "        outputs = model(contexts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    print(f'Validation | {model.name} | Accuracy {correct/total:.4f}')\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def cbow_eval_and_pick_best(\n",
    "    dataset: TensorDataset,\n",
    "    models: list[nn.Module]\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models and pick the best one.\n",
    "    \"\"\"\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for model in models:\n",
    "        accuracy = cbow_eval(dataset, model)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def cbow_create_embeddings() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create multiple embeddings models and pick the best one.  \n",
    "\n",
    "    Returns the embeddings of the best model.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset = cbow_create_dataset(words_train)\n",
    "\n",
    "    m1 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m1,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m1.parameters(), lr=0.02)\n",
    "    )\n",
    "\n",
    "    m2 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m2,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m2.parameters(), lr=0.01)\n",
    "    )\n",
    "    \n",
    "    m3 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m3,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m3.parameters(), lr=0.001)\n",
    "    )\n",
    "\n",
    "    val_dataset = cbow_create_dataset(words_val)\n",
    "    best_model = cbow_eval_and_pick_best(\n",
    "        val_dataset,\n",
    "        [m1, m2, m3]\n",
    "    )\n",
    "\n",
    "    return best_model.embeddings.weights.detach().to(device)\n",
    "\n",
    "embeddings = cbow_create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('absorbed', tensor(0.8978, device='cuda:0')), ('borne', tensor(0.8677, device='cuda:0')), ('duty', tensor(0.8438, device='cuda:0')), ('gone', tensor(0.8303, device='cuda:0')), ('anything', tensor(0.8259, device='cuda:0')), ('absolute', tensor(0.8175, device='cuda:0')), ('thought', tensor(0.8108, device='cuda:0')), ('people', tensor(0.7782, device='cuda:0')), ('gay', tensor(0.7702, device='cuda:0')), ('actions', tensor(0.7661, device='cuda:0'))]\n",
      "[('presented', tensor(0.8658, device='cuda:0')), ('bulbs', tensor(0.8037, device='cuda:0')), ('fire', tensor(0.7927, device='cuda:0')), ('instant', tensor(0.7905, device='cuda:0')), ('merely', tensor(0.7626, device='cuda:0')), ('about', tensor(0.7617, device='cuda:0')), ('many', tensor(0.7599, device='cuda:0')), ('post', tensor(0.7515, device='cuda:0')), ('midst', tensor(0.7476, device='cuda:0')), ('keeping', tensor(0.7431, device='cuda:0'))]\n",
      "pride\n"
     ]
    }
   ],
   "source": [
    "def word_vector_cosine_similarity(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return torch.dot(word_a, word_b) / (word_a.norm() * word_b.norm())\n",
    "\n",
    "def word_vector_distance_similarity(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return (word_a - word_b).norm()\n",
    "\n",
    "def word_similarity(word_a:str, word_b:str, vocabulary:Vocab, embeddings:torch.Tensor):\n",
    "    word_a_idx = vocabulary[word_a]\n",
    "    word_b_idx = vocabulary[word_b]\n",
    "\n",
    "    word_a_embedding = embeddings[word_a_idx]\n",
    "    word_b_embedding = embeddings[word_b_idx]\n",
    "\n",
    "    return word_vector_cosine_similarity(word_a_embedding, word_b_embedding)\n",
    "\n",
    "def word_find_similars(\n",
    "    word: str,\n",
    "    top: int,\n",
    "    vocabulary: Vocab,\n",
    "    embeddings: torch.Tensor\n",
    "):\n",
    "    similarities = []\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        similarity = word_similarity(word, other, vocabulary, embeddings)\n",
    "        similarities.append((other, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    similarities = similarities[1:top+1]\n",
    "\n",
    "    return similarities\n",
    "\n",
    "print(word_find_similars('king', 10, vocabulary, embeddings))\n",
    "\n",
    "print(word_find_similars('queen', 10, vocabulary, embeddings))\n",
    "\n",
    "def word_closest(word_vector:torch.Tensor, vocabulary:Vocab, embeddings:torch.Tensor):\n",
    "    closest_word = None\n",
    "    closest_distance = 1_000_000\n",
    "\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        other_idx = vocabulary[other]\n",
    "        other_embedding = embeddings[other_idx]\n",
    "\n",
    "        distance = word_vector_distance_similarity(word_vector, other_embedding)\n",
    "\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_word = other\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "king_vector = embeddings[vocabulary['king']]\n",
    "he_vector = embeddings[vocabulary['he']]\n",
    "she_vector = embeddings[vocabulary['she']]\n",
    "\n",
    "print(word_closest(king_vector - he_vector + she_vector, vocabulary, embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
