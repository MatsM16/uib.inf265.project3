{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from torch import nn;\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "MIN_WORD_FREQUENCY = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "\n",
    "def read_lines(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads all the lines form all the texts in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scan for all input files\n",
    "    inDirectoryName = os.path.join(DATA_DIR, 'input', dataset)\n",
    "    inFileNames = [os.path.join(inDirectoryName, f) for f in os.listdir(inDirectoryName)]\n",
    "\n",
    "    # Read all the lines from all the files\n",
    "    lines = []\n",
    "    for inFileName in inFileNames:\n",
    "        with open(inFileName, 'r') as file:\n",
    "            lines += file.readlines()\n",
    "\n",
    "    print(f\"Read {len(lines)} lines from {dataset}\")\n",
    "    return lines\n",
    "\n",
    "def create_tokens(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Creates tokens for all the words in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'words.{dataset}.pt')\n",
    "    \n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded tokenized words for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    tokens = []\n",
    "    for line in read_lines(dataset):\n",
    "        tokens += TOKENIZER(line)\n",
    "\n",
    "    # Save tokens so we dont have to do this again\n",
    "    torch.save(tokens, outFileName)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_vocabulary(\n",
    "    dataset: str\n",
    ") -> Vocab:\n",
    "    \"\"\"\n",
    "    Creates a vocabulary for the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'vocabulary.pt')\n",
    "\n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded vocabulary for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    def read_sanitize_tokenize():\n",
    "\n",
    "        for line in read_lines(dataset):\n",
    "\n",
    "            line = re.sub('\\\\w*[0-9]+\\\\w*', ' ', line) # Remove numbers\n",
    "            line = re.sub('\\\\w*[A-Z]+\\\\w*', ' ', line) # Remove uppercase names\n",
    "            line = re.sub('\\\\s+', ' ', line) # Remove double spaces\n",
    "\n",
    "            yield TOKENIZER(line)\n",
    "\n",
    "    vocabulary = build_vocab_from_iterator(read_sanitize_tokenize(), min_freq=MIN_WORD_FREQUENCY, specials=['<unk>'])\n",
    "\n",
    "    vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "    # We removed all uppercase names, this includes 'I'\n",
    "    vocabulary.append_token('i') \n",
    "\n",
    "    # Save vocabulary so we dont have to do this again\n",
    "    torch.save(vocabulary, outFileName)\n",
    "\n",
    "    return vocabulary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized words for train (./data/words.train.pt)\n",
      "Loaded tokenized words for val (./data/words.val.pt)\n",
      "Loaded tokenized words for test (./data/words.test.pt)\n",
      "Loaded vocabulary for train (./data/vocabulary.pt)\n"
     ]
    }
   ],
   "source": [
    "words_train = create_tokens('train')\n",
    "words_val = create_tokens('val')\n",
    "words_test = create_tokens('test')\n",
    "\n",
    "vocabulary = create_vocabulary('train')\n",
    "VOCABULARY_SIZE = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in 'train' dataset ........: 2684706\n",
      "Words in 'val' dataset ..........: 49526\n",
      "Words in 'test' dataset .........: 124152\n",
      "Distinct words in 'train' dataset: 52105\n",
      "Words in vocabulary .............: 1880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Words in 'train' dataset ........:\", len(words_train))\n",
    "print(\"Words in 'val' dataset ..........:\", len(words_val))\n",
    "print(\"Words in 'test' dataset .........:\", len(words_test))\n",
    "print(\"Distinct words in 'train' dataset:\", len(set(words_train)))\n",
    "print(\"Words in vocabulary .............:\", VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW_CrossEntropyLoss_SGD-lr0.020...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 0 | Loss 0.04200461941571892\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 1 | Loss 0.03920111820645949\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 2 | Loss 0.038677689340451696\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 3 | Loss 0.03839519163400319\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 4 | Loss 0.038198763218883595\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 5 | Loss 0.03804670077785199\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 6 | Loss 0.0379212183451714\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 7 | Loss 0.037813812878976095\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 8 | Loss 0.03771976434712841\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 9 | Loss 0.037634661178283915\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 10 | Loss 0.037557615820160234\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 11 | Loss 0.037487376969539625\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 12 | Loss 0.03742200365143083\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 13 | Loss 0.037361382375542006\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 14 | Loss 0.03730420363850574\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 15 | Loss 0.03725100579170641\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 16 | Loss 0.03720056081198614\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 17 | Loss 0.037152804679180286\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 18 | Loss 0.037107588982907225\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 19 | Loss 0.03706488753309959\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 20 | Loss 0.03702427546866485\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 21 | Loss 0.036984885607373036\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 22 | Loss 0.03694780338201535\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 23 | Loss 0.03691229547070605\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 24 | Loss 0.03687815235290634\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 25 | Loss 0.036845598099192425\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 26 | Loss 0.03681393139776087\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 27 | Loss 0.036783795360265444\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 28 | Loss 0.0367542325942442\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 29 | Loss 0.0367262499622863\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 30 | Loss 0.03669901520225157\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 31 | Loss 0.03667291243512778\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 32 | Loss 0.03664761574007683\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 33 | Loss 0.036623037816874204\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 34 | Loss 0.036598835284636914\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 35 | Loss 0.036576017915961594\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 36 | Loss 0.0365535235581169\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 37 | Loss 0.036531791622232794\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 38 | Loss 0.036510854118391584\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 39 | Loss 0.03649011740506671\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 40 | Loss 0.03647032935418134\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 41 | Loss 0.0364508643141266\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 42 | Loss 0.036431684454805206\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 43 | Loss 0.036413290297504265\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 44 | Loss 0.03639551597179723\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 45 | Loss 0.03637796280665892\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 46 | Loss 0.03636068900223898\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 47 | Loss 0.03634392153912112\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 48 | Loss 0.03632799215815839\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 49 | Loss 0.03631185325665689\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 50 | Loss 0.03629625270653976\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 51 | Loss 0.03628087622699883\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 52 | Loss 0.036266343649628024\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 53 | Loss 0.03625173250205516\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 54 | Loss 0.036237380345148305\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 55 | Loss 0.036223307548959825\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 56 | Loss 0.03620978765419315\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 57 | Loss 0.036196721720593836\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 58 | Loss 0.036183338596178866\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 59 | Loss 0.03617050546317821\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 60 | Loss 0.0361576985202449\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 61 | Loss 0.03614543574871094\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 62 | Loss 0.03613324281735657\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 63 | Loss 0.036121678447618565\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 64 | Loss 0.036109534986391405\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 65 | Loss 0.0360979706166534\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 66 | Loss 0.03608688639815011\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 67 | Loss 0.036075898209893764\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 68 | Loss 0.036064691771076185\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 69 | Loss 0.03605443108469062\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 70 | Loss 0.036043506916598904\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 71 | Loss 0.036033447020729685\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 72 | Loss 0.03602287496354343\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 73 | Loss 0.03601295765804088\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 74 | Loss 0.03600311019271792\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 75 | Loss 0.03599348679797117\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 76 | Loss 0.03598415731398021\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 77 | Loss 0.03597492095022872\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 78 | Loss 0.035964986184681275\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 79 | Loss 0.03595589823131142\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 80 | Loss 0.03594708672865247\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 81 | Loss 0.03593785036490097\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 82 | Loss 0.035929193092638624\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 83 | Loss 0.03592072788087016\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 84 | Loss 0.035912125898749994\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 85 | Loss 0.035903713067116226\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 86 | Loss 0.035895646526372955\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 87 | Loss 0.03588729189488885\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 88 | Loss 0.035879210804108164\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 89 | Loss 0.03587100167299822\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 90 | Loss 0.035863351263325044\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 91 | Loss 0.03585564556350968\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 92 | Loss 0.035847855473477304\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 93 | Loss 0.03584026908396876\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 94 | Loss 0.03583309009550784\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 95 | Loss 0.03582557645618637\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 96 | Loss 0.03581822577728395\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 97 | Loss 0.035810968218620995\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 98 | Loss 0.03580400166070635\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.020 | Epoch 99 | Loss 0.03579698272265701\n",
      "Training CBOW_CrossEntropyLoss_SGD-lr0.010...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 0 | Loss 0.044028279359600936\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 1 | Loss 0.04014509844392355\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 2 | Loss 0.039381806391102774\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 3 | Loss 0.03899940812775799\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 4 | Loss 0.03875712672472652\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 5 | Loss 0.03858206649455563\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 6 | Loss 0.038444574460992115\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 7 | Loss 0.03833086591858833\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 8 | Loss 0.038233825899047974\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 9 | Loss 0.03814906029107152\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 10 | Loss 0.03807317020591865\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 11 | Loss 0.03800462206964574\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 12 | Loss 0.03794180955812212\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 13 | Loss 0.03788383347903547\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 14 | Loss 0.03783054251199668\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 15 | Loss 0.037779803621520607\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 16 | Loss 0.03773258584009914\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 17 | Loss 0.03768787939513562\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 18 | Loss 0.037645404925911674\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 19 | Loss 0.03760528465274159\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 20 | Loss 0.037566468062923956\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 21 | Loss 0.03752980778865132\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 22 | Loss 0.037494372627529096\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 23 | Loss 0.03746047977037294\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 24 | Loss 0.0374283270976917\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 25 | Loss 0.03739650325585605\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 26 | Loss 0.03736614023777694\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 27 | Loss 0.03733710127310266\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 28 | Loss 0.03730875198020189\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 29 | Loss 0.037281141829201836\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 30 | Loss 0.037254288280147395\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 31 | Loss 0.03722859873408622\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 32 | Loss 0.03720336605919989\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 33 | Loss 0.03717891617632653\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 34 | Loss 0.03715505993497972\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 35 | Loss 0.037131867175339076\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 36 | Loss 0.037109544507935896\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 37 | Loss 0.037087923152336146\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 38 | Loss 0.03706635126686361\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 39 | Loss 0.03704547778318703\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 40 | Loss 0.037025000060528154\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 41 | Loss 0.037005174179545505\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 42 | Loss 0.036985976860179216\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 43 | Loss 0.03696727715209254\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 44 | Loss 0.03694874331443241\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 45 | Loss 0.03693059650776753\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 46 | Loss 0.03691257192141695\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 47 | Loss 0.036895568747692946\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 48 | Loss 0.036879054455226115\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 49 | Loss 0.03686184467097081\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 50 | Loss 0.036845758149603995\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 51 | Loss 0.03682989569881339\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 52 | Loss 0.03681433006878606\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 53 | Loss 0.0367992009398812\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 54 | Loss 0.03678423768140288\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 55 | Loss 0.03676951886355315\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 56 | Loss 0.03675504157632451\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 57 | Loss 0.03674079417968705\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 58 | Loss 0.036726779583648235\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 59 | Loss 0.03671338190919585\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 60 | Loss 0.036699896934518965\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 61 | Loss 0.03668650217007406\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 62 | Loss 0.03667342459644482\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 63 | Loss 0.03666097849443942\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 64 | Loss 0.03664802314112447\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 65 | Loss 0.03663553920902179\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 66 | Loss 0.03662331135757762\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 67 | Loss 0.036611907038251186\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 68 | Loss 0.03659993235745806\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 69 | Loss 0.036588004236784655\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 70 | Loss 0.036576661027615365\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 71 | Loss 0.036565373108588255\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 72 | Loss 0.0365543179901598\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 73 | Loss 0.036543597522591904\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 74 | Loss 0.036532856684971624\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 75 | Loss 0.03652221478760577\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 76 | Loss 0.036511412839828346\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 77 | Loss 0.03650118125351762\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 78 | Loss 0.036491016597379\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 79 | Loss 0.036480901411367596\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 80 | Loss 0.03647119071639635\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 81 | Loss 0.036460999870190386\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 82 | Loss 0.03645150451577289\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 83 | Loss 0.03644188112102614\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 84 | Loss 0.03643263893725968\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 85 | Loss 0.03642335019337349\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 86 | Loss 0.03641407308951723\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 87 | Loss 0.036405389627187534\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 88 | Loss 0.03639624056366053\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 89 | Loss 0.03638715261029068\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 90 | Loss 0.03637850988806575\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 91 | Loss 0.036370053406319734\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 92 | Loss 0.03636138740403494\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 93 | Loss 0.036352660291592995\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 94 | Loss 0.036344663591029315\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 95 | Loss 0.036335959758647236\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 96 | Loss 0.03632796014807608\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 97 | Loss 0.036320103127871595\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 98 | Loss 0.03631234504792154\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.010 | Epoch 99 | Loss 0.03630415046684901\n",
      "Training CBOW_CrossEntropyLoss_SGD-lr0.001...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 0 | Loss 0.05264271631179785\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 1 | Loss 0.04694781420724319\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 2 | Loss 0.044956207995787985\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 3 | Loss 0.043777908135766326\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 4 | Loss 0.04294035560198324\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 5 | Loss 0.04232295095431484\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 6 | Loss 0.041853997428391465\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 7 | Loss 0.0414869610945502\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 8 | Loss 0.04119219479655276\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 9 | Loss 0.0409494623423614\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 10 | Loss 0.04074598879912512\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 11 | Loss 0.04057178993116924\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 12 | Loss 0.04042099625340029\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 13 | Loss 0.04028764516048528\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 14 | Loss 0.040169362086317995\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 15 | Loss 0.040062835442382594\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 16 | Loss 0.03996681392546134\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 17 | Loss 0.03987899280962759\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 18 | Loss 0.039798568932815984\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 19 | Loss 0.03972482934319315\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 20 | Loss 0.03965673516808762\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 21 | Loss 0.03959330282497008\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 22 | Loss 0.03953487569472355\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 23 | Loss 0.03948004824373366\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 24 | Loss 0.03942861968148408\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 25 | Loss 0.039380520167795224\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 26 | Loss 0.039335016380781324\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 27 | Loss 0.03929250117145261\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 28 | Loss 0.039252183017773674\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 29 | Loss 0.03921402117963974\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 30 | Loss 0.03917759952598073\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 31 | Loss 0.03914318286747761\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 32 | Loss 0.03911041618321742\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 33 | Loss 0.039078985192392\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 34 | Loss 0.03904889571501631\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 35 | Loss 0.03902038928171145\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 36 | Loss 0.03899271511054676\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 37 | Loss 0.03896648430309371\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 38 | Loss 0.03894118760804276\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 39 | Loss 0.03891749432711501\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 40 | Loss 0.038893769036104954\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 41 | Loss 0.038871082617766374\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 42 | Loss 0.03884925174162784\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 43 | Loss 0.03882847137819072\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 44 | Loss 0.03880767646471618\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 45 | Loss 0.03878801936416756\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 46 | Loss 0.038769025745325085\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 47 | Loss 0.038750244557028884\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 48 | Loss 0.03873260700167356\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 49 | Loss 0.03871472500568965\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 50 | Loss 0.03869768982188333\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 51 | Loss 0.038680925268772946\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 52 | Loss 0.038665013347855125\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 53 | Loss 0.03864934004755092\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 54 | Loss 0.03863388499780795\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 55 | Loss 0.03861885480915752\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 56 | Loss 0.03860417091139758\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 57 | Loss 0.03858977219437099\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 58 | Loss 0.038575711038212446\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 59 | Loss 0.03856253743433626\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 60 | Loss 0.03854898261947978\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 61 | Loss 0.03853595160597027\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 62 | Loss 0.03852292059246076\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 63 | Loss 0.038510040899340375\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 64 | Loss 0.03849808658859962\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 65 | Loss 0.038485826727073146\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 66 | Loss 0.038473881146354846\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 67 | Loss 0.03846182498535219\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 68 | Loss 0.03845097356744755\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 69 | Loss 0.03843917639711089\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 70 | Loss 0.038428322069198766\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 71 | Loss 0.03841741536115195\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 72 | Loss 0.038406354422708525\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 73 | Loss 0.03839602389614337\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 74 | Loss 0.03838561188936868\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 75 | Loss 0.03837517369252665\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 76 | Loss 0.0383650672365377\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 77 | Loss 0.03835553696203041\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 78 | Loss 0.03834545960611629\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 79 | Loss 0.0383356005007634\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 80 | Loss 0.038326009116098965\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 81 | Loss 0.03831672910223522\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 82 | Loss 0.03830703295730139\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 83 | Loss 0.03829801193410365\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 84 | Loss 0.0382888716005991\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 85 | Loss 0.038279998987783\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 86 | Loss 0.03827123695525125\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 87 | Loss 0.038262373072457605\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 88 | Loss 0.038253893310651724\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 89 | Loss 0.038245084718000255\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 90 | Loss 0.03823679992669575\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 91 | Loss 0.03822847148527899\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 92 | Loss 0.038220157593899656\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 93 | Loss 0.03821241406398702\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 94 | Loss 0.03820417292279475\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 95 | Loss 0.03819607437196917\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 96 | Loss 0.03818821153174972\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 97 | Loss 0.03818031959145544\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 98 | Loss 0.03817258188155776\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.001 | Epoch 99 | Loss 0.038164762691450554\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.020 | Accuracy 0.1595\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.010 | Accuracy 0.1569\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.001 | Accuracy 0.1519\n",
      "Best model: CBOW_CrossEntropyLoss_SGD-lr0.020 | Accuracy 0.15950808747803963\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EmbeddingBag' object has no attribute 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 196\u001b[0m\n\u001b[1;32m    189\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m cbow_eval_and_pick_best(\n\u001b[1;32m    190\u001b[0m         val_dataset,\n\u001b[1;32m    191\u001b[0m         [m1, m2, m3]\n\u001b[1;32m    192\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 196\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcbow_create_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 194\u001b[0m, in \u001b[0;36mcbow_create_embeddings\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m cbow_create_dataset(words_val)\n\u001b[1;32m    189\u001b[0m best_model \u001b[38;5;241m=\u001b[39m cbow_eval_and_pick_best(\n\u001b[1;32m    190\u001b[0m     val_dataset,\n\u001b[1;32m    191\u001b[0m     [m1, m2, m3]\n\u001b[1;32m    192\u001b[0m )\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EmbeddingBag' object has no attribute 'weights'"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_DIM = 32\n",
    "CONTEXT_SIZE = 5\n",
    "EMBEDDINGS_BATCH_SIZE = 128\n",
    "EMBEDDINGS_EPOCHS = 100\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.EmbeddingBag(VOCABULARY_SIZE, EMBEDDINGS_DIM, sparse=True)\n",
    "        self.linear = nn.Linear(EMBEDDINGS_DIM, VOCABULARY_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "def cbow_create_dataset(\n",
    "    words: list[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "    words_idx = [vocabulary[word] for word in words]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(len(words) - CONTEXT_SIZE):\n",
    "        context = words_idx[i:i+CONTEXT_SIZE]\n",
    "        target = words_idx[i+CONTEXT_SIZE]\n",
    "\n",
    "        contexts.append(torch.tensor(context))\n",
    "        targets.append(target)\n",
    "\n",
    "    contexts = torch.stack(contexts).to(device)\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "def model_nameof(model: nn.Module, criterion: object, optimizer: torch.optim.Optimizer) -> str:\n",
    "    \"\"\"\n",
    "    Creates a good name for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    name = f'{model.__class__.__name__}_{criterion.__class__.__name__}_{optimizer.__class__.__name__}'\n",
    "    options = optimizer.param_groups[0]\n",
    "\n",
    "    if 'lr' in options:\n",
    "        name += f'-lr{options[\"lr\"]:.3f}'\n",
    "\n",
    "    if 'momentum' in options and options['momentum'] != 0.0:\n",
    "        name += f'-m{options[\"momentum\"]:.3f}'\n",
    "\n",
    "    if 'weight_decay' in options and options['weight_decay'] != 0.0:\n",
    "        name += f'-wd{options[\"weight_decay\"]:.3f}'\n",
    "\n",
    "    return name\n",
    "    \n",
    "def cbow_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> CBOW:\n",
    "    \"\"\"\n",
    "    Trains the given model on the given dataset.\n",
    "\n",
    "    Returns the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    model.name = model_nameof(model, criterion, optimizer)\n",
    "    filename = DATA_DIR + f'embeddings/{model.name}.pt'\n",
    "\n",
    "    if (os.path.exists(filename)):\n",
    "        print(f'Model {model.name} loaded from file ({filename})')\n",
    "        return torch.load(filename).to(device)\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=EMBEDDINGS_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f'Training {model.name}...')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(EMBEDDINGS_EPOCHS):\n",
    "\n",
    "        total_loss = torch.tensor([0.0]).to(device)\n",
    "        total_size = 0\n",
    "\n",
    "        for contexts, targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_size += len(targets)\n",
    "\n",
    "        total_loss = total_loss.item() / total_size\n",
    "        losses.append(total_loss)\n",
    "        print(f'Training | {model.name} | Epoch {epoch} | Loss {total_loss}')\n",
    "\n",
    "    torch.save(model, filename)\n",
    "    return model\n",
    "\n",
    "def cbow_eval(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the given model on the given dataset.\n",
    "\n",
    "    Returns the accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=EMBEDDINGS_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for contexts, targets in data_loader:\n",
    "        outputs = model(contexts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    print(f'Validation | {model.name} | Accuracy {correct/total:.4f}')\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def cbow_eval_and_pick_best(\n",
    "    dataset: TensorDataset,\n",
    "    models: list[nn.Module]\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models and pick the best one.\n",
    "    \"\"\"\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for model in models:\n",
    "        accuracy = cbow_eval(dataset, model)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "\n",
    "    print(f'Best model: {best_model.name} | Accuracy {best_accuracy}')\n",
    "    return best_model\n",
    "\n",
    "def cbow_create_embeddings() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create multiple embeddings models and pick the best one.  \n",
    "\n",
    "    Returns the embeddings of the best model.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset = cbow_create_dataset(words_train)\n",
    "\n",
    "    m1 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m1,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m1.parameters(), lr=0.02)\n",
    "    )\n",
    "\n",
    "    m2 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m2,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m2.parameters(), lr=0.01)\n",
    "    )\n",
    "    \n",
    "    m3 = CBOW()\n",
    "    cbow_train(\n",
    "        train_dataset, m3,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m3.parameters(), lr=0.001)\n",
    "    )\n",
    "\n",
    "    val_dataset = cbow_create_dataset(words_val)\n",
    "    best_model = cbow_eval_and_pick_best(\n",
    "        val_dataset,\n",
    "        [m1, m2, m3]\n",
    "    )\n",
    "\n",
    "    return best_model.embeddings.weights.detach().to(device)\n",
    "\n",
    "embeddings = cbow_create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('absorbed', tensor(0.8978, device='cuda:0')), ('borne', tensor(0.8677, device='cuda:0')), ('duty', tensor(0.8438, device='cuda:0')), ('gone', tensor(0.8303, device='cuda:0')), ('anything', tensor(0.8259, device='cuda:0')), ('absolute', tensor(0.8175, device='cuda:0')), ('thought', tensor(0.8108, device='cuda:0')), ('people', tensor(0.7782, device='cuda:0')), ('gay', tensor(0.7702, device='cuda:0')), ('actions', tensor(0.7661, device='cuda:0'))]\n",
      "[('presented', tensor(0.8658, device='cuda:0')), ('bulbs', tensor(0.8037, device='cuda:0')), ('fire', tensor(0.7927, device='cuda:0')), ('instant', tensor(0.7905, device='cuda:0')), ('merely', tensor(0.7626, device='cuda:0')), ('about', tensor(0.7617, device='cuda:0')), ('many', tensor(0.7599, device='cuda:0')), ('post', tensor(0.7515, device='cuda:0')), ('midst', tensor(0.7476, device='cuda:0')), ('keeping', tensor(0.7431, device='cuda:0'))]\n",
      "pride\n"
     ]
    }
   ],
   "source": [
    "def word_vector_cosine_similarity(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return torch.dot(word_a, word_b) / (word_a.norm() * word_b.norm())\n",
    "\n",
    "def word_vector_distance_similarity(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return (word_a - word_b).norm()\n",
    "\n",
    "def word_similarity(word_a:str, word_b:str, vocabulary:Vocab, embeddings:torch.Tensor):\n",
    "    word_a_idx = vocabulary[word_a]\n",
    "    word_b_idx = vocabulary[word_b]\n",
    "\n",
    "    word_a_embedding = embeddings[word_a_idx]\n",
    "    word_b_embedding = embeddings[word_b_idx]\n",
    "\n",
    "    return word_vector_cosine_similarity(word_a_embedding, word_b_embedding)\n",
    "\n",
    "def word_find_similars(\n",
    "    word: str,\n",
    "    top: int,\n",
    "    vocabulary: Vocab,\n",
    "    embeddings: torch.Tensor\n",
    "):\n",
    "    similarities = []\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        similarity = word_similarity(word, other, vocabulary, embeddings)\n",
    "        similarities.append((other, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    similarities = similarities[1:top+1]\n",
    "\n",
    "    return similarities\n",
    "\n",
    "print(word_find_similars('king', 10, vocabulary, embeddings))\n",
    "\n",
    "print(word_find_similars('queen', 10, vocabulary, embeddings))\n",
    "\n",
    "def word_closest(word_vector:torch.Tensor, vocabulary:Vocab, embeddings:torch.Tensor):\n",
    "    closest_word = None\n",
    "    closest_distance = 1_000_000\n",
    "\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        other_idx = vocabulary[other]\n",
    "        other_embedding = embeddings[other_idx]\n",
    "\n",
    "        distance = word_vector_distance_similarity(word_vector, other_embedding)\n",
    "\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_word = other\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "king_vector = embeddings[vocabulary['king']]\n",
    "he_vector = embeddings[vocabulary['he']]\n",
    "she_vector = embeddings[vocabulary['she']]\n",
    "\n",
    "print(word_closest(king_vector - he_vector + she_vector, vocabulary, embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
