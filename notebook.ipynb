{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import typing\n",
    "from torch import nn;\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "MIN_WORD_FREQUENCY = 100\n",
    "FORCE_RETRAIN = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "\n",
    "def read_lines(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads all the lines form all the texts in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scan for all input files\n",
    "    inDirectoryName = os.path.join(DATA_DIR, 'input', dataset)\n",
    "    inFileNames = [os.path.join(inDirectoryName, f) for f in os.listdir(inDirectoryName)]\n",
    "\n",
    "    # Read all the lines from all the files\n",
    "    lines = []\n",
    "    for inFileName in inFileNames:\n",
    "        with open(inFileName, 'r') as file:\n",
    "            lines += file.readlines()\n",
    "\n",
    "    print(f\"Read {len(lines)} lines from {dataset}\")\n",
    "    return lines\n",
    "\n",
    "def create_tokens(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Creates tokens for all the words in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'words.{dataset}.pt')\n",
    "    \n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded tokenized words for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    tokens = []\n",
    "    for line in read_lines(dataset):\n",
    "        tokens += TOKENIZER(line)\n",
    "\n",
    "    # Save tokens so we dont have to do this again\n",
    "    torch.save(tokens, outFileName)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_vocabulary(\n",
    "    dataset: str\n",
    ") -> Vocab:\n",
    "    \"\"\"\n",
    "    Creates a vocabulary for the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'vocabulary.pt')\n",
    "\n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded vocabulary for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    def read_sanitize_tokenize():\n",
    "\n",
    "        for line in read_lines(dataset):\n",
    "\n",
    "            line = re.sub('\\\\w*[0-9]+\\\\w*', ' ', line) # Remove numbers\n",
    "            line = re.sub('\\\\w*[A-Z]+\\\\w*', ' ', line) # Remove uppercase names\n",
    "            line = re.sub('\\\\s+', ' ', line) # Remove double spaces\n",
    "\n",
    "            yield TOKENIZER(line)\n",
    "\n",
    "    vocabulary = build_vocab_from_iterator(read_sanitize_tokenize(), min_freq=MIN_WORD_FREQUENCY, specials=['<unk>'])\n",
    "\n",
    "    vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "    # We removed all uppercase names, this includes 'I'\n",
    "    vocabulary.append_token('i') \n",
    "\n",
    "    # Save vocabulary so we dont have to do this again\n",
    "    torch.save(vocabulary, outFileName)\n",
    "\n",
    "    return vocabulary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized words for train (./data/words.train.pt)\n",
      "Loaded tokenized words for val (./data/words.val.pt)\n",
      "Loaded tokenized words for test (./data/words.test.pt)\n",
      "Loaded vocabulary for train (./data/vocabulary.pt)\n"
     ]
    }
   ],
   "source": [
    "words_train = create_tokens('train')\n",
    "words_val = create_tokens('val')\n",
    "words_test = create_tokens('test')\n",
    "\n",
    "vocabulary = create_vocabulary('train')\n",
    "VOCABULARY_SIZE = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in 'train' dataset ........: 2684706\n",
      "Words in 'val' dataset ..........: 49526\n",
      "Words in 'test' dataset .........: 124152\n",
      "Distinct words in 'train' dataset: 52105\n",
      "Words in vocabulary .............: 1880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Words in 'train' dataset ........:\", len(words_train))\n",
    "print(\"Words in 'val' dataset ..........:\", len(words_val))\n",
    "print(\"Words in 'test' dataset .........:\", len(words_test))\n",
    "print(\"Distinct words in 'train' dataset:\", len(set(words_train)))\n",
    "print(\"Words in vocabulary .............:\", VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "This section contains som utilites which come in handy for all the next assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nameof(\n",
    "    model: nn.Module, \n",
    "    criterion: object, \n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Creates a good name for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    name = f'{model.__class__.__name__}_{criterion.__class__.__name__}_{optimizer.__class__.__name__}'\n",
    "    options = optimizer.param_groups[0]\n",
    "\n",
    "    if 'lr' in options:\n",
    "        name += f'-lr{options[\"lr\"]:.3f}'\n",
    "\n",
    "    if 'momentum' in options and options['momentum'] != 0.0:\n",
    "        name += f'-m{options[\"momentum\"]:.3f}'\n",
    "\n",
    "    if 'weight_decay' in options and options['weight_decay'] != 0.0:\n",
    "        name += f'-wd{options[\"weight_decay\"]:.3f}'\n",
    "\n",
    "    return name\n",
    "\n",
    "def model_save(model: nn.Module, folder: str | None = None):\n",
    "    \"\"\"\n",
    "    Save the given model to a file.\n",
    "    \"\"\"\n",
    "\n",
    "    folder = '' if folder is None else folder + '/'\n",
    "    filename = DATA_DIR + f'{folder}{model.name}.pt'\n",
    "\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f'Saved {model.name} ({filename})')\n",
    "\n",
    "def model_load(model: nn.Module, folder: str | None = None) -> bool:\n",
    "    \"\"\"\n",
    "    Save the given model to a file.\n",
    "\n",
    "    Returns `True` if the model was loaded, `False` otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    folder = '' if folder is None else folder + '/'\n",
    "    filename = DATA_DIR + f'{folder}{model.name}.pt'\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        return False\n",
    "    \n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    print(f'Loaded {model.name} ({filename}')\n",
    "    return True\n",
    "\n",
    "def dataset_create(\n",
    "    words: list[str],\n",
    "    context_size: int,\n",
    "    vocabulary_index_to_target: dict[int, int] = {},\n",
    "    dataset_name: str | None = None\n",
    ") -> TensorDataset:\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "\n",
    "    filename = DATA_DIR + f'dataset/{dataset_name}.pt'\n",
    "    if os.path.exists(filename) and dataset_name is not None and not FORCE_RETRAIN:\n",
    "        return torch.load(filename)\n",
    "\n",
    "    word_idx = [vocabulary[word] for word in words]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(len(words) - context_size):\n",
    "        context = word_idx[i:i+context_size]\n",
    "        target = word_idx[i+context_size]\n",
    "        target = vocabulary_index_to_target.get(target, target)\n",
    "\n",
    "        contexts.append(torch.tensor(context))\n",
    "        targets.append(target)\n",
    "\n",
    "    contexts = torch.stack(contexts).to(device)\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "\n",
    "    dataset = TensorDataset(contexts, targets)\n",
    "    torch.save(dataset, filename)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def model_train(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    criterion: typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    tranform_targets: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    tranform_contexts: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    model_category: str | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the given `model` with the given `dataset`.  \n",
    "\n",
    "    dataset: The dataset to train the model with.\n",
    "    model: The model to train.\n",
    "    criterion: The loss function to use.\n",
    "    optimizer: The optimizer to use.\n",
    "    batch_size: The batch size to use.\n",
    "    epochs: The number of epochs to train.\n",
    "    force_retrain: If `True`, the model will be trained even if it has been trained before.\n",
    "    tranform_targets: A function to transform the targets before they are passed to `criterion` along with the `model` output.\n",
    "    tranform_contexts: A function to transform the contexts before they are passed to `model`.\n",
    "    model_category: The category of the model. If `None`, the model's class name will be used.\n",
    "    \"\"\"\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Use the model's class name as the category if none is given\n",
    "    if model_category is None:\n",
    "        model_category = model.__class__.__name__\n",
    "\n",
    "    # Name the model for easier referencing\n",
    "    model.name = model_nameof(model, criterion, optimizer)\n",
    "\n",
    "    # If the model has already been trained,\n",
    "    # and we are not forcing a retrain:\n",
    "    #     Load the trained model and return\n",
    "    if model_load(model, model_category) and not FORCE_RETRAIN:\n",
    "        return\n",
    "    \n",
    "    # Prepare a data loader for the given dataset.\n",
    "    # Ensure the data is shuffled.\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(f'Training {model.name}...')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = torch.tensor([0.0]).to(device)\n",
    "        total_size = 0\n",
    "\n",
    "        for contexts, targets in data_loader:\n",
    "\n",
    "            # Perform transformations\n",
    "            contexts = tranform_contexts(contexts).to(device)\n",
    "            targets = tranform_targets(targets).to(device)\n",
    "\n",
    "            # Perform a training step\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts).to(device)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_size += len(targets)\n",
    "\n",
    "        total_loss = total_loss.item() / total_size\n",
    "        losses.append(total_loss)\n",
    "        print(f'Training | {model.name} | Epoch {epoch} | Loss {total_loss}')\n",
    "\n",
    "    # Save the model so we can skip training every time.\n",
    "    model_save(model, model_category)\n",
    "\n",
    "\n",
    "def model_accuracy(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation',\n",
    "    transform_contexts: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    transform_outputs: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the given model on the given dataset.\n",
    "\n",
    "    Returns the accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_loader = DataLoader(dataset, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for contexts, targets in data_loader:\n",
    "        contexts = transform_contexts(contexts).to(device)\n",
    "        outputs = model(contexts)\n",
    "        outputs = transform_outputs(outputs)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (outputs == targets).sum().item()\n",
    "\n",
    "    print(f'{dataset_name} | {model.name} | Accuracy {correct/total:.4f}')\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def model_pick_best(\n",
    "    models: list[nn.Module],\n",
    "    dataset: TensorDataset,\n",
    "    performance_measure: typing.Callable[[nn.Module, TensorDataset], float],\n",
    "):\n",
    "    \"\"\"\n",
    "    Pick the best model from the given list of `models` on a given `dataset`.\n",
    "    \"\"\"\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for model in models:\n",
    "        accuracy = performance_measure(model, dataset)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "This section contains the training and selecting of the best performing embeddings using `CBOW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIM = 32\n",
    "EMBEDDINGS_CONTEXT_SIZE = 5\n",
    "EMBEDDINGS_BATCH_SIZE = 128\n",
    "EMBEDDINGS_EPOCHS = 100\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(VOCABULARY_SIZE, EMBEDDINGS_DIM, sparse=True)\n",
    "        self.linear = nn.Linear(EMBEDDINGS_DIM*EMBEDDINGS_CONTEXT_SIZE, VOCABULARY_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "def cbow_create_dataset(\n",
    "    words: list[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "    return dataset_create(words, EMBEDDINGS_CONTEXT_SIZE, dataset_name='cbow.' + dataset_name)\n",
    "\n",
    "def cbow_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "):\n",
    "    return model_train(\n",
    "        model, dataset, criterion, optimizer, \n",
    "        model_category='embeddings',\n",
    "        epochs=EMBEDDINGS_EPOCHS, batch_size=EMBEDDINGS_BATCH_SIZE,\n",
    "        tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=VOCABULARY_SIZE).float()\n",
    "    )\n",
    "\n",
    "def cbow_performance(\n",
    "    model: CBOW,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation'\n",
    "):\n",
    "    return model_accuracy(\n",
    "        model, dataset, dataset_name,\n",
    "        transform_outputs=lambda x: torch.argmax(x, dim=1)\n",
    "    )\n",
    "\n",
    "def cbow_create_embeddings() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create multiple embeddings models and pick the best one.  \n",
    "\n",
    "    Returns the embeddings of the best model.\n",
    "    \"\"\"\n",
    "    training_data = cbow_create_dataset(words_train, 'train')\n",
    "\n",
    "    m1 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m1,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m1.parameters(), lr=0.02)\n",
    "    )\n",
    "\n",
    "    m2 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m2,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m2.parameters(), lr=0.01)\n",
    "    )\n",
    "    \n",
    "    m3 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m3,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m3.parameters(), lr=0.001)\n",
    "    )\n",
    "\n",
    "    models = [m1, m2, m3]\n",
    "\n",
    "    validation_data = cbow_create_dataset(words_val, 'val')\n",
    "    best_model, best_model_accuracy = model_pick_best(\n",
    "        models,\n",
    "        dataset = validation_data,\n",
    "        performance_measure=cbow_performance\n",
    "    )\n",
    "\n",
    "    print(f'Best model on validation: {best_model.name} | Accuracy {best_model_accuracy}')\n",
    "\n",
    "    return best_model.embeddings.weight.detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CBOW_CrossEntropyLoss_SGD-lr0.020 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.020.pt\n",
      "Loaded CBOW_CrossEntropyLoss_SGD-lr0.010 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.010.pt\n",
      "Loaded CBOW_CrossEntropyLoss_SGD-lr0.001 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.001.pt\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.020 | Accuracy 0.2166\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.010 | Accuracy 0.2084\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.001 | Accuracy 0.1869\n",
      "Best model on validation: CBOW_CrossEntropyLoss_SGD-lr0.020 | Accuracy 0.21659497990751397\n"
     ]
    }
   ],
   "source": [
    "embeddings = cbow_create_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insepecting the embeddings\n",
    "In this section we try to understand the embeddings we created in the previous section.  \n",
    "We will identify which words the model believes are similar and take a look at the embeddings using the `Tensorflow Projector` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_similarity_cosine(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return torch.dot(word_a, word_b) / (word_a.norm() * word_b.norm())\n",
    "\n",
    "def word_vector_similarity_euclidian(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return (word_a - word_b).norm()\n",
    "\n",
    "def word_similarity_cosine(word_a:str, word_b:str):\n",
    "    word_a_idx = vocabulary[word_a]\n",
    "    word_b_idx = vocabulary[word_b]\n",
    "\n",
    "    word_a_embedding = embeddings[word_a_idx]\n",
    "    word_b_embedding = embeddings[word_b_idx]\n",
    "\n",
    "    return word_vector_similarity_cosine(word_a_embedding, word_b_embedding)\n",
    "\n",
    "def word_find_top_closest(\n",
    "    word: str,\n",
    "    top: int\n",
    "):\n",
    "    similarities = []\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        similarity = word_similarity_cosine(word, other).item()\n",
    "        similarities.append((other, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    similarities = similarities[1:top+1]\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def word_find_closest(\n",
    "    word_vector:torch.Tensor,\n",
    "):\n",
    "    closest_word = None\n",
    "    closest_distance = 1_000_000\n",
    "\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        other_idx = vocabulary[other]\n",
    "        other_embedding = embeddings[other_idx]\n",
    "\n",
    "        distance = word_vector_similarity_euclidian(word_vector, other_embedding)\n",
    "\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_word = other\n",
    "    \n",
    "    return closest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar words\n",
      "king : ['earl', 'prince', 'father', 'spirit', 'building', 'aloud', 'rest', 'wrong', 'bishop', 'feel']\n",
      "queen : ['fixed', 'can', 'sprang', 'desired', 'trust', 'wound', 'large', 'walls', 'spring', 'crossed']\n",
      "man : ['wood', 'social', 'together', 'doctor', 'chamber', 'glancing', 'party', 'sun', 'harm', 'ye']\n",
      "woman : ['fatigue', 'soul', 'paid', 'empty', 'size', 'heat', 'man', 'thick', 'singing', 'le']\n",
      "he : ['who', 'she', 'growth', 'fully', 'face', 'wrong', 'count', 'they', 'example', 'i']\n",
      "she : ['he', 'himself', 'everything', 'never', 'excellent', 'brothers', 'key', 'child', 'her', 'i']\n",
      "doctor : ['smile', 'report', 'torn', 'glancing', 'man', 'really', 'cases', 'otherwise', 'uttered', 'dream']\n",
      "nurse : Not in vocabulary\n",
      "black : ['poor', 'skald', 'words', 'white', 'turned', 'o', 'minutes', 'called', 'years', 'human']\n",
      "white : ['single', 'poor', 'excellent', 'rain', 'nose', 'personal', 'whose', 'black', 'horrible', 'gaze']\n",
      "slave : Not in vocabulary\n",
      "master : ['begged', 'bring', 'rage', 'troubled', 'life', 'stout', 'approaching', 'eastward', 'autumn', 'court']\n",
      "poor : ['personal', 'black', 'white', 'curious', 'doctor', 'la', 'increased', 'dangerous', 'interesting', 'good']\n",
      "rich : ['-', 'built', 'march', 'others', 'manure', 'shut', 'peasants', 'erect', 'rest', 'bishop']\n",
      "smart : Not in vocabulary\n",
      "dumb : Not in vocabulary\n",
      "strong : ['huge', 'early', 'direct', 'driver', 'the', 'de', 'enemy', 'chief', 'wise', 'quick']\n",
      "weak : ['remember', 'trench', 'ladies', 'kind', 'fingers', 'military', 'shouting', 'anxiety', 'tea', 'rose']\n",
      "good : ['curious', 'popular', 'treated', 'become', 'frost', 'loud', 'sick', 'all', 'confidence', 'broad']\n",
      "bad : ['delicate', 'often', 'mine', 'tender', 'level', 'gardener', 'equal', 'curious', 'stay', 'also']\n"
     ]
    }
   ],
   "source": [
    "def print_most_similar_words(words, top = 10):\n",
    "    print(f\"Top {top} most similar words\")\n",
    "    for word in words:\n",
    "        if vocabulary[word] == vocabulary['<unk>']:\n",
    "            print(word, ':', \"Not in vocabulary\")\n",
    "        else:\n",
    "            print(word, ':', [x[0] for x in word_find_top_closest(word, top)])\n",
    "\n",
    "print_most_similar_words([\n",
    "    'king', 'queen', 'man', 'woman', 'he', 'she', 'doctor', 'nurse',\n",
    "    'black', 'white', 'slave', 'master',\n",
    "    'poor', 'rich', \n",
    "    'smart', 'dumb', \n",
    "    'strong', 'weak',\n",
    "    'good', 'bad',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_projector_create_data():\n",
    "    e = embeddings.cpu().numpy()\n",
    "    e = pd.DataFrame(e)\n",
    "    e.to_csv(DATA_DIR + 'tensorflow_projector/embeddings.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "    v = vocabulary.lookup_tokens(range(len(vocabulary)))\n",
    "    v = pd.DataFrame(v)\n",
    "    v.to_csv(DATA_DIR + 'tensorflow_projector/vocabulary.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "tensorflow_projector_create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugating _be_ and _have_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BeHaveRNN_CrossEntropyLoss_Adam-lr0.001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.001.pt\n",
      "Validation | BeHaveRNN_CrossEntropyLoss_Adam-lr0.001 | Accuracy 0.9474\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m validation_data, _ \u001b[38;5;241m=\u001b[39m behave_create_dataset(words_val, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m behave_performance(validation_data, model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mbehave_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 69\u001b[0m, in \u001b[0;36mbehave_performance\u001b[0;34m(dataset, model, dataset_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbehave_performance\u001b[39m(\n\u001b[1;32m     65\u001b[0m     dataset: TensorDataset,\n\u001b[1;32m     66\u001b[0m     model: CBOW,\n\u001b[1;32m     67\u001b[0m     dataset_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m ):\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_accuracy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 192\u001b[0m, in \u001b[0;36mmodel_accuracy\u001b[0;34m(model, dataset, dataset_name, transform_contexts, transform_outputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m transform_outputs(outputs)\n\u001b[1;32m    191\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;241m/\u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m total\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BEHAVE_CONTEXT_SIZE = 5\n",
    "BEHAVE_BATCH_SIZE = 512\n",
    "BEHAVE_EPOCHS = 1\n",
    "BEHAVE_WORDS = ['<unk>', 'be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']\n",
    "BEHAVE_WORDS_SIZE = len(BEHAVE_WORDS)\n",
    "\n",
    "class BeHaveRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BeHaveRNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(EMBEDDINGS_DIM*BEHAVE_CONTEXT_SIZE, EMBEDDINGS_DIM, batch_first=True)\n",
    "        self.fc1 = nn.Linear(EMBEDDINGS_DIM, BEHAVE_WORDS_SIZE * 4)\n",
    "        self.fc2 = nn.Linear(BEHAVE_WORDS_SIZE * 4, BEHAVE_WORDS_SIZE)\n",
    "        self.hidden = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            return [self.forward(x) for x in x]\n",
    "\n",
    "        x = embeddings[x]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x, hidden = self.rnn(x, self.hidden)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        self.hidden = hidden.data\n",
    "\n",
    "        return x\n",
    "\n",
    "def behave_create_dataset(\n",
    "    words: list[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.  \n",
    "    \"\"\"\n",
    "    label_to_vocabulary:dict[int, int] = {}\n",
    "    vocabulary_to_label = { vocabulary[word]: 0 for word in vocabulary.lookup_tokens(range(VOCABULARY_SIZE)) }\n",
    "\n",
    "    for label, word in enumerate(BEHAVE_WORDS):\n",
    "        vocabulary_index = vocabulary[word]\n",
    "\n",
    "        vocabulary_to_label[vocabulary_index] = label\n",
    "        label_to_vocabulary[label] = vocabulary_index\n",
    "\n",
    "    return dataset_create(words, BEHAVE_CONTEXT_SIZE, vocabulary_to_label, dataset_name='behave.'+dataset_name), label_to_vocabulary\n",
    "\n",
    "def behave_rnn_transform_targets(targets: torch.Tensor) -> list[torch.Tensor]:\n",
    "    return [torch.nn.functional.one_hot(targets, num_classes=BEHAVE_WORDS_SIZE).float()] * BEHAVE_CONTEXT_SIZE\n",
    "\n",
    "def behave_rnn_transform_contexts(contexts: torch.Tensor) -> list[torch.Tensor]:\n",
    "    contextList = []\n",
    "    for i in range(BEHAVE_CONTEXT_SIZE - 1):\n",
    "        contextList.append(contexts[:, 0:i])\n",
    "    return contextList\n",
    "\n",
    "def behave_rnn_criterion(criterion, outputs: list[torch.Tensor], targets: list[torch.Tensor]) -> torch.Tensor:\n",
    "    loss = 0\n",
    "    for i in range(BEHAVE_CONTEXT_SIZE - 1):\n",
    "        loss += criterion(outputs[i], targets[i])\n",
    "    return loss\n",
    "\n",
    "def behave_rnn_create_criterion(criterion: object) -> object:\n",
    "    return lambda x, y: behave_rnn_criterion(criterion, x, y)\n",
    "\n",
    "def behave_rnn_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer\n",
    "):\n",
    "    return model_train(\n",
    "        model, dataset, criterion, optimizer, \n",
    "        model_category='behave',\n",
    "        epochs=BEHAVE_EPOCHS, batch_size=BEHAVE_BATCH_SIZE,\n",
    "        tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=BEHAVE_WORDS_SIZE).float()\n",
    "    )\n",
    "\n",
    "def behave_performance(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW,\n",
    "    dataset_name: str = 'Validation'\n",
    "):\n",
    "    return model_accuracy(\n",
    "        model, dataset, dataset_name,\n",
    "        transform_outputs=lambda x: torch.argmax(x, dim=1)\n",
    "    )\n",
    "\n",
    "model = BeHaveRNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "training_data, label_to_vocabulary = behave_create_dataset(words_train, 'train')\n",
    "\n",
    "behave_rnn_train(training_data, model, criterion, optimizer)\n",
    "\n",
    "validation_data, _ = behave_create_dataset(words_val, 'val')\n",
    "behave_performance(validation_data, model, 'Validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
