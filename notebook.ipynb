{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from torch import nn;\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "DOCS_DIR = './docs/'\n",
    "MIN_WORD_FREQUENCY = 100\n",
    "FORCE_RETRAIN = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "\n",
    "def read_lines(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads all the lines form all the texts in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scan for all input files\n",
    "    inDirectoryName = os.path.join(DATA_DIR, 'input', dataset)\n",
    "    inFileNames = [os.path.join(inDirectoryName, f) for f in os.listdir(inDirectoryName)]\n",
    "\n",
    "    # Read all the lines from all the files\n",
    "    lines = []\n",
    "    for inFileName in inFileNames:\n",
    "        with open(inFileName, 'r') as file:\n",
    "            lines += file.readlines()\n",
    "\n",
    "    print(f\"Read {len(lines)} lines from {dataset}\")\n",
    "    return lines\n",
    "\n",
    "def create_tokens(\n",
    "    dataset: str\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Creates tokens for all the words in the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'words.{dataset}.pt')\n",
    "    \n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded tokenized words for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    tokens = []\n",
    "    for line in read_lines(dataset):\n",
    "        tokens += TOKENIZER(line)\n",
    "\n",
    "    # Save tokens so we dont have to do this again\n",
    "    torch.save(tokens, outFileName)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_vocabulary(\n",
    "    dataset: str\n",
    ") -> Vocab:\n",
    "    \"\"\"\n",
    "    Creates a vocabulary for the given `dataset`.\n",
    "\n",
    "    Datasets are `train`, `val` and `test`.\n",
    "    \"\"\"\n",
    "\n",
    "    outFileName = os.path.join(DATA_DIR, f'vocabulary.pt')\n",
    "\n",
    "    # If the file exists, don't create it again.\n",
    "    if os.path.isfile(outFileName):\n",
    "        print(f\"Loaded vocabulary for {dataset} ({outFileName})\")\n",
    "        return torch.load(outFileName)\n",
    "\n",
    "    def read_sanitize_tokenize():\n",
    "\n",
    "        for line in read_lines(dataset):\n",
    "\n",
    "            line = re.sub('\\\\w*[0-9]+\\\\w*', ' ', line) # Remove numbers\n",
    "            line = re.sub('\\\\w*[A-Z]+\\\\w*', ' ', line) # Remove uppercase names\n",
    "            line = re.sub('\\\\s+', ' ', line) # Remove double spaces\n",
    "\n",
    "            yield TOKENIZER(line)\n",
    "\n",
    "    vocabulary = build_vocab_from_iterator(read_sanitize_tokenize(), min_freq=MIN_WORD_FREQUENCY, specials=['<unk>'])\n",
    "\n",
    "    vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "    # We removed all uppercase names, this includes 'I'\n",
    "    vocabulary.append_token('i') \n",
    "\n",
    "    # Save vocabulary so we dont have to do this again\n",
    "    torch.save(vocabulary, outFileName)\n",
    "\n",
    "    return vocabulary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized words for train (./data/words.train.pt)\n",
      "Loaded tokenized words for val (./data/words.val.pt)\n",
      "Loaded tokenized words for test (./data/words.test.pt)\n",
      "Loaded vocabulary for train (./data/vocabulary.pt)\n"
     ]
    }
   ],
   "source": [
    "words_train = create_tokens('train')\n",
    "words_val = create_tokens('val')\n",
    "words_test = create_tokens('test')\n",
    "\n",
    "vocabulary = create_vocabulary('train')\n",
    "VOCABULARY_SIZE = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in 'train' dataset ........: 2684706\n",
      "Words in 'val' dataset ..........: 49526\n",
      "Words in 'test' dataset .........: 124152\n",
      "Distinct words in 'train' dataset: 52105\n",
      "Words in vocabulary .............: 1880\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def print_dataset_info():\n",
    "    print(\"Words in 'train' dataset ........:\", len(words_train))\n",
    "    print(\"Words in 'val' dataset ..........:\", len(words_val))\n",
    "    print(\"Words in 'test' dataset .........:\", len(words_test))\n",
    "    print(\"Distinct words in 'train' dataset:\", len(set(words_train)))\n",
    "    print(\"Words in vocabulary .............:\", VOCABULARY_SIZE)\n",
    "\n",
    "    bars = plt.bar(['Train', 'Validation', 'Test', 'Unique', 'Vocabulary'], [len(words_train), len(words_val), len(words_test), len(set(words_train)), VOCABULARY_SIZE])\n",
    "    plt.title('Word counts')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Word count')\n",
    "    plt.gca().yaxis.set_visible(False)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, yval, ha='center', va='bottom')\n",
    "    plt.savefig(DOCS_DIR + 'word_counts.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "print_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "This section contains som utilites which come in handy for all the next assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nameof(\n",
    "    model: nn.Module, \n",
    "    criterion: object, \n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Creates a good name for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    name = f'{model.__class__.__name__}_{criterion.__class__.__name__}_{optimizer.__class__.__name__}'\n",
    "    options = optimizer.param_groups[0]\n",
    "\n",
    "    if 'lr' in options:\n",
    "        name += f'-lr{options[\"lr\"]:.4f}'\n",
    "\n",
    "    if 'momentum' in options and options['momentum'] != 0.0:\n",
    "        name += f'-m{options[\"momentum\"]:.4f}'\n",
    "\n",
    "    if 'weight_decay' in options and options['weight_decay'] != 0.0:\n",
    "        name += f'-wd{options[\"weight_decay\"]:.4f}'\n",
    "\n",
    "    return name\n",
    "\n",
    "def model_save(model: nn.Module, folder: str | None = None):\n",
    "    \"\"\"\n",
    "    Save the given model to a file.\n",
    "    \"\"\"\n",
    "\n",
    "    folder = '' if folder is None else folder + '/'\n",
    "    filename = DATA_DIR + f'{folder}{model.name}.pt'\n",
    "\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f'Saved {model.name} ({filename})')\n",
    "\n",
    "def model_load(model: nn.Module, folder: str | None = None) -> bool:\n",
    "    \"\"\"\n",
    "    Save the given model to a file.\n",
    "\n",
    "    Returns `True` if the model was loaded, `False` otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    if FORCE_RETRAIN:\n",
    "        return False\n",
    "\n",
    "    folder = '' if folder is None else folder + '/'\n",
    "    filename = DATA_DIR + f'{folder}{model.name}.pt'\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        return False\n",
    "    \n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    print(f'Loaded {model.name} ({filename}')\n",
    "    return True\n",
    "\n",
    "def dataset_create(\n",
    "    words: list[str],\n",
    "    context_size_before: int = 0,\n",
    "    context_size_after: int = 0,\n",
    "    vocabulary_index_to_target: dict[int, int] | None = None,\n",
    "    dataset_name: str | None = None\n",
    ") -> TensorDataset:\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert in case they are floats\n",
    "    context_size_before = int(context_size_before)\n",
    "    context_size_after = int(context_size_after)\n",
    "\n",
    "    filename = DATA_DIR + f'dataset/{dataset_name}.pt'\n",
    "    if os.path.exists(filename) and dataset_name is not None and not FORCE_RETRAIN:\n",
    "        return torch.load(filename)\n",
    "\n",
    "    word_idx = [vocabulary[word] for word in words]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(context_size_before, len(words) - context_size_after):\n",
    "\n",
    "        context_before = word_idx[i-context_size_before:i]\n",
    "        context_after = word_idx[i+1:i+1+context_size_after]\n",
    "        context = context_before + context_after\n",
    "        target = word_idx[i]\n",
    "\n",
    "        if vocabulary_index_to_target is not None:\n",
    "            target = vocabulary_index_to_target.get(target, None)\n",
    "\n",
    "        if target is not None:\n",
    "            contexts.append(torch.tensor(context))\n",
    "            targets.append(target)\n",
    "\n",
    "    contexts = torch.stack(contexts).to(device)\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "\n",
    "    dataset = TensorDataset(contexts, targets)\n",
    "    torch.save(dataset, filename)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def model_train(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    criterion: typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    tranform_targets: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    tranform_contexts: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    model_category: str = \"models\",\n",
    "    retrain: bool = False,\n",
    "    figure_tag: str = \"\"\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Trains the given `model` with the given `dataset`.  \n",
    "\n",
    "    dataset: The dataset to train the model with.\n",
    "    model: The model to train.\n",
    "    criterion: The loss function to use.\n",
    "    optimizer: The optimizer to use.\n",
    "    batch_size: The batch size to use.\n",
    "    epochs: The number of epochs to train.\n",
    "    force_retrain: If `True`, the model will be trained even if it has been trained before.\n",
    "    tranform_targets: A function to transform the targets before they are passed to `criterion` along with the `model` output.\n",
    "    tranform_contexts: A function to transform the contexts before they are passed to `model`.\n",
    "    model_category: The category of the model. If `None`, the model's class name will be used.\n",
    "    \"\"\"\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Name the model for easier referencing\n",
    "    model.name = model_nameof(model, criterion, optimizer)\n",
    "\n",
    "    # If the model has already been trained,\n",
    "    # and we are not forcing a retrain:\n",
    "    #     Load the trained model and return\n",
    "    if (not retrain) and model_load(model, model_category):\n",
    "        return []\n",
    "    \n",
    "    # Prepare a data loader for the given dataset.\n",
    "    # Ensure the data is shuffled.\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(f'Training {model.name}...')\n",
    "\n",
    "    losses = []\n",
    "    dataset_size = dataset.tensors[0].size(0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = torch.tensor([0.0])\n",
    "\n",
    "        for contexts, targets in data_loader:\n",
    "\n",
    "            # Perform transformations\n",
    "            contexts = tranform_contexts(contexts)\n",
    "            targets = tranform_targets(targets)\n",
    "\n",
    "            # Perform a training step\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.cpu().detach()\n",
    "\n",
    "        loss_sum = loss_sum.item() / dataset_size\n",
    "        losses.append(loss_sum)\n",
    "        print(f'Training | {model.name} | Epoch {epoch} | Loss {loss_sum}')\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(losses)\n",
    "    plt.title(f'{model.name} | Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(DOCS_DIR + f'loss_{model.name}{figure_tag}.png')\n",
    "\n",
    "    # Save the model so we can skip training every time.\n",
    "    model_save(model, model_category)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def model_accuracy(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation',\n",
    "    transform_contexts: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x,\n",
    "    transform_outputs: typing.Callable[[torch.Tensor], torch.Tensor] = lambda x: x\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the given model on the given dataset.\n",
    "\n",
    "    Returns the accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_loader = DataLoader(dataset, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for contexts, targets in data_loader:\n",
    "        contexts = transform_contexts(contexts).to(device)\n",
    "        outputs = model(contexts)\n",
    "        outputs = transform_outputs(outputs)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (outputs == targets).sum().item()\n",
    "\n",
    "    print(f'{dataset_name} | {model.name} | Accuracy {correct/total:.4f}')\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def model_pick_best(\n",
    "    models: list[nn.Module],\n",
    "    dataset: TensorDataset,\n",
    "    performance_measure: typing.Callable[[nn.Module, TensorDataset], float],\n",
    "    figure_tag: str = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Pick the best model from the given list of `models` on a given `dataset`.\n",
    "    \"\"\"\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    accuracies: dict[str, float] = {}\n",
    "\n",
    "    for model in models:\n",
    "        accuracy = performance_measure(model, dataset)\n",
    "        accuracies[model.name] = accuracy\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "\n",
    "    print(f'Best model: {best_model.name} | Accuracy {best_accuracy:.4f}')\n",
    "\n",
    "    colors = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.BASE_COLORS.values()) + list(mcolors.CSS4_COLORS.values())\n",
    "    bars = plt.bar(accuracies.keys(), accuracies.values(), color=colors[:len(accuracies)])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.gca().xaxis.set_visible(False)\n",
    "    plt.legend(bars, accuracies.keys())\n",
    "    plt.gca().yaxis.set_visible(False)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.4f}', ha='center', va='bottom')\n",
    "    plt.savefig(DOCS_DIR + f'accuracy{figure_tag}.png')\n",
    "\n",
    "    return best_model, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "This section contains the training and selecting of the best performing embeddings using `CBOW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIM = 32\n",
    "EMBEDDINGS_CONTEXT_SIZE = 5\n",
    "EMBEDDINGS_BATCH_SIZE = 8192\n",
    "EMBEDDINGS_EPOCHS = 100\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(VOCABULARY_SIZE, EMBEDDINGS_DIM, sparse=True)\n",
    "        self.linear = nn.Linear(EMBEDDINGS_DIM*EMBEDDINGS_CONTEXT_SIZE, VOCABULARY_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "def cbow_create_dataset(\n",
    "    words: list[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.\n",
    "    \"\"\"\n",
    "    return dataset_create(words, EMBEDDINGS_CONTEXT_SIZE, dataset_name='cbow.' + dataset_name)\n",
    "\n",
    "def cbow_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: CBOW,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "):\n",
    "    return model_train(\n",
    "        model, dataset, criterion, optimizer, \n",
    "        model_category='embeddings',\n",
    "        epochs=EMBEDDINGS_EPOCHS, batch_size=EMBEDDINGS_BATCH_SIZE,\n",
    "        tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=VOCABULARY_SIZE).float()\n",
    "    )\n",
    "\n",
    "def cbow_performance(\n",
    "    model: CBOW,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation'\n",
    "):\n",
    "    return model_accuracy(\n",
    "        model, dataset, dataset_name,\n",
    "        transform_outputs=lambda x: torch.argmax(x, dim=1)\n",
    "    )\n",
    "\n",
    "def cbow_create_embeddings() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create multiple embeddings models and pick the best one.  \n",
    "\n",
    "    Returns the embeddings of the best model.\n",
    "    \"\"\"\n",
    "    training_data = cbow_create_dataset(words_train, 'train')\n",
    "\n",
    "    m1 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m1,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m1.parameters(), lr=0.02)\n",
    "    )\n",
    "\n",
    "    m2 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m2,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m2.parameters(), lr=0.01)\n",
    "    )\n",
    "    \n",
    "    m3 = CBOW()\n",
    "    cbow_train(\n",
    "        training_data, m3,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        torch.optim.SGD(m3.parameters(), lr=0.001)\n",
    "    )\n",
    "\n",
    "    validation_data = cbow_create_dataset(words_val, 'val')\n",
    "    best_model, validation_accuracy = model_pick_best(\n",
    "        [m1, m2, m3],\n",
    "        dataset = validation_data,\n",
    "        performance_measure=cbow_performance,\n",
    "        figure_tag='cbow'\n",
    "    )\n",
    "\n",
    "    test_data = cbow_create_dataset(words_test, 'test')\n",
    "    test_accuracy = cbow_performance(best_model, test_data, 'Test')\n",
    "    train_accuracy = cbow_performance(best_model, training_data, 'Train')\n",
    "\n",
    "    plt.clf()\n",
    "    plt.bar(['Train', 'Validation', 'Test'], [train_accuracy, validation_accuracy, test_accuracy])\n",
    "    plt.title('CBOW Model Accuracy')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.savefig(DOCS_DIR + 'accuracy_cbow_best.png')\n",
    "    plt.show()\n",
    "\n",
    "    embeddings = best_model.embeddings.weight.detach().to(device)\n",
    "\n",
    "    torch.save(embeddings, DATA_DIR + 'embeddings.pt')\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW_CrossEntropyLoss_SGD-lr0.0200...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 0 | Loss 0.0008294658958006404\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 1 | Loss 0.0007528100694598998\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 2 | Loss 0.0007222870919069526\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 3 | Loss 0.0007017355278081945\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 4 | Loss 0.0006865225543127288\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 5 | Loss 0.0006746415848543283\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 6 | Loss 0.0006650152801000931\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 7 | Loss 0.0006570243995513839\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 8 | Loss 0.0006502696810877403\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 9 | Loss 0.0006444505754987939\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 10 | Loss 0.0006393556980222146\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 11 | Loss 0.0006348395482838452\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 12 | Loss 0.0006308011853991143\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 13 | Loss 0.0006271571739972161\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 14 | Loss 0.0006238429937559853\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 15 | Loss 0.0006208060826652577\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 16 | Loss 0.000618014839862521\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 17 | Loss 0.0006154379827673314\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 18 | Loss 0.000613037181124872\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 19 | Loss 0.0006108173910416374\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 20 | Loss 0.0006087442835231\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 21 | Loss 0.0006067932144433868\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 22 | Loss 0.0006049734594513504\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 23 | Loss 0.0006032545089372848\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 24 | Loss 0.0006016248592778581\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 25 | Loss 0.0006000956958143337\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 26 | Loss 0.0005986372819077432\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 27 | Loss 0.0005972604391484145\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 28 | Loss 0.0005959407052859425\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 29 | Loss 0.0005946863101852403\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 30 | Loss 0.0005934897060143988\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 31 | Loss 0.000592338752585949\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 32 | Loss 0.0005912403611676635\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 33 | Loss 0.0005901826643852752\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 34 | Loss 0.0005891661623963199\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 35 | Loss 0.0005881891728527217\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 36 | Loss 0.0005872486948092637\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 37 | Loss 0.0005863317241700054\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 38 | Loss 0.0005854630869362873\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 39 | Loss 0.0005846082722381142\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 40 | Loss 0.0005837961982748498\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 41 | Loss 0.0005829994018508719\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 42 | Loss 0.0005822296139338469\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 43 | Loss 0.0005814796049739338\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 44 | Loss 0.000580755831550236\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 45 | Loss 0.0005800527009921216\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 46 | Loss 0.0005793649843798946\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 47 | Loss 0.0005786918632739503\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 48 | Loss 0.0005780492517777119\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 49 | Loss 0.000577404594182462\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 50 | Loss 0.0005767970391826222\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 51 | Loss 0.000576181890882006\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 52 | Loss 0.0005755983434439021\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 53 | Loss 0.0005750182061708176\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 54 | Loss 0.0005744521642464794\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 55 | Loss 0.000573901263454827\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 56 | Loss 0.0005733622755063087\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 57 | Loss 0.0005728304261698975\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 58 | Loss 0.0005723200381386745\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 59 | Loss 0.0005718112415177938\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 60 | Loss 0.0005713124025787695\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 61 | Loss 0.0005708254310140124\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 62 | Loss 0.0005703575109044964\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 63 | Loss 0.0005698873173516343\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 64 | Loss 0.0005694214433411299\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 65 | Loss 0.0005689664363898205\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 66 | Loss 0.0005685272071353337\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 67 | Loss 0.0005680876595987784\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 68 | Loss 0.0005676599794364903\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 69 | Loss 0.0005672442121173363\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 70 | Loss 0.0005668318094943348\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 71 | Loss 0.0005664261817325049\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 72 | Loss 0.0005660200992820057\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 73 | Loss 0.00056562993093493\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 74 | Loss 0.0005652402627453905\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 75 | Loss 0.0005648500034605809\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 76 | Loss 0.0005644782045357425\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 77 | Loss 0.0005641054962335657\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 78 | Loss 0.0005637386079463551\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 79 | Loss 0.0005633850420371533\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 80 | Loss 0.0005630261108016544\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 81 | Loss 0.0005626794561602251\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 82 | Loss 0.0005623299369801795\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 83 | Loss 0.0005619907847017926\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 84 | Loss 0.0005616529510205467\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 85 | Loss 0.0005613198006325938\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 86 | Loss 0.0005610007001245204\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 87 | Loss 0.0005606736880336022\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 88 | Loss 0.0005603609531668983\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 89 | Loss 0.0005600405795305511\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 90 | Loss 0.0005597288904477864\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 91 | Loss 0.0005594158372990139\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 92 | Loss 0.0005591208352904104\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 93 | Loss 0.0005588102829293187\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 94 | Loss 0.0005585180545215976\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 95 | Loss 0.0005582284178392911\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 96 | Loss 0.0005579369624023076\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 97 | Loss 0.0005576554191783135\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 98 | Loss 0.0005573704203204333\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Epoch 99 | Loss 0.0005570966068038164\n",
      "Saved CBOW_CrossEntropyLoss_SGD-lr0.0200 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.0200.pt)\n",
      "Training CBOW_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.0008586276264156139\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.000779665573675616\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.0007490706189063279\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.0007310522163216453\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.0007170999126305173\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.0007058278622379643\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.0006964434063868983\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.0006884138318324368\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.0006814093528827791\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.0006752209491566752\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.0006697165335957105\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.000664799030169002\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.0006603657247061684\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.0006563547795481927\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.0006527113592415645\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.0006493862664722719\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.0006463266209480823\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.0006434988666452055\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.0006408844522659367\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.0006384470027167364\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.0006361633743443404\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.0006340246552508314\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.000632003700522656\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.0006300945082693799\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.0006282862114318084\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.0006265650784121305\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.0006249255165397142\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.0006233683442541646\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.0006218670532060647\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.000620444696110945\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 30 | Loss 0.0006190734460222479\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 31 | Loss 0.0006177598049879438\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 32 | Loss 0.0006164923148535675\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 33 | Loss 0.0006152716576521231\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 34 | Loss 0.0006140990610430174\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 35 | Loss 0.0006129717968942352\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 36 | Loss 0.0006118828630002699\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 37 | Loss 0.0006108366243723463\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 38 | Loss 0.0006098216228559996\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 39 | Loss 0.0006088434965907284\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 40 | Loss 0.0006078876046013829\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 41 | Loss 0.000606974635222414\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 42 | Loss 0.0006060789440128761\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 43 | Loss 0.0006052153992922535\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 44 | Loss 0.0006043715425910092\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 45 | Loss 0.000603558104561737\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 46 | Loss 0.0006027685376876\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 47 | Loss 0.0006020021144667274\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 48 | Loss 0.0006012448304881065\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 49 | Loss 0.0006005217845662795\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 50 | Loss 0.0005998113789894573\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 51 | Loss 0.0005991212979961502\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 52 | Loss 0.0005984428570327757\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 53 | Loss 0.0005977826036161709\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 54 | Loss 0.0005971424019698795\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 55 | Loss 0.0005965158864525323\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 56 | Loss 0.0005958998742434447\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 57 | Loss 0.0005953062327168835\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 58 | Loss 0.0005947286417003467\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 59 | Loss 0.0005941539606912929\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 60 | Loss 0.0005935867820452817\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 61 | Loss 0.000593041473924261\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 62 | Loss 0.0005925085788039106\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 63 | Loss 0.0005919802760391195\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 64 | Loss 0.0005914612943920478\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 65 | Loss 0.0005909613642002173\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 66 | Loss 0.0005904727557562509\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 67 | Loss 0.000589981464649136\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 68 | Loss 0.0005895056329567753\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 69 | Loss 0.000589035530341647\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 70 | Loss 0.0005885852976213645\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 71 | Loss 0.0005881288356663135\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 72 | Loss 0.0005876816948289818\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 73 | Loss 0.0005872476035564575\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 74 | Loss 0.000586819605112101\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 75 | Loss 0.0005863987452798514\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 76 | Loss 0.0005859787038872066\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 77 | Loss 0.0005855685292386843\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 78 | Loss 0.0005851670391437445\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 79 | Loss 0.000584775552199528\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 80 | Loss 0.0005843926134022932\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 81 | Loss 0.0005840032180259552\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 82 | Loss 0.0005836219161079297\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 83 | Loss 0.0005832586653300731\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 84 | Loss 0.0005828857751524286\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 85 | Loss 0.0005825241612537811\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 86 | Loss 0.0005821728233190586\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 87 | Loss 0.0005818186663145868\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 88 | Loss 0.0005814694199477427\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 89 | Loss 0.0005811365878418584\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 90 | Loss 0.0005807978902521408\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 91 | Loss 0.0005804641487689178\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 92 | Loss 0.0005801411834071559\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 93 | Loss 0.000579815262569044\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 94 | Loss 0.0005794927064270844\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 95 | Loss 0.0005791844275093391\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 96 | Loss 0.0005788740115548482\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 97 | Loss 0.0005785621405966158\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 98 | Loss 0.0005782580448146274\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Epoch 99 | Loss 0.0005779595417032706\n",
      "Saved CBOW_CrossEntropyLoss_SGD-lr0.0100 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Training CBOW_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.0009312253109145115\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.0009133367674137641\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.0008957306765143493\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0008788288983635887\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.0008635055264592314\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.0008506572977947545\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.0008402684801422021\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0008316435817131032\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.000824164407792758\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.0008175057650449249\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.0008114955992771635\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.000806039517121739\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.0008010515824201653\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.0007964829616093654\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.000792257994494778\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.0007883645863578011\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.0007847610877163323\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.0007814231272577002\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.0007783226052221457\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.0007754112336729956\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0007726718253785523\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.0007700920128070128\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.0007676389674364575\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.0007653123255159513\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.0007630792585235749\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.0007609355833235712\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.0007588774350622518\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.0007568888086984594\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.0007549727051774108\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.0007531126193004128\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 30 | Loss 0.0007513135981116938\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 31 | Loss 0.0007495646381454583\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 32 | Loss 0.0007478731508270149\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 33 | Loss 0.0007462144465616237\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 34 | Loss 0.0007446107141563437\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 35 | Loss 0.0007430416290276599\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 36 | Loss 0.0007415022805379444\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 37 | Loss 0.0007400101742009632\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 38 | Loss 0.0007385472588765471\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 39 | Loss 0.0007371277663200436\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 40 | Loss 0.0007357294622555268\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 41 | Loss 0.0007343654871855376\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 42 | Loss 0.000733031748912053\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 43 | Loss 0.0007317228821087758\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 44 | Loss 0.0007304436155378662\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 45 | Loss 0.0007291953132653319\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 46 | Loss 0.0007279593330557342\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 47 | Loss 0.0007267709132809389\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 48 | Loss 0.0007255872222683038\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 49 | Loss 0.0007244364507153218\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 50 | Loss 0.0007233091410976725\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 51 | Loss 0.0007221992460560552\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 52 | Loss 0.0007211124946677023\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 53 | Loss 0.0007200414300384382\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 54 | Loss 0.0007189981468868646\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 55 | Loss 0.0007179698229925093\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 56 | Loss 0.0007169658704108251\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 57 | Loss 0.0007159746945807466\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 58 | Loss 0.0007149892932967675\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 59 | Loss 0.0007140414038280008\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 60 | Loss 0.0007130982885902611\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 61 | Loss 0.0007121739065256941\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 62 | Loss 0.0007112606188646566\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 63 | Loss 0.0007103740214285036\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 64 | Loss 0.0007094901975932329\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 65 | Loss 0.0007086201053557733\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 66 | Loss 0.0007077663819104064\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 67 | Loss 0.0007069352110230338\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 68 | Loss 0.0007061063135790075\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 69 | Loss 0.0007052911932016591\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 70 | Loss 0.0007044874400410419\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 71 | Loss 0.0007036991462951787\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 72 | Loss 0.0007029170817840841\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 73 | Loss 0.0007021543415414324\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 74 | Loss 0.0007013988308486215\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 75 | Loss 0.0007006483217311723\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 76 | Loss 0.0006999159092227589\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 77 | Loss 0.000699190317044384\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 78 | Loss 0.0006984849130429236\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 79 | Loss 0.0006977780540377215\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 80 | Loss 0.0006970757419192118\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 81 | Loss 0.0006963911171899357\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 82 | Loss 0.0006957100390322796\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 83 | Loss 0.0006950406918422899\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 84 | Loss 0.0006943808476454873\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 85 | Loss 0.000693723913456168\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 86 | Loss 0.0006930886679163713\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 87 | Loss 0.0006924409184381706\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 88 | Loss 0.0006918125841661464\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 89 | Loss 0.0006911904791288909\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 90 | Loss 0.0006905761947367463\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 91 | Loss 0.0006899632744106094\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 92 | Loss 0.0006893539915938264\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 93 | Loss 0.0006887658972690302\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 94 | Loss 0.000688174074497146\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 95 | Loss 0.0006875983022352862\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 96 | Loss 0.0006870165735518593\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 97 | Loss 0.0006864457573964941\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 98 | Loss 0.0006858895822162784\n",
      "Training | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Epoch 99 | Loss 0.000685333907193599\n",
      "Saved CBOW_CrossEntropyLoss_SGD-lr0.0010 (./data/embeddings/CBOW_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Accuracy 0.1773\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.0100 | Accuracy 0.1714\n",
      "Validation | CBOW_CrossEntropyLoss_SGD-lr0.0010 | Accuracy 0.1593\n",
      "Best model: CBOW_CrossEntropyLoss_SGD-lr0.0200 | Accuracy 0.1773\n",
      "Test | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Accuracy 0.2187\n",
      "Train | CBOW_CrossEntropyLoss_SGD-lr0.0200 | Accuracy 0.1891\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = cbow_create_embeddings()\n",
    "# embeddings = torch.load(DATA_DIR + 'embeddings.pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insepecting the embeddings\n",
    "In this section we try to understand the embeddings we created in the previous section.  \n",
    "We will identify which words the model believes are similar and take a look at the embeddings using the `Tensorflow Projector` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_similarity_cosine(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return torch.dot(word_a, word_b) / (word_a.norm() * word_b.norm())\n",
    "\n",
    "def word_vector_similarity_euclidian(word_a:torch.Tensor, word_b:torch.Tensor):\n",
    "    return (word_a - word_b).norm()\n",
    "\n",
    "def word_similarity_cosine(word_a:str, word_b:str):\n",
    "    word_a_idx = vocabulary[word_a]\n",
    "    word_b_idx = vocabulary[word_b]\n",
    "\n",
    "    word_a_embedding = embeddings[word_a_idx]\n",
    "    word_b_embedding = embeddings[word_b_idx]\n",
    "\n",
    "    return word_vector_similarity_cosine(word_a_embedding, word_b_embedding)\n",
    "\n",
    "def word_find_top_closest(\n",
    "    word: str,\n",
    "    top: int\n",
    "):\n",
    "    similarities = []\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        similarity = word_similarity_cosine(word, other).item()\n",
    "        similarities.append((other, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    similarities = similarities[1:top+1]\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def word_find_closest(\n",
    "    word_vector:torch.Tensor,\n",
    "):\n",
    "    closest_word = None\n",
    "    closest_distance = 1_000_000\n",
    "\n",
    "    for other in vocabulary.lookup_tokens(range(len(vocabulary))):\n",
    "        other_idx = vocabulary[other]\n",
    "        other_embedding = embeddings[other_idx]\n",
    "\n",
    "        distance = word_vector_similarity_euclidian(word_vector, other_embedding)\n",
    "\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_word = other\n",
    "    \n",
    "    return closest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar words\n",
      "king : ['powerful', 'particularly', 'slept', 'effort', 'repeated', 'priest', 'arrival', 'dress', 'would', 'suppose']\n",
      "queen : ['laid', 'listening', 'difficult', 'vast', 'conceal', 'foot', 'figure', 'note', 'fact', 'honest']\n",
      "man : ['begin', 'morning', 'hung', 'crossing', 'paces', 'slightly', 'guests', 'frequently', 'terror', 'hastily']\n",
      "woman : ['bad', 'branch', 'appear', 'shudder', 'usual', 'there', 'captain', 'wind', 'boy', 'gradually']\n",
      "he : ['hearts', 'begged', 'shudder', 'military', 'marriage', 'proper', 'yourself', 'skald', 'husband', 'beds']\n",
      "she : ['wanted', 'quick', 'provided', 'reality', 'larger', 'found', 'driving', 'daughter', 'relations', 'suppose']\n",
      "doctor : ['perhaps', 'thirty', 'evening', 'do', 'promised', 'plunged', 'beside', 'filled', 'tears', 'about']\n",
      "nurse : Not in vocabulary\n",
      "black : ['recognized', 'boots', 'lay', 'gave', 'belonged', 'doors', 'say', 'little', 'enormous', 'free']\n",
      "white : ['roof', 'iron', 'showed', 'interrupted', 'devil', 'journey', 'given', 'gentleman', 'century', 'gloom']\n",
      "slave : Not in vocabulary\n",
      "master : ['clothes', 'again', 'without', 'breath', 'setting', 'eaten', 'advantage', 'candle', 'hair', 'convinced']\n",
      "poor : ['clean', 'next', 'address', 'money', 'lower', 'raise', 'shoes', 'trembling', 'start', 'convinced']\n",
      "rich : ['stay', 'ran', 'glory', 'kind', 'reading', 'laughing', 'fully', 'smoke', 'handsome', 'sleeping']\n",
      "smart : Not in vocabulary\n",
      "dumb : Not in vocabulary\n",
      "strong : ['coffin', 'becomes', 'used', 'stones', 'arrived', 'asking', 'movements', 'peasant', 'sake', 'victory']\n",
      "weak : ['horse', 'smiled', 'growth', 'lawn', 'tried', 'comrades', 'cries', 'shook', 'faith', 'word']\n",
      "good : ['driving', 'proper', 'low', 'er', 'every', 'learn', 'd', 'gentle', 'fought', 'grown']\n",
      "bad : ['woman', 'stood', 'race', 'part', 'repeated', 'report', 'beg', 'lifted', 'saying', 'other']\n"
     ]
    }
   ],
   "source": [
    "def print_most_similar_words(words, top = 10):\n",
    "    print(f\"Top {top} most similar words\")\n",
    "    for word in words:\n",
    "        if vocabulary[word] == vocabulary['<unk>']:\n",
    "            print(word, ':', \"Not in vocabulary\")\n",
    "        else:\n",
    "            print(word, ':', [x[0] for x in word_find_top_closest(word, top)])\n",
    "\n",
    "print_most_similar_words([\n",
    "    'king', 'queen', 'man', 'woman', 'he', 'she', 'doctor', 'nurse',\n",
    "    'black', 'white', 'slave', 'master',\n",
    "    'poor', 'rich', \n",
    "    'smart', 'dumb', \n",
    "    'strong', 'weak',\n",
    "    'good', 'bad',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_projector_create_data():\n",
    "    e = embeddings.cpu().numpy()\n",
    "    e = pd.DataFrame(e)\n",
    "    e.to_csv(DATA_DIR + 'tensorflow_projector/embeddings.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "    v = vocabulary.lookup_tokens(range(len(vocabulary)))\n",
    "    v = pd.DataFrame(v)\n",
    "    v.to_csv(DATA_DIR + 'tensorflow_projector/vocabulary.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "tensorflow_projector_create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugating _be_ and _have_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training be/have models\n",
      "Context size: 0 torch.Size([124029, 20])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.000324812972610693\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0003016474884618561\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00028465627832943805\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00028068503050642706\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00027945843074628717\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.00027909584243174414\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.0002789509793422966\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.00027859288147596154\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.0002781764692959125\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.00027793044809580825\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.00027697533591328064\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.0002747609913298692\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.0002731832446711681\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.0002720208029587114\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.00027142883270762913\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.00027069824293549793\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.0002699204111876998\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.0002690730390056679\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.0002687674732320667\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.0002687907866549544\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.00026781497535157794\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.0002675233422700191\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.00026640903447157203\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.0002656094578821095\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.0002650964703090954\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.0002646890697819569\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.000264159935049291\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.00026340793875693575\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.0002625042514333373\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.0002628937824371279\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 1 torch.Size([124029, 18])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.0002625999964009488\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0002618019268797204\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.0002609422520998586\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0002603306899617321\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00025956405357795923\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.0002596102805893052\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.0002586093104733665\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.0002582797238775023\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.0002574345507848325\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.00025677121008808927\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.00025590942848834904\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.0002556054159177239\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.0002547101558736389\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.0002545962030612391\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.0002535630001389859\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.00025326543105397417\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.000252955159536703\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.00025231357905987145\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.00025167687348743006\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.00025097382615619064\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.0002504027241866774\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.00025000145958020654\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.0002497998815147629\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.0002490568661188235\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.00024911110519700626\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.00024834952825659095\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.00024746447936870486\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.00024704534523888597\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.0002462021713183636\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.0002457106056438209\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 2 torch.Size([124029, 16])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.0002454814543809909\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00024428902508632366\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00024336809874751601\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00024279262935577315\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00024208956664628647\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.00024086342823356521\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.0002390884709316502\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.00023815322744461792\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.00023699243120462094\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.00023582514534426866\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.0002349401579693717\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.0002333464640682336\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.00023292470025812858\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.00023204283466743097\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.00023095125591847472\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.00022996755557423975\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.00022884963388768013\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.00022852269235034938\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.00022774769020005213\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.00022707043218951376\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.00022599392412289918\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.00022553852808597654\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.00022582181077925828\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.00022499052424388902\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.0002246164945133642\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.00022348471702599858\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.00022394829429047813\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.00022242543259634592\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.00022251702543718983\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.00022128330554855587\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 3 torch.Size([124029, 14])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.000221576614859069\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00021978188113114234\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00021846937848165487\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00021802360922753417\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00021819276994769588\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.0002173008469832723\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.00021714214347126604\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.000215511803206842\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.00021585157020241044\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.00021495846311294574\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.00021470398387683337\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.0002141024637341856\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.00021347250921230335\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.00021309544996706288\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.0002133101149209481\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.00021289656309489457\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.00021225376773652735\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.0002124936837724585\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.00021250649385244895\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.0002115032016211698\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.0002110682125183976\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.000210878137381925\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.0002113072212377388\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.00020933154705222827\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.00020995493506251878\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.00020890816852615806\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.0002089279449521697\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.00020877439315300473\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.0002081894353826855\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.00020794096904126253\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 4 torch.Size([124029, 12])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00021018453229452007\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00020663815468756615\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00020537756900092103\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0002050295900213011\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00020429581695198144\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.00020417482090232393\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.00020376451388644807\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.00020364513255275574\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.00020325161858291774\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.0002027311264252274\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.00020283057755043522\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.00020206435637933908\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.00020179236131956632\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.00020089182653666178\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.0002011106589955619\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.0002005463388331024\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.00020075532921373857\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.00020019696043297933\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.0001999354379596093\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.0002000024717395352\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.00020002883005538585\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.00019887406208832556\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.00019894561707295395\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.00019885488541395813\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.00019781911432440794\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.0001980074209624425\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.0001977514039016014\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.00019697807798025845\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.00019782940237184323\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.00019710253413555563\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 5 torch.Size([124029, 10])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00020308353434006612\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00019685625150524746\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00019450365647872203\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00019458348496039106\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00019380283899333932\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.00019337709221718685\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.00019291974314285872\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.00019245842648073043\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.00019249276610692326\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.0001922140507530859\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.0001918808502691092\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.00019132557251605472\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.00019124356032327087\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.00019130545776860277\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.00019100329058765206\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.00019103546188097736\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.00019016099322722503\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.00018983897272902627\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.00019006358740890645\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.00018947543096315146\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.00018957269837724443\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.0001894000160384539\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.00018906137165493743\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.00018856394686817826\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.00018822267280437565\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.00018787349432146727\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.00018827498960164747\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.00018795080077058118\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.00018794717150422136\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.0001879573826604202\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 6 torch.Size([124029, 8])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00020253879068640355\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00019045325681692261\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00018689316181338828\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00018632607356641704\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00018561255364877492\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.0001851291538235419\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.00018480737937729975\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.00018468123161480095\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.00018419191116436228\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.0001835828095458018\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.00018421324079334993\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.00018333783406651304\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.00018340588281075992\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.00018312859763392028\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.00018268758025821142\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.0001827714993536589\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.00018269975983006305\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.00018259664868200084\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.0001824864789184337\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.00018208899736079552\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.00018199026901321024\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.00018114263540097445\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.00018146239529682196\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.0001818555248104778\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.00018138744171954303\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.00018111850693098048\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.0001808324715314343\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.0001808459274978108\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.00018048018664257388\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.00018021783374385037\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 7 torch.Size([124029, 6])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.00020806948505384357\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.00018719542126382268\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00018347659199178753\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00018175192155850238\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00018047298962284338\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.00017948678262430057\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.00017952867296991152\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.00017876997176173893\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.0001783049335637671\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.00017818178455948936\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.00017779130010436698\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.00017788642994208702\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.0001777245738893868\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.00017703229133124863\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.0001769079120671879\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.00017626474763088578\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.00017637611689774133\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.00017610738202639354\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.00017558549044820006\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.00017535232546282806\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.0001753008852256516\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.0001752278846857782\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.0001752357122136475\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.00017472172505455976\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.00017517398392903575\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.0001744582956785261\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.0001741439181692292\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.000174471505592946\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.0001738094720472222\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.00017346333845727675\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 8 torch.Size([124029, 4])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.0002361662502473511\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0001973599352386526\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00018819709877913665\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00018423655421623768\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.0001818316423924404\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.00018050400754762212\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.00017940706179036254\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.0001786745958720623\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.0001776898651852591\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.0001772113248861689\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.00017681441232368034\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.00017578663792271963\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.00017613375572049148\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.00017483489357634794\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.00017451891672933315\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.00017433076387377147\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.00017407031787771156\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.0001735272812094979\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.00017305307757614272\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.00017325611657507866\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.00017261581249277197\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.00017316643063689836\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.0001725626960266412\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.00017200517304948276\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.00017173803751585284\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.0001715713373152571\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.00017129232977472128\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.00017121688409352915\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.00017034535268500872\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.00017071070908406344\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Context size: 0 torch.Size([124029, 20])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.0003135895586623926\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.0002844423053966808\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.0002804240001369702\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.00027973271716490573\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.00027902051977653035\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.0002788840839665963\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.0002785241714670813\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.0002783378178664522\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.0002780745422728914\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.0002777105699160926\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.000275777001371655\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.0002735013283380613\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.00027144664071798795\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.0002699340055583019\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.0002679982840592441\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.00026771550884812284\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.0002660297146239803\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.0002643295878733659\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.00026382044486217374\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.00026308084343713216\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.00026192867439386343\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.0002616809308300629\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.00025988438246895635\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.0002592448934337559\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.0002599335928602761\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.0002582936565695447\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.00025618506819272894\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.00025612286318245126\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.00025572032218145553\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.00025503674371128205\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 1 torch.Size([124029, 18])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.00025493364794146713\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.00025398876229338566\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.0002540548272437324\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.00025422484914574227\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.0002534769588454127\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.0002530305283266587\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.00025224765251375027\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.00025159061689839483\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.00025148346127129605\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.0002505804505905782\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.00024918249102091455\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.00024886780594667187\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.00024816274406503784\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.00024737783832348747\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.0002468719708789675\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.0002474212203590853\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.00024666421075811437\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.0002462271609702056\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.0002443515222832998\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.00024243762251714287\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.00024071370561797481\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.00023891506581523713\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.00023799756882557455\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.00023715680928987625\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.00023627715816678806\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.00023554276996757688\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.0002351252505537232\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.0002347663222620346\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.00023518121199560214\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.0002336160601214295\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 2 torch.Size([124029, 16])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.0002322799826188504\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.00023006200876907914\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.0002292962950801435\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.00022977660387767175\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.00022816274909433982\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.00022813205411275413\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.00022734484163411064\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.00022694320794970486\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.00022599978323511568\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.00022483163619291531\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.0002248868287224299\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.00022487561798015737\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.00022294329507375007\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.00022212706384247522\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.00022157638418535968\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.00022103997554172688\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.00022066975961652932\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.00022050813423753844\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.0002200499547378568\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.00021867919928764454\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.00021785075772802348\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.00021723204470490835\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.00021702163952552175\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.00021633830710730487\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.0002154932109058715\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.00021556082905919432\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.00021547271170223735\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.00021435538976732194\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.00021347044852716685\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.00021324935546591548\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 3 torch.Size([124029, 14])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.00021491638822836734\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.00021218694925206346\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.00021125568873107863\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.0002112068166611992\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.00021094192635167324\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.0002104398265777387\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.00020922279208741157\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.0002088615416803826\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.00020851733037134806\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.0002076477827567277\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.0002075339222138116\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.00020672802516471386\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.00020642264393008013\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.00020658648377668037\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.00020606939021164058\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.00020588506653965348\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.0002051343158853285\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.00020464365752737577\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.00020396155536894187\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.00020395578852620908\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.00020413454527267816\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.00020364490187904644\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.0002031647929987329\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.0002036409035347517\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.00020424333099398944\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.00020220359103027946\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.00020193279547379511\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.00020173117127360966\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.00020118309054028568\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.00020109415813622248\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 4 torch.Size([124029, 12])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.00020549296736856588\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.00019935555501089495\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.00019794958337439447\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.0001979424632459004\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.0001974125288443756\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.00019666614561228025\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.00019719474210629103\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.00019604165036818505\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.00019651765325647284\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.00019608786200128378\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.00019584594679320524\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.00019505934944445325\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.00019492223699163854\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.0001943566250564069\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.00019409848579744014\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.00019469788374196226\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.00019415363219221285\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.00019343628309099618\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.00019208264363001484\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.00019213228461225864\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.00019173140446196996\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.00019237267737385574\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.00019180129859589133\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.00019187546788255859\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.0001911731740854363\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.00019092769112398706\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.00019042107014535\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.0001905073267343852\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.00019052670332596737\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.00019040746039650065\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 5 torch.Size([124029, 10])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.00019904043934572825\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.00018972106770707354\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.00018792510371936388\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.00018672586186114834\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.0001863757914398973\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.00018646555426931404\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.00018534255776114954\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.00018532985532889013\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.00018541566594875399\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.00018411820322511362\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.00018424368972297904\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.00018391624070348783\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.0001843102621554863\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.00018299093156420323\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.00018290684330803556\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.00018234853604026546\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.00018338852076957242\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.00018260439931863372\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.0001829569610159453\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.0001820287146314288\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.00018159621680471718\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.00018107354093591205\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.00018018247915333656\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.00018034456587974606\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.0001799731350730128\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.0001797994070134067\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.00017952176813687947\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.00017928786499563768\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.00017945804068012037\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.00017882594858186518\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 6 torch.Size([124029, 8])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.00019813123123135358\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.00018230755301124448\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.00017963544414082813\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.0001779455439246599\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.00017684389242373036\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.00017686576029137308\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.00017600756182325082\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.00017586600505697003\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.00017506302987485694\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.0001749466319211384\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.00017482426720747234\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.00017468454045261876\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.0001741732752433009\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.00017389971160230484\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.00017332762542496518\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.00017372552219528013\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.0001727137719279929\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.00017292337743852055\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.00017229814403855556\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.00017230185019615185\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.00017207440591877077\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.00017164842846890897\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.00017189241974037133\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.0001708831299926501\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.0001710568272957616\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.00017041477009326417\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.0001712095025348312\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.0001712968356011765\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.00017088076174256783\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.000170069882141349\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 7 torch.Size([124029, 6])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.00021007485463486614\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.00018356097243465365\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.00017567945153912625\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.00017359068572306394\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.00017135075173616623\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.0001707051421585454\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.00016985810829795383\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.00016942810174730273\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.00016915530701867103\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.000168209268002043\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.00016860937923996732\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.00016769832573591817\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.00016746214661407784\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.00016750766622604862\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.00016804296763587673\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.00016796752195468462\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.0001670874863754142\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.0001671019573061117\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.0001663404572569328\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.00016607464425256958\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.0001661482906788291\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.0001656436534940921\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.00016582917666936765\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.00016547324713590012\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.0001652518311314557\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.00016549509962529555\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.00016601915953635653\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.0001652024208229212\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.0001648262535160234\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.00016460306901314094\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 8 torch.Size([124029, 4])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 0 | Loss 0.00022095098162467453\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 1 | Loss 0.00019126495146524767\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 2 | Loss 0.00018223928897153558\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 3 | Loss 0.00017925923069918848\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 4 | Loss 0.00017691461698300523\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 5 | Loss 0.00017555131998272738\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 6 | Loss 0.00017452603685782722\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 7 | Loss 0.00017309450657433507\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 8 | Loss 0.00017290237075272592\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 9 | Loss 0.00017185366655720702\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 10 | Loss 0.000171097564272826\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 11 | Loss 0.00017011887723720676\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 12 | Loss 0.00016965002523390763\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 13 | Loss 0.00016884383599811146\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 14 | Loss 0.0001684859688054857\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 15 | Loss 0.00016779181010117845\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 16 | Loss 0.0001680052140387861\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 17 | Loss 0.0001674801545416514\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 18 | Loss 0.00016712929982978873\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 19 | Loss 0.00016701025681753676\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 20 | Loss 0.00016694922055405294\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 21 | Loss 0.00016593891584201073\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 22 | Loss 0.0001660720607070253\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 23 | Loss 0.0001655533831825149\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 24 | Loss 0.00016587726444863545\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 25 | Loss 0.00016550561834644014\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 26 | Loss 0.00016496273546069934\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 27 | Loss 0.00016473670598206872\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 28 | Loss 0.00016451965739985394\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Epoch 29 | Loss 0.00016410399875392204\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020.pt)\n",
      "Context size: 0 torch.Size([124029, 20])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.0003031498201958596\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.00028178925017465357\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.00027977879039377886\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.00027883019858810114\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.0002782439798015043\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.0002780810934062359\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.0002758255351200941\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.00027380209609850884\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.00027258807574465007\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.0002704482541475935\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.0002692827982986684\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.00026908764834059095\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.00026914710064460416\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.0002675807031324012\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.0002658465289422925\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.0002638427433207405\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.00026367662749354173\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.0002648564773819278\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.00026322336903299203\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.000261919662740953\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.00026131852705448737\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.00026001691220407943\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.00025978909884876345\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.0002586579672477839\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.0002577775933470732\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.00025751465607495275\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.00025677859164678727\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.00025540668282802845\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.0002552788742148226\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.00025443899123921967\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 1 torch.Size([124029, 18])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.0002546487197757256\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.0002537443558092465\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.00025222599994156956\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.0002513041201514301\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.0002505583674274735\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.0002499231227885244\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.0002487745983898628\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.0002496411626245094\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.000248162590282565\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.0002467519590371364\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.000245620996596877\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.0002450647038795016\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.0002446016648536771\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.00024208475325488552\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.0002398519086399873\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.0002386007190624348\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.00023726351894780374\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.0002375147995084803\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.00023511543923195383\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.0002331407184977751\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.00023304914103517845\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.00023467442185624493\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.00023239032154313773\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.00023025702032293102\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.0002285497273090807\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.00022788844729747397\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.00022691701879457439\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.00022633538272566925\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.00022616130096637555\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.00022446592609417297\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 2 torch.Size([124029, 16])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.00022366358142019866\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.00022137543662805487\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.00022021806973720296\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.00021920390508509163\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.00021808893602201116\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.00021774207965444321\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.0002173074596296059\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.0002154185802717856\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.0002151335598365604\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.000214601933827834\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.0002131086752597301\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.00021406564821017948\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.00021260437639643344\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.00021179414268160068\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.00021193582247385975\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.00021095764292040098\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.00021067878916233805\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.00020940353262778073\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.00020964500186668793\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.00020900563585746578\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.00020862147186197853\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.00020796091462799433\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.00020778606395633625\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.00020744774251601282\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.00020608261550430777\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.00020543000882417117\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.00020517237704736487\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.00020402828158392204\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.00020336426424429814\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.00020325686256524275\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 3 torch.Size([124029, 14])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.0002062130999325416\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.00020221583211512024\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.00020033805585184148\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.00019899887194331031\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.0001999270260583431\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.0001984292001507948\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.00019750801238178325\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.00019694532231353623\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.0001981993568668369\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.00019691155168249304\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.00019586570784096958\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.0001951149418083973\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.0001948031324663974\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.00019420059735942866\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.00019472168926876318\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.0001950743739920531\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.00019475598276021415\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.00019273222079543582\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.00019216027302232177\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.0001914726039383698\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.00019199391114316637\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.0001922985696001776\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.00019140135651868716\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.00019104528858099405\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.000190533746563225\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.0001893871905802162\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.00018905608153787056\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.00018925658312600406\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.00018981196852678955\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.00018874179629805736\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 4 torch.Size([124029, 12])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.00019746812120831965\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.00018898143552553739\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.00018675317362833082\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.00018597142042747438\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.00018618084139903458\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.00018566482431130491\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.0001845741528789386\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.00018399177865416366\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.0001851613251168672\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.00018476668853497722\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.00018428510334292408\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.00018229811076740998\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.00018236728212370885\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.00018346802630804843\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.00018299926657423303\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.00018242293600064205\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.00018495576408537613\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.00018320587332653962\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.00018203194406335918\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.00018184674383127668\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.0001810768626373261\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.00018185263369998775\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.00018114617239785056\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.00018020814544805928\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.00018052056991995068\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.00017977798511493531\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.0001801400967038124\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.00018081575537663288\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.00018243749920082326\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.00018013632903322697\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 5 torch.Size([124029, 10])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.0001970918154971963\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.00018260618319531906\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.0001790482103899104\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.00017853054782972097\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.000178121947799294\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.0001771855048089733\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.00017767879284721213\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.00017628841475346113\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.00017594929364427877\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.00017571305300944926\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.00017549769603443613\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.00017570877785670337\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.00017504066990330102\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.00017526293171134623\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.0001750428997491577\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.0001750613690241499\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.00017530000866555623\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.00017446457000341937\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.00017479203440115787\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.00017404292921929264\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.00017445214437961112\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.00017433806854123298\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.00017352182193171084\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.00017434995592638616\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.0001742716037564567\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.0001756426975281093\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.00017481933079009307\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.0001732876266037706\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.0001732855505403868\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.0001735242978295241\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 6 torch.Size([124029, 8])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.00020661674816734203\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.00018115526094199743\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.00017708733007829034\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.00017462142812575115\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.00017335925847963545\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.00017362877764159492\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.00017231124630524445\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.00017275433974433713\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.00017169307152078438\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.00017089274139720474\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.0001710784798679423\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.00017078094153942514\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.00017053304419315177\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.00017043051741848648\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.0001698118966648551\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.00016953673368614114\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.0001698556785348824\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.00016946253364297928\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.0001695619847681871\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.00016888186640365326\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.00016888040547016097\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.00016863659873766606\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.00016883028776225122\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.00016860135179488328\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.00016803372530925699\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.00016777335620443354\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.0001678832491595495\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.00016748332246059263\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.00016818633903533745\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.0001674141203477992\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 7 torch.Size([124029, 6])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.00021625308086083793\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.00018365970078223893\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.0001768155656922269\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.00017322955834201325\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.0001719012622325616\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.00017090347541381137\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.00016995185409341798\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.00017001707324016397\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.00017011683193031752\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.000168520600618377\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.00016893484446555847\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.00016779896098616712\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.00016676869530914582\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.00016710494068608545\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.00016733003209163156\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.0001668167984666609\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.00016668720597676974\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.00016648219856218102\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.00016577084697740643\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.0001654588377181918\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.000165271161588296\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.0001652709309145867\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.0001650052255579545\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.0001656587087981865\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.0001651550865777705\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.00016500794750772438\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.0001645564114108709\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.00016469001762330407\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.0001639900305632749\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.00016439776941185377\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 8 torch.Size([124029, 4])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 0 | Loss 0.00025043034351880563\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 1 | Loss 0.00020088833567452754\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 2 | Loss 0.00018469985467226604\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 3 | Loss 0.0001784458444436618\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 4 | Loss 0.00017478740554872436\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 5 | Loss 0.00017275761531100936\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 6 | Loss 0.00017102251842606334\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 7 | Loss 0.00017036943502026083\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 8 | Loss 0.00016964352023530503\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 9 | Loss 0.0001691817576040054\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 10 | Loss 0.00016870234074481982\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 11 | Loss 0.00016811050889796312\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 12 | Loss 0.00016739963403885445\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 13 | Loss 0.00016690288589497592\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 14 | Loss 0.00016711755084886115\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 15 | Loss 0.00016649136399756432\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 16 | Loss 0.0001663762731948652\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 17 | Loss 0.00016558863012529769\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 18 | Loss 0.00016518253674917857\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 19 | Loss 0.00016554733953133094\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 20 | Loss 0.0001652096485991463\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 21 | Loss 0.0001648744489430222\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 22 | Loss 0.00016467476240199496\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 23 | Loss 0.00016414055284772424\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 24 | Loss 0.0001642409420460166\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 25 | Loss 0.0001640502210231579\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 26 | Loss 0.00016398344867343587\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 27 | Loss 0.00016357037357304826\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 28 | Loss 0.0001632253472169075\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Epoch 29 | Loss 0.00016337828388618097\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030.pt)\n",
      "Context size: 0 torch.Size([124029, 20])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.000336271458447029\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.00033584972539341855\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.0003353962208809123\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0003349457920178635\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.00033453974477648627\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.0003341146285084726\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.0003337457658690363\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0003333181275684675\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.000332887352105452\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.0003325111079073178\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.00033216275984976297\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.00033175403679335766\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.0003313916637742767\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.0003309911834584175\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.0003306283490919179\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.00033021199842485805\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.0003298613436302101\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.00032950315349439133\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.00032914616286186093\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.00032879972170696975\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.00032844506856802703\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.0003280608584377979\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.0003277043291526862\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.00032734733852015583\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.00032705628981199395\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.00032667008050961746\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.0003263529502940561\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.0003259843029500819\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.00032565548526658207\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.0003253004322932099\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 1 torch.Size([124029, 18])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.0003250225473647261\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.00032470194166527776\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.00032440381896336367\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0003240765391045926\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.00032371930242010565\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.00032342099517922407\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.00032306517329348756\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0003227846125499757\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.0003224432154601948\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.000322140294745127\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.00032182611715304485\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.00032151775253843733\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.0003212080961510577\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.0003208486449989613\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.0003205369586829397\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.00032022825574689186\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.00031996430351045044\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.00031966276683763854\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.0003193503423657471\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.0003190430542284497\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0003187969100023671\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.00031847365924438537\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.00031812694128104303\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.0003178371843456533\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.00031753875407879346\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.0003172432456789182\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.0003170258587752631\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.00031659862030912375\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.0003163779117040546\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.00031607975824564594\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 2 torch.Size([124029, 16])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.0003157161242102874\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.0003154357172492484\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.00031516112326568406\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0003148098840976392\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.00031454540900079\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.0003142891766444869\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.0003140360506941358\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0003136656502299708\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.000313410986454891\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.00031314611152361227\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.0003128779456474141\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.0003125825602735172\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.00031230196877351076\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.00031203835485850965\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.00031176554475163067\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.0003114183346843751\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.00031116148719818046\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.00031087259144463883\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.000310664154680905\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.00031030830203867394\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0003100893773102901\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.0003097771373773661\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.0003094868575815686\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.00030926627200247777\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.0003090697072456498\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.0003087384905555732\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.0003084807203745413\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.00030815291765536246\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.00030791360137107554\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.00030763968403039186\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 3 torch.Size([124029, 14])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.0003072777416022349\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.0003071130713302811\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.0003068451515060395\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0003066797738347105\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.00030620963005863926\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.0003061720148657742\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.000305750989211539\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0003056073563818744\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.000305346079960461\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.00030493926380671936\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.00030479237078862985\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.0003045806430799765\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.0003041708127897665\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.00030391553388479524\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.0003037206914916635\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.00030341392621477387\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.0003030500153709641\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.0003029735239689565\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.00030282063343442484\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.00030258676104967763\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0003024144631670693\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.0003020531358688039\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.00030189975323035906\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.0003014671631341637\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.00030131700992764925\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.0003010162114107071\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.0003007955643186271\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.0003006974511009333\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.00030055160380365934\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.00030018151090444007\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 4 torch.Size([124029, 12])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.00029971610362853333\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.00029966458650012044\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.00029928818851951335\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0002990003692432818\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.0002989163117436087\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.0002987534253483403\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.0002985629503774382\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0002983387970449766\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.0002980523310545064\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.00029804956296999467\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.0002978869226266828\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.00029762130953953433\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.0002972361459579734\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.00029715891640009595\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.0002969314721227149\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.00029654855376525787\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.00029650321869225456\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.0002964328785891619\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.00029602606243542024\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.0002959413282928665\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0002956455430845401\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.00029561598609325365\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.00029530891325141827\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.0002952021882152435\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.0002949771429444393\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.0002948316032121111\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.00029473425890678165\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.00029452935913992393\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.0002942227784020017\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.00029415597529578515\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 5 torch.Size([124029, 10])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.00029375727885661126\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.0002937590012203075\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.0002935309418130349\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0002933924760744589\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.0002931485155594911\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.0002929627155757644\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.000292832308038767\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0002926061247776635\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.0002925398445318547\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.0002923550902689435\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.00029221868521550403\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.00029229680671172416\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.00029187599635295097\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.0002917986745255898\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.0002917757916936261\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.0002916952711908291\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.0002914004701903291\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.0002911932329298837\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.0002911022244624367\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.00029115755539617686\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0002908828076301397\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.0002907516004242833\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.0002909187619722977\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.0002902657400794843\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.0002905391653162548\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.0002902902530056605\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.00029023030859773405\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.00029001110706089906\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.0002898296437429074\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.00028981371187871763\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 6 torch.Size([124029, 8])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.00028969090119588024\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.00028957295003918565\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.00028964476645401796\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0002894114169296785\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.00028933344921593123\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.0002892495762552256\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.0002894122781115266\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0002891671180932704\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.00028864370406859547\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.00028895019253703395\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.00028859587771953155\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.0002886259575712258\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.00028854208461052017\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.00028855915446500917\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.0002883184848949609\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.00028840254239463403\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.00028826367682162856\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.00028789216912365886\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.0002880786149937716\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.0002879213262805158\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0002877189485462132\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.00028767770408698833\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.0002876813641098428\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.0002875332715884648\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.0002874304526271011\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.00028740212589559763\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.0002874076928211157\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.00028734461125074265\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.00028715339812397075\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.00028721444976570183\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 7 torch.Size([124029, 6])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.0002872869428234148\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.0002870109647975946\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.0002870575608868755\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.00028696615258499904\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.00028709237723873425\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.00028686139596447704\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.00028699244938786055\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0002866187887352706\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.00028686314908466784\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.00028691263628443877\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.00028635194538833913\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.0002867537789899596\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.0002864877660683817\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.00028654900224908025\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.0002863520684143174\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.00028634364113480393\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.00028608931568116443\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.00028616556103121547\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.00028597643934607467\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.0002859637061573207\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0002859575856149003\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.00028603106288043964\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.00028579506829756675\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.0002857783675210126\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.0002860105482985582\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.00028572116044110334\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.0002856911113459037\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.0002858057408011842\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.0002857629892737252\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.00028567936236497613\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 8 torch.Size([124029, 4])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 0 | Loss 0.00028568108472867235\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 1 | Loss 0.0002857151014116721\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 2 | Loss 0.00028545428633767735\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 3 | Loss 0.0002857539161078256\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 4 | Loss 0.00028564128582469247\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 5 | Loss 0.00028541494878111607\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 6 | Loss 0.0002855130312423153\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 7 | Loss 0.0002853637084611544\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 8 | Loss 0.0002852416974471759\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 9 | Loss 0.0002852948139133067\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 10 | Loss 0.0002850819789708487\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 11 | Loss 0.0002851130430303693\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 12 | Loss 0.00028517941554566183\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 13 | Loss 0.00028537877914349606\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 14 | Loss 0.0002850799797987013\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 15 | Loss 0.0002850755816199771\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 16 | Loss 0.00028478926941197973\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 17 | Loss 0.0002847512543846852\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 18 | Loss 0.00028474193516682904\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 19 | Loss 0.00028452873114643613\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 20 | Loss 0.0002847390748128336\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 21 | Loss 0.0002849240136147122\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 22 | Loss 0.00028475928182976925\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 23 | Loss 0.00028459344281102166\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 24 | Loss 0.0002843061463951979\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 25 | Loss 0.00028430522370036064\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 26 | Loss 0.0002844620203097033\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 27 | Loss 0.0002843154348565595\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 28 | Loss 0.00028432380062308384\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Epoch 29 | Loss 0.00028428529349187615\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010.pt)\n",
      "Context size: 0 torch.Size([124029, 20])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.00033008632738802516\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.0003263201638708393\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.0003227371860353413\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.00031927726493463724\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.0003160576750825412\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.00031298897659282933\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.00030991723321015463\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.0003070019173588876\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.00030438097191719655\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.0003017776191904023\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.0002993939293478617\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.0002973028875512009\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.00029522627055049563\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.0002936208122901826\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.0002921624315869266\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.00029095087175263384\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.000289651778934781\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.0002884942275049617\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.00028818466338706574\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.00028718692270305734\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.00028633026205966385\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.00028605745195278486\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.0002854556703799332\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.0002849325331637094\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.00028455810359875514\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.00028427105323488796\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.0002837760889676948\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.00028350804611747493\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.00028329348881132074\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.00028315290087461905\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 1 torch.Size([124029, 18])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.0002830005332004952\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.0002828606834196633\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.00028274414706171917\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.00028262511942771446\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.00028207860727561386\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.00028204083830027593\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.00028187137001516846\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.0002817843598920162\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.00028149103520325576\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.0002814837766705361\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.00028130975642423156\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.0002815782913783646\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.0002815336944612311\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.0002813914149173278\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.0002811828243711211\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.0002810436512331699\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.0002811592341397822\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.00028105247834711285\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.0002810225522778915\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.000280843826287917\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.00028094335430436126\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.0002807837896105069\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.00028061850420866164\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.0002805010759123748\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.00028041843321145217\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.00028058113506775317\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.0002806732507690049\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.0002805445348392091\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.00028057956648652987\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.0002806962566269469\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 2 torch.Size([124029, 16])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.0002804812687298686\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.00028030700243160747\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.0002802278352145718\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.0002800902306578439\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.0002802508410725138\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.00028006980834544617\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.00028032782457843464\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.0002801333205067433\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.00028000414322952886\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.0002801124676034215\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.0002799335878309741\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.00028007159222213153\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.00028021691665899775\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.0002800844484368638\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.00027991510317773463\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.00028019717098948065\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.00027998648900164286\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.00027980576383952103\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.0002798133299371864\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.0002800807884140094\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.0002799816602319946\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.0002800317318051625\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.00027971976868068973\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.0002799756934720471\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.000279650443541918\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.000279918117314203\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.00027976033649703395\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.0002798131146417244\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.000279730964044715\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.0002795719222112684\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 3 torch.Size([124029, 14])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.000279927159723608\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.00027976993252334134\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.0002796116596022591\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.00027964318500919833\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.00027965662559732756\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.00027968070793257967\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.0002795284940409287\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.00027961390482636304\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.0002795744442438235\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.0002796601010812145\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.0002795267101642433\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.00027969304128690415\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.00027949392374102656\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.00027953403020995217\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.0002797525858604011\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.0002795680776494465\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.00027931851945246615\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.00027964782923987913\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.0002795578664932477\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.00027952243501149746\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.0002793861222275417\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.00027950896366687366\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.000279441729969733\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.00027975027912330796\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.0002793810474059368\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.00027921816101066836\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.0002795677085715116\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.0002794656892790068\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.00027953147742090247\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.00027965441112971816\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 4 torch.Size([124029, 12])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.00027927186185019607\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.0002795776429192593\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.0002791460370308903\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.0002794603068924562\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.0002796456762852589\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.00027946894946743176\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.00027960400123510997\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.00027947534681830335\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.00027947162528245976\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.00027920499723099035\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.00027927395329182717\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.0002791209704878118\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.0002794339793331001\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.00027939138158811395\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.00027922501970895856\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.0002793908894842008\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.00027898407333045913\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.0002790788340902443\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.0002791970928118846\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.00027929400652628995\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.00027915338783309374\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.00027926897073970606\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.00027915015840116337\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.00027916965801872383\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.00027918531307446245\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.0002789817050803769\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.0002791608616612754\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.00027892757364992514\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.00027915990820994357\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.0002790861233794585\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 5 torch.Size([124029, 10])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.0002793788329383274\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.00027913019743618426\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.00027935687280120097\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.00027928459503895006\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.0002792281568714052\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.0002792763830548986\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.0002791779007592699\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.0002791967544904443\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.00027914511433605306\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.00027926318851872594\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.0002791081142730795\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.00027913493393634883\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.0002792925917275395\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.000279006002711091\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.0002790065563279934\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.0002789793368302946\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.00027909953321109316\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.0002790434333649886\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.0002790998100195443\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.0002791052539190841\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.0002792232973452624\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.00027895894527439147\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.0002792554686385877\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.0002788947872267083\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.0002792625426323399\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.00027911681836104423\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.00027903180741003933\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.000278941906176397\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.00027887894763200226\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.0002789141638182905\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 6 torch.Size([124029, 8])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.000279068868986002\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.00027918918839277886\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.00027913318081615804\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.00027907271354782386\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.00027915308026814795\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.00027903512911145337\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.00027897124787222143\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.0002790963037791628\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.00027921108701691615\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.0002790345447380565\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.00027881085275301354\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.00027906253314811957\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.0002791591700540738\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.0002788284762244049\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.0002788155584966835\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.0002788972169897797\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.00027909833370780475\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.00027894919546561127\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.0002790944583894883\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.0002787680397125653\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.0002786610071114448\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.00027888851290181507\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.0002789944075126363\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.00027877139217047397\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.0002788082384509747\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.00027871480022045625\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.0002789530707839277\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.00027890124609056906\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.0002788059317138816\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.0002786501500688599\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 7 torch.Size([124029, 6])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.00027893876901395034\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.000278942121471859\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.0002793122143710783\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.0002791185099682458\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.0002789799212036915\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.0002791015323832405\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.00027875776704337736\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.0002790334990172409\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.0002789795521257566\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.0002787245500292365\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.0002789195462048411\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.00027903832778688916\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.000279047893056702\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.0002792265267771927\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.00027873789834788196\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.0002787840638462388\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.00027850833187237524\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.0002788532352025377\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.0002789202536042163\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.0002787994113370317\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.0002789038603926079\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.00027876911618987543\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.0002789594681347992\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.00027879236809977406\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.00027892803499734374\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.00027883050615304687\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.0002787671477742227\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.00027863544846445313\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.0002787163072886904\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.0002787491244684018\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 8 torch.Size([124029, 4])\n",
      "Training BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 0 | Loss 0.00027862083912953006\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 1 | Loss 0.000278871227751864\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 2 | Loss 0.0002787117860839879\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 3 | Loss 0.00027861324227537005\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 4 | Loss 0.00027878378703778765\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 5 | Loss 0.0002786839206999031\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 6 | Loss 0.0002789744773041518\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 7 | Loss 0.00027868155244982083\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 8 | Loss 0.0002786606995464991\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 9 | Loss 0.0002785075322035163\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 10 | Loss 0.00027854170266898895\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 11 | Loss 0.0002784548770848041\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 12 | Loss 0.00027844934091578063\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 13 | Loss 0.0002785135912329475\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 14 | Loss 0.0002785117765997676\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 15 | Loss 0.00027877160746593604\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 16 | Loss 0.0002783563332761863\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 17 | Loss 0.00027832099406391975\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 18 | Loss 0.00027843380888602036\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 19 | Loss 0.0002787135084476841\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 20 | Loss 0.0002782173446772025\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 21 | Loss 0.00027821156245622244\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 22 | Loss 0.00027809536441971864\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 23 | Loss 0.00027817142523080225\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 24 | Loss 0.00027826169554237947\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 25 | Loss 0.00027797710569807827\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 26 | Loss 0.0002780451083075833\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 27 | Loss 0.0002779340466056735\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 28 | Loss 0.00027811206519627276\n",
      "Training | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Epoch 29 | Loss 0.00027811837027766063\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Saved BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 (./data/behave/BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100.pt)\n",
      "Context size: 0 torch.Size([124029, 20])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.0003326064222840053\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.0003307292303941234\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.0003287824058005241\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.00032684194780130177\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.0003248011621167762\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.0003225261657260632\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.0003200787176702689\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.0003173492018027286\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.0003144366232794787\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.0003111271014372458\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0003076991670908996\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.0003040153694661852\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.00030035057935511803\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.000296635071784497\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.00029330786495788345\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.00029032214749053457\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.00028822846063534033\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.00028612976047153035\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.0002846340106273659\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.0002834704001681153\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.00028267740546849174\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.0002819990094676541\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.0002814901432649131\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.00028099056552353366\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.0002806741734638421\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.00028077151776917153\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.0002804941249446009\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.00028015069792617807\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.0002799360483505401\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.00027971589336237327\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Context size: 1 torch.Size([124029, 18])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.0002798602028349185\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.00028007608267033947\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.00027962491565142086\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.00027951471513135915\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.0002793243324299408\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.00027963952498634393\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.00027946651970436035\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.0002793913200751248\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.00027918291406788556\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.00027908904524644315\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0002792205292607506\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.0002790516145925455\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.00027914532963151513\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.0002789064439381522\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.00027921099474743243\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.0002790096319774508\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.00027903082320221293\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.0002789010000386124\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.0002792373530632831\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.00027909119820106334\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.0002788368727474239\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.0002789081355453538\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.00027919004957462697\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.0002788501595530802\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.0002788267231042142\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.00027874869387747777\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.0002787612425272643\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.0002786013395119696\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.0002787404203804371\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.00027881881868510843\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Context size: 2 torch.Size([124029, 16])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.0002788127288991826\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.0002786363404027958\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.00027820371955010584\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.000278443312642844\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.0002786353561949694\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.00027842021451541827\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.0002787168609055928\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.00027878664739178314\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.00027826267975020587\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.00027864516751673876\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0002785122687036808\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.0002782283555022603\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.00027802416313477786\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.00027820547267029663\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.0002781416529440538\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.0002782092557191293\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.000278104160777167\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.00027799159200702305\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.00027793023280034624\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.00027785617116140996\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.00027759843173687265\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.0002773342949614638\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.0002770304207950642\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.0002772988942362081\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.0002766208365568108\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.00027664617990834047\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.00027584958669885166\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.0002758084960220997\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.00027516285568798425\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.00027487774298327536\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Context size: 3 torch.Size([124029, 14])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.00027482804048804235\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.00027471088900020673\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.00027457543739809907\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.0002743880995896436\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.00027404968587983644\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.0002738895675690797\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.0002738692067696712\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.0002732823728531829\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.0002735527224404959\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.00027330688577935906\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0002731547641571918\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.0002728962865767847\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.00027287749435859945\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.00027272826384692225\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.00027246074385711016\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.00027224661714188\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.00027237367222096874\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.0002722957660202106\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.00027230720743619247\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.0002717882531032309\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.00027174528628030984\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.00027153325100671077\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.0002714484245946733\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.00027150535486613134\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.00027159906990510094\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.00027142671050950346\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.0002712393419445534\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.0002709857854032783\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.00027132650585017854\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.00027095389091840415\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Context size: 4 torch.Size([124029, 12])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.00027136449012097854\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.00027134120745458533\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.00027121175336891975\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.00027093033144355984\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.00027101180539768863\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.00027097984939982535\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.0002706307324299061\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.00027068354133109114\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.00027035952166074506\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.00027041845110435046\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0002704851926975779\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.00027037529974246197\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.0002701205129414038\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.0002700221536717534\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.0002697201556515229\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.00026995984101374477\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.00026988215010844866\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.0002696549826395188\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.0002693274874852857\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.00026931321647180297\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.0002692655746617065\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.0002693727610452999\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.0002694625084964693\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.0002686150748014483\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.0002686666534428503\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.00026871214229832655\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.000268497246670732\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.00026823883060331406\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.0002679766314870634\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.0002682364931097264\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Context size: 5 torch.Size([124029, 10])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.00026883685988382755\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.0002684925716835566\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.0002683004051054529\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.00026824261365214675\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.0002678313070501972\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.00026760011048047796\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.0002673448930884958\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.00026724391951680657\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.0002668679521271235\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.0002666812602050541\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0002663566254048165\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.000266225787276895\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.00026547160727342494\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.0002655690130917435\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.000265047598239216\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.00026469506729839896\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.0002644141374769522\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.00026399098962459134\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.00026371325847858034\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.00026325138819954975\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.0002630802590637352\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.00026276740400091975\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.0002623202353262959\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.00026228262013343085\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.00026187918719409244\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.0002612273648045675\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.00026137834843643546\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.0002610105315178148\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.00026070364321494684\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.00026035523364440285\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Context size: 6 torch.Size([124029, 8])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.00026045251643674313\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.0002592271776928808\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.00025817696642912774\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.0002585317118375541\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.0002575267741338152\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.0002566824007100044\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.0002562631281759599\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.00025593394141452524\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.00025544252952245544\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.00025481987966803475\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0002542861007046881\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.00025395915916735735\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.00025304658321682676\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.0002526165459096811\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.0002521865239807827\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.0002514589791016145\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.00025076323643783663\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.0002499858506592098\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.00024991143532058596\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.0002493408100767386\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.0002489790675657964\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.00024798732439823014\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.000247791497797272\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.0002473774231108107\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.0002466658562305741\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.00024614133497209446\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.0002457599236828717\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.0002447925396590087\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.0002446534895470357\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.0002442709248892664\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Context size: 7 torch.Size([124029, 6])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.00024730250029002634\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.00024379144651709165\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.000242234106792541\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.00024142724091386422\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.00024081421784224544\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.00024040288048380129\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.00023971999403475574\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.00023913883469151652\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.00023905194759434254\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.0002384887808004296\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0002383895757271784\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.00023770079940942177\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.00023702384896382914\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.00023694726529233776\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.00023694466636854617\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.00023606589180555335\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.00023580438471043063\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.0002356807436022397\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.00023543110851402286\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.00023501915602568724\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.00023472186374912666\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.0002342219784428015\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.0002345084905680136\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.00023343007559873535\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.00023390813917215965\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.00023296654446899767\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.0002331446399508334\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.00023251563888028297\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.00023226100586169773\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.0002320753289039493\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Context size: 8 torch.Size([124029, 4])\n",
      "Training BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.0002471855025846636\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.00024124917618852307\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.0002377481951675616\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.00023559050404715705\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.0002345294972538082\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.00023327028023117168\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.00023282008204183221\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.00023221437901592225\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.00023190547616265967\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.00023095779167357186\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.00023081651171574227\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.00022990116768069994\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.0002292807476721359\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.0002292382883313753\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.00022897287516144158\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.00022820423960552132\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.00022818927657091066\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.0002275749002135306\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.00022736149627592292\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.00022668374616147138\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.00022651043331454204\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.0002260604504206646\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.00022549362360389726\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.00022532735399422558\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.00022521535421923125\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.0002244590366393882\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.00022407258128505513\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.0002240696901745651\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.00022347501335196021\n",
      "Training | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.00022361066487128263\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Saved BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Training BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.0003038649702077141\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0002745516626277927\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.00025719780266784254\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.00023878342801845676\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.00022253561773816034\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.0002088451177122796\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.00019943675215657256\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.00019327410409510296\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.00018920947955456264\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.00018602346041629502\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.00018319686167362918\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.00018112748782739635\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.00017966452440644866\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.00017767365651261813\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.00017565798350591298\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.00017449118974947393\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.00017271863145413573\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.00017170911103270517\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.00017017879088863857\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.00016973622030995366\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.00016794917570567072\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.00016704095179912245\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.00016574292008033245\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.00016504394798462426\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.00016405006724068503\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.00016296754627938105\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.00016191375188401\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.0001611912664481993\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.00016034161828556886\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.0001596101827098369\n",
      "Saved BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 (./data/behave/BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Training BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.00033113616957384335\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.00032377558680517763\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.0003124636556654908\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.0002991815865093169\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.00028925495864177617\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.00028471935989981113\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.00028177168821625135\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.0002798375045419223\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.00027811083493648977\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.00027655898524622075\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.00027481475368238605\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.00027267453225089997\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.0002698063045928271\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.00026635459547617456\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.00026252101372288057\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.00025763845096561655\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.0002532392111423491\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.00024796311903699974\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.00024331530522457828\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.00023899134192178277\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.0002351605743877424\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.00023055020660746586\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.00022787988161373487\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.00022437387190591041\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.000221497109320593\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.00021838851965741757\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.00021604610503059643\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.0002134093199941993\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.0002117393038517737\n",
      "Training | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.00020936602508264668\n",
      "Saved BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 (./data/behave/BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Training BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001...\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 0 | Loss 0.00033201820432526114\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 1 | Loss 0.0003320032259124032\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 2 | Loss 0.00033196625660592417\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 3 | Loss 0.00033195730646600293\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 4 | Loss 0.0003319136630002012\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 5 | Loss 0.00033189065714225917\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 6 | Loss 0.0003318586396314068\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 7 | Loss 0.00033184830544922964\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 8 | Loss 0.00033179847992801834\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 9 | Loss 0.0003317723369076297\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 10 | Loss 0.00033176043414422926\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 11 | Loss 0.0003317356444096019\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 12 | Loss 0.00033170762524304423\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 13 | Loss 0.00033167785295629577\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 14 | Loss 0.0003316496492507706\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 15 | Loss 0.0003316090814344264\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 16 | Loss 0.00033159847044379807\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 17 | Loss 0.0003315657147770758\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 18 | Loss 0.00033157408054360017\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 19 | Loss 0.0003315288377400806\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 20 | Loss 0.000331508046349748\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 21 | Loss 0.0003314463488216308\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 22 | Loss 0.0003314667711340285\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 23 | Loss 0.000331412116843169\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 24 | Loss 0.0003314035357811826\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 25 | Loss 0.0003313630602343221\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 26 | Loss 0.00033134251489594607\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 27 | Loss 0.000331322553930967\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 28 | Loss 0.00033128690715375476\n",
      "Training | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Epoch 29 | Loss 0.0003312645471821988\n",
      "Saved BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 (./data/behave/BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001.pt)\n",
      "Validating be/have models\n",
      "Validation | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.1881\n",
      "Validation | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0020 | Accuracy 0.1939\n",
      "Validation | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0030 | Accuracy 0.1981\n",
      "Validation | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0010 | Accuracy 0.2514\n",
      "Validation | BeHaveRNN_CrossEntropyLoss_SGD-lr0.0100 | Accuracy 0.2518\n",
      "Validation | BeHaveRNN_CrossEntropyLoss_Adam-lr0.0001 | Accuracy 0.2024\n",
      "Validation | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.4986\n",
      "Validation | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0001 | Accuracy 0.4353\n",
      "Validation | BeHaveMLP_CrossEntropyLoss_SGD-lr0.0001 | Accuracy 0.0626\n",
      "Validation | Always_<unk> | Accuracy 0.0000\n",
      "Best model: BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.4986\n",
      "Test | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.4493\n",
      "Train | BeHaveMLP_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.5447\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BEHAVE_CONTEXT_SIZE = 20\n",
    "BEHAVE_BATCH_SIZE = 8192\n",
    "BEHAVE_EPOCHS = 30\n",
    "BEHAVE_WORDS = ['<unk>', 'be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']\n",
    "BEHAVE_WORDS_SIZE = len(BEHAVE_WORDS)\n",
    "\n",
    "class BeHaveRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BeHaveRNN, self).__init__()\n",
    "\n",
    "        # self.rnn = nn.RNN(EMBEDDINGS_DIM*BEHAVE_CONTEXT_SIZE, EMBEDDINGS_DIM, batch_first=True).to(device)\n",
    "        self.rnn = nn.RNN(EMBEDDINGS_DIM, EMBEDDINGS_DIM, batch_first=True).to(device)\n",
    "        self.fc1 = nn.Linear(EMBEDDINGS_DIM, BEHAVE_WORDS_SIZE * 4).to(device)\n",
    "        self.fc2 = nn.Linear(BEHAVE_WORDS_SIZE * 4, BEHAVE_WORDS_SIZE).to(device)\n",
    "        self.hidden = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = embeddings[x]\n",
    "\n",
    "        # Reset the state if its incompatible with current input\n",
    "        if self.hidden is not None and self.hidden.size(1) != x.size(0):\n",
    "            self.reset()\n",
    "\n",
    "        x, hidden = self.rnn(x, self.hidden)\n",
    "        x = x[:, -1, :] # Keep only the last output\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        self.hidden = hidden.data\n",
    "\n",
    "        return x\n",
    "\n",
    "class BeHaveAlways(nn.Module):\n",
    "    def __init__(self, word):\n",
    "        super(BeHaveAlways, self).__init__()\n",
    "        self.name = \"Always_\" + word\n",
    "        self.label = BEHAVE_WORDS.index(word)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.zeros(x.size(0), BEHAVE_WORDS_SIZE)\n",
    "        x[:, self.label] = 1.0\n",
    "        return x.to(device)\n",
    "\n",
    "class BeHaveMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BeHaveMLP, self).__init__()\n",
    "        FC2_SCALE = 16\n",
    "        FC3_SCALE = 4\n",
    "\n",
    "        self.fc1 = nn.Linear(EMBEDDINGS_DIM*BEHAVE_CONTEXT_SIZE, EMBEDDINGS_DIM).to(device)\n",
    "        self.fc2 = nn.Linear(EMBEDDINGS_DIM, BEHAVE_WORDS_SIZE * FC2_SCALE).to(device)\n",
    "        self.fc3 = nn.Linear(BEHAVE_WORDS_SIZE * FC2_SCALE, BEHAVE_WORDS_SIZE * FC3_SCALE).to(device)\n",
    "        self.fc4 = nn.Linear(BEHAVE_WORDS_SIZE * FC3_SCALE, BEHAVE_WORDS_SIZE).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = embeddings[x]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = torch.log_softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "\n",
    "def behave_create_dataset(\n",
    "    words: list[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.  \n",
    "    \"\"\"\n",
    "    label_to_vocabulary:dict[int, int] = {}\n",
    "\n",
    "    vocabulary_to_label = { vocabulary[word]: None for word in vocabulary.lookup_tokens(range(VOCABULARY_SIZE)) }\n",
    "\n",
    "    for label, word in enumerate(BEHAVE_WORDS):\n",
    "\n",
    "        # Skip '<unk>' to avoid bias to just guess <unk>\n",
    "        if word == '<unk>': continue\n",
    "\n",
    "        vocabulary_index = vocabulary[word]\n",
    "\n",
    "        vocabulary_to_label[vocabulary_index] = label\n",
    "        label_to_vocabulary[label] = vocabulary_index\n",
    "\n",
    "    return dataset_create(\n",
    "        words, \n",
    "        context_size_before=BEHAVE_CONTEXT_SIZE / 2, \n",
    "        context_size_after=BEHAVE_CONTEXT_SIZE / 2, \n",
    "        vocabulary_index_to_target=vocabulary_to_label, \n",
    "        dataset_name='behave.'+dataset_name\n",
    "    ), label_to_vocabulary\n",
    "\n",
    "def behave_rnn_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: BeHaveRNN,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> list[tuple[int, float]]: \n",
    "\n",
    "    model.name = model_nameof(model, criterion, optimizer)\n",
    "\n",
    "    if model_load(model, 'behave'):\n",
    "        return\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    # Train with different sequence lengths\n",
    "    for context_size in range(0, (BEHAVE_CONTEXT_SIZE // 2) - 1):\n",
    "        contexts = dataset.tensors[0][:,context_size:BEHAVE_CONTEXT_SIZE - context_size]\n",
    "        targets = dataset.tensors[1]\n",
    "\n",
    "        print(\"Context size:\", context_size, contexts.shape)\n",
    "        \n",
    "        losses.append(model_train(\n",
    "            model, TensorDataset(contexts, targets), criterion, optimizer, \n",
    "            model_category='behave',\n",
    "            epochs=BEHAVE_EPOCHS, \n",
    "            batch_size=BEHAVE_BATCH_SIZE,\n",
    "            tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=BEHAVE_WORDS_SIZE).float(),\n",
    "            retrain=True,\n",
    "            figure_tag=f'_context-{context_size}'\n",
    "        ))\n",
    "\n",
    "        model.reset()\n",
    "\n",
    "    plt.clf()\n",
    "    for i, loss in enumerate(losses):\n",
    "        plt.plot(loss, label=f'Context size {i+1}')\n",
    "    plt.title(f'{model.name} | Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(DOCS_DIR + f'loss_{model.name}.png')\n",
    "\n",
    "    model_save(model, 'behave')\n",
    "\n",
    "def behave_mlp_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: BeHaveMLP,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> list[tuple[int, float]]: \n",
    "    \n",
    "    model_train(\n",
    "        model, dataset, \n",
    "        criterion, \n",
    "        optimizer,\n",
    "        model_category='behave',\n",
    "        epochs=BEHAVE_EPOCHS, batch_size=BEHAVE_BATCH_SIZE,\n",
    "        tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=BEHAVE_WORDS_SIZE).float()\n",
    "    )\n",
    "\n",
    "def behave_performance(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation'\n",
    "):\n",
    "    return model_accuracy(\n",
    "        model, dataset, dataset_name,\n",
    "        transform_outputs=lambda x: torch.argmax(x, dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def behave_create_model():\n",
    "\n",
    "    print(\"Training be/have models\")\n",
    "    training_data, label_to_vocabulary = behave_create_dataset(words_train, 'train')\n",
    "\n",
    "    rnn1 = BeHaveRNN()\n",
    "    behave_rnn_train(\n",
    "        training_data, rnn1, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(rnn1.parameters(), lr=0.001)\n",
    "    )\n",
    "    \n",
    "    rnn2 = BeHaveRNN()\n",
    "    behave_rnn_train(\n",
    "        training_data, rnn2, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(rnn2.parameters(), lr=0.002)\n",
    "    )\n",
    "\n",
    "    rnn3 = BeHaveRNN()\n",
    "    behave_rnn_train(\n",
    "        training_data, rnn3, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(rnn3.parameters(), lr=0.003)\n",
    "    )\n",
    "\n",
    "    rnn4 = BeHaveRNN()\n",
    "    behave_rnn_train(\n",
    "        training_data, rnn4, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.SGD(rnn4.parameters(), lr=0.001)\n",
    "    )\n",
    "\n",
    "    rnn5 = BeHaveRNN()\n",
    "    behave_rnn_train(\n",
    "        training_data, rnn5, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.SGD(rnn5.parameters(), lr=0.01)\n",
    "    )\n",
    "\n",
    "    rnn6 = BeHaveRNN()\n",
    "    behave_rnn_train(\n",
    "        training_data, rnn6, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(rnn6.parameters(), lr=0.0001)\n",
    "    )\n",
    "\n",
    "    mlp1 = BeHaveMLP()\n",
    "    behave_mlp_train(\n",
    "        training_data, mlp1,\n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(mlp1.parameters(), lr=0.001)\n",
    "    )\n",
    "\n",
    "    mlp2 = BeHaveMLP()\n",
    "    behave_mlp_train(\n",
    "        training_data, mlp2,\n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(mlp2.parameters(), lr=0.0001)\n",
    "    )\n",
    "\n",
    "    mlp3 = BeHaveMLP()\n",
    "    behave_mlp_train(\n",
    "        training_data, mlp3,\n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.SGD(mlp3.parameters(), lr=0.0001)\n",
    "    )\n",
    "\n",
    "    alwaysUnknown = BeHaveAlways('<unk>')\n",
    "\n",
    "    print(\"Validating be/have models\")\n",
    "\n",
    "    validation_data, _ = behave_create_dataset(words_val, 'val')\n",
    "\n",
    "    best_model, validation_accuracy = model_pick_best(\n",
    "        [rnn1, rnn2, rnn3, rnn4, rnn5, rnn6, mlp1, mlp2, mlp3, alwaysUnknown], \n",
    "        validation_data, \n",
    "        behave_performance,\n",
    "        figure_tag='_behave'\n",
    "    )\n",
    "\n",
    "    test_data, _ = behave_create_dataset(words_test, 'test')\n",
    "    test_accuracy = behave_performance(best_model, test_data, 'Test')\n",
    "    train_accuracy = behave_performance(best_model, training_data, 'Train')\n",
    "\n",
    "    plt.clf()\n",
    "    plt.bar(['Train', 'Validation', 'Test'], [train_accuracy, validation_accuracy, test_accuracy])\n",
    "    plt.title('Be/Have Model Accuracy')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.savefig(DOCS_DIR + 'accuracy_behave_best.png')\n",
    "    plt.show()\n",
    "\n",
    "    return best_model\n",
    "     \n",
    "\n",
    "behave_model = behave_create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BeHaveMLP' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m     word \u001b[38;5;241m=\u001b[39m BEHAVE_WORDS[output]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mafter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mbehave_try\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhen we\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myounger we\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m behave_try(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# From the beginning of Dracula \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# This is a text snippet from the training data, and will have a bias for success\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36mbehave_try\u001b[0;34m(before, after)\u001b[0m\n\u001b[1;32m      2\u001b[0m behave_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m behave_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mbehave_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m()\n\u001b[1;32m      6\u001b[0m words_before \u001b[38;5;241m=\u001b[39m TOKENIZER(before)\n\u001b[1;32m      7\u001b[0m words_after \u001b[38;5;241m=\u001b[39m TOKENIZER(after)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BeHaveMLP' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "def behave_try(before: str, after:str):\n",
    "    behave_model.to(device)\n",
    "    behave_model.eval()\n",
    "    behave_model.reset()\n",
    "    \n",
    "    words_before = TOKENIZER(before)\n",
    "    words_after = TOKENIZER(after)\n",
    "\n",
    "    if len(words_before) != len(words_after):\n",
    "        print(\"Contexts must have the same length\")\n",
    "        return\n",
    "\n",
    "    # Make context\n",
    "    context = torch.zeros(1, len(words_before) + len(words_after)).long().to(device)\n",
    "    for i, word in enumerate(words_before):\n",
    "        context[0, i] = vocabulary[word]\n",
    "    for i, word in enumerate(words_after):\n",
    "        context[0, i + len(words_before)] = vocabulary[word]\n",
    "\n",
    "    output = behave_model(context)\n",
    "    output = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    word = BEHAVE_WORDS[output]\n",
    "    \n",
    "    print(f'{before} ({word}) {after}')\n",
    "\n",
    "\n",
    "behave_try(\"When we\", \"younger we\")\n",
    "\n",
    "behave_try(\"Who\", \"you\")\n",
    "\n",
    "# From the beginning of Dracula \n",
    "# This is a text snippet from the training data, and will have a bias for success\n",
    "behave_try(\"and that as it was a national dish I should\", \"able to get it anywhere along the Carpathians I found\")\n",
    "\n",
    "behave_try(\"This ai\", \"emberrasing me\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010...\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 0 | Loss 0.0006678065030765127\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 1 | Loss 0.0006111510889522946\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 2 | Loss 0.000583511239304\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 3 | Loss 0.0005712848083380872\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 4 | Loss 0.0005625690371729805\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 5 | Loss 0.0005554868663517595\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 6 | Loss 0.000550308329940764\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 7 | Loss 0.0005561175604986687\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 8 | Loss 0.0005522471083708703\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 9 | Loss 0.0005482773526930683\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 10 | Loss 0.0005458095157537954\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 11 | Loss 0.000543523744361962\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 12 | Loss 0.0005412701340870877\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 13 | Loss 0.000539670104959724\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 14 | Loss 0.0005379052026526713\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 15 | Loss 0.0005362228454046603\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 16 | Loss 0.0005350272979135206\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 17 | Loss 0.0005337170984809786\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 18 | Loss 0.000532305154941497\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 19 | Loss 0.0005298282065896624\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 20 | Loss 0.0005283423955269122\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 21 | Loss 0.0005269499764429201\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 22 | Loss 0.0005256104903271439\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 23 | Loss 0.0005243350010376945\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 24 | Loss 0.0005231771465925691\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 25 | Loss 0.0005220653373216399\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 26 | Loss 0.0005210107455522145\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 27 | Loss 0.0005200903844142793\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 28 | Loss 0.0005193265985624502\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 29 | Loss 0.0005183600295465234\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 30 | Loss 0.0005174628808167813\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 31 | Loss 0.0005165017894953112\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 32 | Loss 0.000515941274918967\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 33 | Loss 0.0005151472261611293\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 34 | Loss 0.0005144395731188324\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 35 | Loss 0.0005136476395103393\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 36 | Loss 0.000512973774622959\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 37 | Loss 0.000512319976537054\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 38 | Loss 0.0005116124319639543\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 39 | Loss 0.0005109141615072121\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 40 | Loss 0.0005102356324443538\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 41 | Loss 0.0005096611795761741\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 42 | Loss 0.0005090103101585926\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 43 | Loss 0.0005087664714033683\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 44 | Loss 0.0005079596775148638\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 45 | Loss 0.0005074233515694872\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 46 | Loss 0.0005067799122919113\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 47 | Loss 0.0005063287346663084\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 48 | Loss 0.0005059007152142999\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 49 | Loss 0.0005053110224239187\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 50 | Loss 0.0005048966158561543\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 51 | Loss 0.0005045656221010088\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 52 | Loss 0.0005041633640833268\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 53 | Loss 0.0005036506301883334\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 54 | Loss 0.0005035622820272434\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 55 | Loss 0.0005027687213807928\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 56 | Loss 0.0005027489257523104\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 57 | Loss 0.0005024537810668277\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 58 | Loss 0.0005019521652645435\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 59 | Loss 0.0005016635287308905\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 60 | Loss 0.0005014790226265149\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 61 | Loss 0.0005009799016157655\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 62 | Loss 0.0005007749490677244\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 63 | Loss 0.0005004741639839891\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 64 | Loss 0.000500348610888272\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 65 | Loss 0.0005000682722482021\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 66 | Loss 0.000499623331601436\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 67 | Loss 0.0004994097015176219\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 68 | Loss 0.0004992677153385345\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 69 | Loss 0.0004989327624576925\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 70 | Loss 0.0004987873052642958\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 71 | Loss 0.0004983987143654564\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 72 | Loss 0.0004982908959834755\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 73 | Loss 0.0004979492180124095\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 74 | Loss 0.0004979427640951782\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 75 | Loss 0.0004975480989212976\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 76 | Loss 0.0004976165972193065\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 77 | Loss 0.0004972579438188843\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 78 | Loss 0.000496982323588891\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 79 | Loss 0.0004969623110220143\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 80 | Loss 0.0004965521889875379\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 81 | Loss 0.000496465522099004\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 82 | Loss 0.0004963419756834344\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 83 | Loss 0.0004961707570557113\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 84 | Loss 0.0004958497967013035\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 85 | Loss 0.0004969421899859405\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 86 | Loss 0.0004955527538048703\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 87 | Loss 0.0004953594616955244\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 88 | Loss 0.0004952326069694413\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 89 | Loss 0.000495132110258269\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 90 | Loss 0.0004948312167053366\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 91 | Loss 0.0004949458686467387\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 92 | Loss 0.0004947604405541871\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 93 | Loss 0.0004945712702743334\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 94 | Loss 0.0004942743358470975\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 95 | Loss 0.0004943546030529986\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 96 | Loss 0.0004942059460182857\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 97 | Loss 0.0004939044558847687\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 98 | Loss 0.0004938712643104366\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Epoch 99 | Loss 0.0004936824194381744\n",
      "Saved Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 (./data/gen/Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010.pt)\n",
      "Training Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100...\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 0 | Loss 0.0006432671911945476\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 1 | Loss 0.0006106987724001214\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 2 | Loss 0.0006020727052610691\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 3 | Loss 0.0005962243715575868\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 4 | Loss 0.0005912692274581501\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 5 | Loss 0.0005882081724795882\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 6 | Loss 0.0005862377752785087\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 7 | Loss 0.0005838570933391568\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 8 | Loss 0.0005824513867785122\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 9 | Loss 0.0005821644857520138\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 10 | Loss 0.0005798107041735587\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 11 | Loss 0.0005784508258487158\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 12 | Loss 0.0005779740494925821\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 13 | Loss 0.0005774236225515743\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 14 | Loss 0.000576887784717585\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 15 | Loss 0.0005763264566222621\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 16 | Loss 0.0005762544873099439\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 17 | Loss 0.0005757806938567323\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 18 | Loss 0.0005756935388568111\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 19 | Loss 0.0005758912782032409\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 20 | Loss 0.0005752811932037929\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 21 | Loss 0.0005759426926026965\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 22 | Loss 0.0005753937842304489\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 23 | Loss 0.0005752931790500795\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 24 | Loss 0.0005750292192587813\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 25 | Loss 0.0005751238586333067\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 26 | Loss 0.0005743751499998834\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 27 | Loss 0.0005747212752080332\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 28 | Loss 0.0005746360184190624\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 29 | Loss 0.0005740036429995973\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 30 | Loss 0.0005741020787960235\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 31 | Loss 0.0005752920943581078\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 32 | Loss 0.0005754763835240891\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 33 | Loss 0.0005747621138607654\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 34 | Loss 0.0005747545752515624\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 35 | Loss 0.0005746050504632721\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 36 | Loss 0.0005753687820805026\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 37 | Loss 0.0005743976031236963\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 38 | Loss 0.0005743506359613243\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 39 | Loss 0.0005739333007252365\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 40 | Loss 0.0005733908462702201\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 41 | Loss 0.0005733174668583388\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 42 | Loss 0.0005738761374583311\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 43 | Loss 0.0005734151433703849\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 44 | Loss 0.0005723619074659241\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 45 | Loss 0.0005746704573891619\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 46 | Loss 0.0005730306743010377\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 47 | Loss 0.0005732604120606306\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 48 | Loss 0.0005736647852276574\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 49 | Loss 0.0005728270776179611\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 50 | Loss 0.0005725664803717751\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 51 | Loss 0.0005730382129102405\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 52 | Loss 0.0005720478349055362\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 53 | Loss 0.0005724134845691755\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 54 | Loss 0.000572742905520962\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 55 | Loss 0.0005725979364389525\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 56 | Loss 0.0005719339964831128\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 57 | Loss 0.0005722922160067463\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 58 | Loss 0.0005719657237232831\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 59 | Loss 0.0005719524362466306\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 60 | Loss 0.0005712083375540867\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 61 | Loss 0.0005719932206647643\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 62 | Loss 0.0005724493878734368\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 63 | Loss 0.0005717442296226746\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 64 | Loss 0.0005713082919192732\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 65 | Loss 0.0005720283104500467\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 66 | Loss 0.000571860020490647\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 67 | Loss 0.000572095995229077\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 68 | Loss 0.0005718356149212851\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 69 | Loss 0.0005718490108671349\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 70 | Loss 0.0005726178947712307\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 71 | Loss 0.0005715706789072125\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 72 | Loss 0.0005715557643926024\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 73 | Loss 0.0005718388147626015\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 74 | Loss 0.0005718940255839579\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 75 | Loss 0.0005715103700335894\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 76 | Loss 0.0005717500327247229\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 77 | Loss 0.0005718001454938125\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 78 | Loss 0.0005724842607203248\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 79 | Loss 0.000571758710260496\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 80 | Loss 0.0005722968259476257\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 81 | Loss 0.0005714936657772262\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 82 | Loss 0.0005713657805937701\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 83 | Loss 0.0005715820681729147\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 84 | Loss 0.0005717724858485358\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 85 | Loss 0.0005718512887202753\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 86 | Loss 0.0005713272740287768\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 87 | Loss 0.0005725459796935111\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 88 | Loss 0.0005747336949311084\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 89 | Loss 0.00057265119481476\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 90 | Loss 0.0005716019722705943\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 91 | Loss 0.0005718826905528543\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 92 | Loss 0.0005718848599367976\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 93 | Loss 0.0005721516941618206\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 94 | Loss 0.0005716269744205406\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 95 | Loss 0.0005719130076934616\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 96 | Loss 0.0005718859988633677\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 97 | Loss 0.0005717302913308391\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 98 | Loss 0.0005718958695603097\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Epoch 99 | Loss 0.000572183801044181\n",
      "Saved Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 (./data/gen/Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100.pt)\n",
      "Training Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001...\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 0 | Loss 0.0007504169148094796\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 1 | Loss 0.0006542857634154425\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 2 | Loss 0.0006430187967330425\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 3 | Loss 0.000632943364181487\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 4 | Loss 0.0006244736011555478\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 5 | Loss 0.0006161062872863299\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 6 | Loss 0.0006100170977305454\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 7 | Loss 0.0006044645052931302\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 8 | Loss 0.0006005829893072211\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 9 | Loss 0.0005955768646851173\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 10 | Loss 0.0005912681427661785\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 11 | Loss 0.0005877481003798179\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 12 | Loss 0.0005845339953640577\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 13 | Loss 0.0005816079303013663\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 14 | Loss 0.0005788898549242364\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 15 | Loss 0.0005763857350385122\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 16 | Loss 0.0005740159542534754\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 17 | Loss 0.0005720468044481631\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 18 | Loss 0.0005704228036282263\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 19 | Loss 0.0005686722734897977\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 20 | Loss 0.0005670472964470864\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 21 | Loss 0.0005655534586637461\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 22 | Loss 0.000563980329897279\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 23 | Loss 0.0005623771551631977\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 24 | Loss 0.0005610259001395293\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 25 | Loss 0.0005600693102897414\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 26 | Loss 0.0005593546067496289\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 27 | Loss 0.000560981048126502\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 28 | Loss 0.0005653168873447318\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 29 | Loss 0.0005622116311683257\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 30 | Loss 0.0005612961511442629\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 31 | Loss 0.0005605931080428456\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 32 | Loss 0.0005599419674522711\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 33 | Loss 0.0005587017306518998\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 34 | Loss 0.0005577270264461856\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 35 | Loss 0.0005571061487616198\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 36 | Loss 0.0005559547482337257\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 37 | Loss 0.0005552544710968361\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 38 | Loss 0.0005541622947505935\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 39 | Loss 0.0005533799064314507\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 40 | Loss 0.0005528610440268175\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 41 | Loss 0.0005521854978668813\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 42 | Loss 0.0005514814785426893\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 43 | Loss 0.0005509515522799455\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 44 | Loss 0.0005503960815212695\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 45 | Loss 0.0005499439276728921\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 46 | Loss 0.0005495504014255817\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 47 | Loss 0.0005490644594222877\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 48 | Loss 0.000548600807839011\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 49 | Loss 0.000548150281028591\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 50 | Loss 0.0005476831041964064\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 51 | Loss 0.0005471818138017136\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 52 | Loss 0.0005468022258462388\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 53 | Loss 0.0005463567970880855\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 54 | Loss 0.0005459893576826931\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 55 | Loss 0.0005455799406979983\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 56 | Loss 0.0005451377117811613\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 57 | Loss 0.0005447529715388212\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 58 | Loss 0.0005444585861377185\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 59 | Loss 0.0005439840876347254\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 60 | Loss 0.0005435691929555737\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 61 | Loss 0.0005432338604325417\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 62 | Loss 0.0005427636464628365\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 63 | Loss 0.0005424058608159915\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 64 | Loss 0.0005419918338904172\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 65 | Loss 0.0005416369769118957\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 66 | Loss 0.0005412886823198027\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 67 | Loss 0.0005409223818409805\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 68 | Loss 0.0005406091227995713\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 69 | Loss 0.0005402584961197392\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 70 | Loss 0.0005398573227940288\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 71 | Loss 0.0005396077894059534\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 72 | Loss 0.0005393940508529421\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 73 | Loss 0.0005390658230623243\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 74 | Loss 0.0005388572910307769\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 75 | Loss 0.0005384253666876705\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 76 | Loss 0.0005381206767128372\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 77 | Loss 0.0005379368214236445\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 78 | Loss 0.0005375579385179513\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 79 | Loss 0.0005372784676314587\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 80 | Loss 0.0005371077913497214\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 81 | Loss 0.0005365433718822794\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 82 | Loss 0.0005361966500935453\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 83 | Loss 0.0005358885975736\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 84 | Loss 0.0005356939495992895\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 85 | Loss 0.0005355043996772456\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 86 | Loss 0.0005352946744845294\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 87 | Loss 0.0005349721955613614\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 88 | Loss 0.0005348020073910112\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 89 | Loss 0.0005345030120490292\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 90 | Loss 0.0005343478468624863\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 91 | Loss 0.0005339965151328727\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 92 | Loss 0.0005336859678213926\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 93 | Loss 0.0005334595383723131\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 94 | Loss 0.0005331097794460583\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 95 | Loss 0.0005328689778283547\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 96 | Loss 0.0005326751433730229\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 97 | Loss 0.0005324251218735602\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 98 | Loss 0.0005321874658625743\n",
      "Training | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Epoch 99 | Loss 0.0005319076153338916\n",
      "Saved Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 (./data/gen/Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001.pt)\n",
      "Validation | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.2010\n",
      "Validation | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0100 | Accuracy 0.1427\n",
      "Validation | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0001 | Accuracy 0.1786\n",
      "Best model: Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.2010\n",
      "Test | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.2228\n",
      "Train | Gen1Rnn_CrossEntropyLoss_Adam-lr0.0010 | Accuracy 0.2138\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GEN_CONTEXT_SIZE = 20\n",
    "GEN_BATCH_SIZE = 8192\n",
    "GEN_EPOCHS = 100\n",
    "\n",
    "class Gen1Rnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gen1Rnn, self).__init__()\n",
    "\n",
    "        self.hidden = None\n",
    "        self.rnn = nn.RNN(EMBEDDINGS_DIM, EMBEDDINGS_DIM * 8, batch_first=True)\n",
    "        self.fc1 = nn.Linear(EMBEDDINGS_DIM * 8, EMBEDDINGS_DIM * 4)\n",
    "        self.fc2 = nn.Linear(EMBEDDINGS_DIM * 4, EMBEDDINGS_DIM * 2)\n",
    "        self.fc3 = nn.Linear(EMBEDDINGS_DIM * 2, VOCABULARY_SIZE)\n",
    "\n",
    "    def reset(self):\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = embeddings[x]\n",
    "\n",
    "        # Reset the state if its incompatible with current input\n",
    "        if self.hidden is not None and self.hidden.size(1) != x.size(0):\n",
    "            self.reset()\n",
    "\n",
    "        x, hidden = self.rnn(x, self.hidden)\n",
    "        x = nn.functional.relu(x[:, -1, :]) # Keep only the last output\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        self.hidden = hidden.data\n",
    "\n",
    "        return x\n",
    "\n",
    "def gen_create_dataset(\n",
    "    words: list[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given words.  \n",
    "    \"\"\"\n",
    "    vocabulary_idx_to_label = { vocabulary[word]: vocabulary[word] for word in vocabulary.lookup_tokens(range(VOCABULARY_SIZE)) }\n",
    "    vocabulary_idx_to_label[vocabulary['<unk>']] = None\n",
    "\n",
    "    return dataset_create(words, context_size_before=GEN_CONTEXT_SIZE, vocabulary_index_to_target=vocabulary_idx_to_label, dataset_name='gen.'+dataset_name)\n",
    "\n",
    "def gen_train(\n",
    "    dataset: TensorDataset,\n",
    "    model: nn.Module,\n",
    "    criterion: object,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> list[tuple[int, float]]: \n",
    "    \n",
    "    model_train(\n",
    "        model, dataset, criterion, optimizer, \n",
    "        model_category='gen',\n",
    "        epochs=GEN_EPOCHS, \n",
    "        batch_size=GEN_BATCH_SIZE,\n",
    "        tranform_targets=lambda x: torch.nn.functional.one_hot(x, num_classes=VOCABULARY_SIZE).float(),\n",
    "        retrain=True\n",
    "    )\n",
    "\n",
    "def gen_performance(\n",
    "    model: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    dataset_name: str = 'Validation'\n",
    "):\n",
    "    return model_accuracy(\n",
    "        model, dataset, dataset_name,\n",
    "        transform_outputs=lambda x: torch.argmax(x, dim=1)\n",
    "    )\n",
    "\n",
    "def gen_create_model():\n",
    "\n",
    "    training_data = gen_create_dataset(words_train, 'train')\n",
    "\n",
    "    m1 = Gen1Rnn()\n",
    "    gen_train(\n",
    "        training_data, m1, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(m1.parameters(), lr=0.001)\n",
    "    )\n",
    "\n",
    "    m2 = Gen1Rnn()\n",
    "    gen_train(\n",
    "        training_data, m2, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(m2.parameters(), lr=0.01)\n",
    "    )\n",
    "\n",
    "    m3 = Gen1Rnn()\n",
    "    gen_train(\n",
    "        training_data, m3, \n",
    "        nn.CrossEntropyLoss(), \n",
    "        torch.optim.Adam(m3.parameters(), lr=0.0001)\n",
    "    )\n",
    "\n",
    "    validation_data = gen_create_dataset(words_val, 'val')\n",
    "\n",
    "    best_model, validation_accuracy = model_pick_best(\n",
    "        [m1, m2, m3], \n",
    "        validation_data, \n",
    "        gen_performance,\n",
    "        figure_tag='_gen'\n",
    "    )\n",
    "\n",
    "    test_data = gen_create_dataset(words_test, 'test')\n",
    "    test_accuracy = gen_performance(best_model, test_data, 'Test')\n",
    "    train_accuracy = gen_performance(best_model, training_data, 'Train')\n",
    "\n",
    "    plt.clf()\n",
    "    plt.bar(['Train', 'Validation', 'Test'], [train_accuracy, validation_accuracy, test_accuracy])\n",
    "    plt.title('GEN Model Accuracy')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.savefig(DOCS_DIR + 'accuracy_gen_best.png')\n",
    "    plt.show()\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "gen_model = gen_create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> once upon a time , there was (no|a|not|the|sure)\n",
      "> hello , my name is one and (the|a|have|you|see)\n",
      "> hello , my name is one and the (other|door|two|man|same)\n",
      "> hello , my name is one and the other (of|,|was|and|which)\n",
      "> hello , my name is one and the other of (the|his|a|it|my)\n",
      "> hello , my name is one and the other of the (room|floor|man|wall|table)\n",
      "> hello , my name is one and the other of his (eyes|own|hands|hand|face)\n",
      "> hello , my name is one and the other of a (man|few|little|great|small)\n",
      "> hello , my name is one and the other of it (,|.|to|in|of)\n",
      "> hello , my name is one and the other of my (companion|eyes|own|face|mind)\n",
      "> hello , my name is one and the other , (and|but|he|i|for)\n",
      "> hello , my name is one and the other , and (the|i|he|we|then)\n",
      "> hello , my name is one and the other , but (he|the|i|a|we)\n",
      "> hello , my name is one and the other , he (said|answered|had|cried|continued)\n",
      "> hello , my name is one and the other , i (said|answered|cried|asked|had)\n",
      "> hello , my name is one and the other , for (the|i|he|a|it)\n",
      "> hello , my name is one and the other was (a|to|not|no|the)\n",
      "> hello , my name is one and the other was a (little|man|few|small|great)\n",
      "> hello , my name is one and the other was to (be|find|have|tell|keep)\n",
      "> hello , my name is one and the other was not (a|to|,|in|the)\n",
      "> hello , my name is one and the other was no (,|doubt|one|of|time)\n",
      "> hello , my name is one and the other was the (other|door|two|same|room)\n",
      "> hello , my name is one and the other and (have|had|then|was|i)\n",
      "> hello , my name is one and the other and have (have|had|were|be|are)\n",
      "> hello , my name is one and the other and had (been|not|have|come|be)\n",
      "> hello , my name is one and the other and then (,|i|was|is|were)\n",
      "> hello , my name is one and the other and was (a|to|in|so|not)\n",
      "> hello , my name is one and the other and i (have|had|am|was|could)\n",
      "> hello , my name is one and the other which (was|had|is|said|i)\n",
      "> hello , my name is one and the other which was (a|to|in|and|so)\n",
      "> hello , my name is one and the other which had (been|have|not|be|come)\n",
      "> hello , my name is one and the other which is (a|the|to|in|of)\n",
      "> hello , my name is one and the other which said (,|.|to|in|that)\n",
      "> hello , my name is one and the other which i (had|have|am|was|could)\n",
      "> hello , my name is one and the door (,|of|.|which|and)\n",
      "> hello , my name is one and the door , (and|but|he|i|with)\n",
      "> hello , my name is one and the door , and (the|i|he|then|we)\n",
      "> hello , my name is one and the door , but (the|he|i|a|we)\n",
      "> hello , my name is one and the door , he (said|answered|cried|had|was)\n",
      "> hello , my name is one and the door , i (said|have|am|cried|answered)\n",
      "> hello , my name is one and the door , with (the|a|his|my|it)\n",
      "> hello , my name is one and the door of (the|his|a|it|them)\n",
      "> hello , my name is one and the door of the (room|man|door|case|house)\n",
      "> hello , my name is one and the door of his (own|hands|eyes|hand|face)\n",
      "> hello , my name is one and the door of a (few|man|little|great|moment)\n",
      "> hello , my name is one and the door of it (,|.|was|to|s)\n",
      "> hello , my name is one and the door of them (,|.|in|of|to)\n",
      "> hello , my name is one and the door . (i|he|there|the|it)\n",
      "> hello , my name is one and the door . i (have|had|am|ll|was)\n",
      "> hello , my name is one and the door . he (was|had|is|would|has)\n",
      "> hello , my name is one and the door . there (was|is|were|s|had)\n",
      "> hello , my name is one and the door . the (man|other|young|latter|fellow)\n",
      "> hello , my name is one and the door . it (was|is|had|s|were)\n",
      "> hello , my name is one and the door which (was|i|he|had|is)\n",
      "> hello , my name is one and the door which was (a|to|in|so|the)\n",
      "> hello , my name is one and the door which i (had|have|am|could|was)\n",
      "> hello , my name is one and the door which he (had|was|could|said|would)\n",
      "> hello , my name is one and the door which had (been|not|have|be|already)\n",
      "> hello , my name is one and the door which is (a|the|to|that|in)\n",
      "> hello , my name is one and the door and (the|,|in|of|to)\n",
      "> hello , my name is one and the door and the (man|door|whole|other|case)\n",
      "> hello , my name is one and the door and , (and|he|said|i|but)\n",
      "> hello , my name is one and the door and in (the|his|a|my|him)\n",
      "> hello , my name is one and the door and of (the|a|his|it|my)\n",
      "> hello , my name is one and the door and to (be|the|have|see|a)\n",
      "> hello , my name is one and the two (of|,|in|to|and)\n",
      "> hello , my name is one and the two of (the|his|a|my|this)\n",
      "> hello , my name is one and the two of the (room|man|door|case|wall)\n",
      "> hello , my name is one and the two of his (own|hand|hands|eyes|face)\n",
      "> hello , my name is one and the two of a (few|little|man|great|small)\n",
      "> hello , my name is one and the two of my (companion|own|mind|eyes|friend)\n",
      "> hello , my name is one and the two of this (of|man|girl|is|thing)\n",
      "> hello , my name is one and the two , (and|he|i|but|the)\n",
      "> hello , my name is one and the two , and (the|i|he|then|we)\n",
      "> hello , my name is one and the two , he (said|answered|had|cried|was)\n",
      "> hello , my name is one and the two , i (have|said|asked|am|answered)\n",
      "> hello , my name is one and the two , but (the|he|i|a|it)\n",
      "> hello , my name is one and the two , the (man|door|other|young|most)\n",
      "> hello , my name is one and the two in (the|a|his|him|an)\n",
      "> hello , my name is one and the two in the (table|wall|room|floor|door)\n",
      "> hello , my name is one and the two in a (little|few|small|great|way)\n",
      "> hello , my name is one and the two in his (own|hand|eyes|hands|face)\n",
      "> hello , my name is one and the two in him (.|,|to|and|in)\n",
      "> hello , my name is one and the two in an (of|,|.|way|time)\n",
      "> hello , my name is one and the two to (the|be|his|me|him)\n",
      "> hello , my name is one and the two to the (two|other|same|young|very)\n",
      "> hello , my name is one and the two to be (a|not|have|no|to)\n",
      "> hello , my name is one and the two to his (hand|own|daughter|seat|eyes)\n",
      "> hello , my name is one and the two to me (.|,|to|in|that)\n",
      "> hello , my name is one and the two to him (.|,|to|that|in)\n",
      "> hello , my name is one and the two and (the|have|a|had|in)\n",
      "> hello , my name is one and the two and the (man|other|door|young|two)\n",
      "> hello , my name is one and the two and have (have|not|been|see|had)\n",
      "> hello , my name is one and the two and a (man|little|great|few|very)\n",
      "> hello , my name is one and the two and had (been|not|to|a|have)\n",
      "> hello , my name is one and the two and in (the|his|a|my|him)\n",
      "> hello , my name is one and the man (of|was|had|s|,)\n",
      "> hello , my name is one and the man of (the|a|his|an|one)\n",
      "> hello , my name is one and the man of the (man|room|case|world|girl)\n",
      "> hello , my name is one and the man of a (few|man|little|great|dull)\n",
      "> hello , my name is one and the man of his (own|companion|eyes|hands|face)\n",
      "> hello , my name is one and the man of an (of|,|.|eyes|way)\n",
      "> hello , my name is one and the man of one (of|,|and|.|in)\n",
      "> hello , my name is one and the man was (a|no|the|not|some)\n",
      "> hello , my name is one and the man was a (little|few|small|very|man)\n",
      "> hello , my name is one and the man was no (idea|doubt|time|eyes|other)\n",
      "> hello , my name is one and the man was the (other|door|time|room|house)\n",
      "> hello , my name is one and the man was not (to|a|,|.|in)\n",
      "> hello , my name is one and the man was some (way|time|of|,|chance)\n",
      "> hello , my name is one and the man had (been|not|have|be|a)\n",
      "> hello , my name is one and the man had been (to|a|,|been|in)\n",
      "> hello , my name is one and the man had not (be|been|have|not|come)\n",
      "> hello , my name is one and the man had have (been|have|not|a|be)\n",
      "> hello , my name is one and the man had be (to|a|be|in|not)\n",
      "> hello , my name is one and the man had a (little|few|very|small|way)\n",
      "> hello , my name is one and the man s (private|own|hand|daughter|way)\n",
      "> hello , my name is one and the man s private (,|.|of|to|in)\n",
      "> hello , my name is one and the man s own (own|companion|face|eyes|hand)\n",
      "> hello , my name is one and the man s hand (,|.|of|to|?)\n",
      "> hello , my name is one and the man s daughter (,|.|was|of|is)\n",
      "> hello , my name is one and the man s way (,|.|to|?|in)\n",
      "> hello , my name is one and the man , (and|he|i|but|the)\n",
      "> hello , my name is one and the man , and (the|i|he|we|then)\n",
      "> hello , my name is one and the man , he (said|answered|had|was|cried)\n",
      "> hello , my name is one and the man , i (said|have|am|answered|had)\n",
      "> hello , my name is one and the man , but (the|he|i|a|that)\n",
      "> hello , my name is one and the man , the (man|door|other|little|whole)\n",
      "> hello , my name is one and the same (of|man|way|little|thing)\n",
      "> hello , my name is one and the same of (the|his|a|this|it)\n",
      "> hello , my name is one and the same of the (man|room|floor|wall|door)\n",
      "> hello , my name is one and the same of his (companion|eyes|own|face|hands)\n",
      "> hello , my name is one and the same of a (man|little|few|great|gentleman)\n",
      "> hello , my name is one and the same of this (of|girl|man|thing|case)\n",
      "> hello , my name is one and the same of it (,|.|of|s|was)\n",
      "> hello , my name is one and the same man (.|,|of|to|in)\n",
      "> hello , my name is one and the same man . (i|the|it|he|you)\n",
      "> hello , my name is one and the same man , (and|he|but|i|said)\n",
      "> hello , my name is one and the same man of (the|a|his|an|them)\n",
      "> hello , my name is one and the same man to (the|be|his|a|him)\n",
      "> hello , my name is one and the same man in (the|a|his|my|an)\n",
      "> hello , my name is one and the same way (the|he|i|that|to)\n",
      "> hello , my name is one and the same way the (man|door|other|case|little)\n",
      "> hello , my name is one and the same way he (was|had|would|has|is)\n",
      "> hello , my name is one and the same way i (was|had|could|said|might)\n",
      "> hello , my name is one and the same way that (he|is|was|i|s)\n",
      "> hello , my name is one and the same way to (the|his|you|do|a)\n",
      "> hello , my name is one and the same little (of|he|which|that|,)\n",
      "> hello , my name is one and the same little of (the|a|which|one|his)\n",
      "> hello , my name is one and the same little he (had|was|spoke|said|answered)\n",
      "> hello , my name is one and the same little which (was|had|is|were|would)\n",
      "> hello , my name is one and the same little that (of|was|,|he|had)\n",
      "> hello , my name is one and the same little , (and|but|said|which|or)\n",
      "> hello , my name is one and the same thing (of|the|in|which|that)\n",
      "> hello , my name is one and the same thing of (the|his|a|it|this)\n",
      "> hello , my name is one and the same thing the (other|door|whole|room|house)\n",
      "> hello , my name is one and the same thing in (the|a|his|my|this)\n",
      "> hello , my name is one and the same thing which (he|i|,|was|is)\n",
      "> hello , my name is one and the same thing that (he|is|,|i|was)\n",
      "> hello , my name is one and a (little|few|man|very|great)\n",
      "> hello , my name is one and a little (of|man|thing|girl|case)\n",
      "> hello , my name is one and a little of (the|a|which|life|one)\n",
      "> hello , my name is one and a little of the (room|man|street|road|case)\n",
      "> hello , my name is one and a little of a (man|little|few|great|small)\n",
      "> hello , my name is one and a little of which (was|is|had|would|has)\n",
      "> hello , my name is one and a little of life (the|,|that|.|it)\n",
      "> hello , my name is one and a little of one (of|,|.|and|in)\n",
      "> hello , my name is one and a little man (of|,|was|s|had)\n",
      "> hello , my name is one and a little man of (the|a|his|them|my)\n",
      "> hello , my name is one and a little man , (and|said|he|but|i)\n",
      "> hello , my name is one and a little man was (a|to|the|no|not)\n",
      "> hello , my name is one and a little man s (hand|own|private|eyes|way)\n",
      "> hello , my name is one and a little man had (been|not|have|to|come)\n",
      "> hello , my name is one and a little thing (,|the|in|of|to)\n",
      "> hello , my name is one and a little thing , (and|he|i|the|but)\n",
      "> hello , my name is one and a little thing the (other|whole|door|two|house)\n",
      "> hello , my name is one and a little thing in (the|a|his|my|this)\n",
      "> hello , my name is one and a little thing of (the|his|a|it|my)\n",
      "> hello , my name is one and a little thing to (the|his|be|me|him)\n",
      "> hello , my name is one and a little girl (of|,|and|.|in)\n",
      "> hello , my name is one and a little girl of (the|his|a|it|her)\n",
      "> hello , my name is one and a little girl , (and|said|but|he|in)\n",
      "> hello , my name is one and a little girl and (,|the|i|of|in)\n",
      "> hello , my name is one and a little girl . (i|he|there|in|and)\n",
      "> hello , my name is one and a little girl in (the|a|his|him|my)\n",
      "> hello , my name is one and a little case (of|,|and|in|.)\n",
      "> hello , my name is one and a little case of (the|his|a|my|it)\n",
      "> hello , my name is one and a little case , (and|said|he|i|but)\n",
      "> hello , my name is one and a little case and (have|a|had|the|then)\n",
      "> hello , my name is one and a little case in (the|a|his|my|him)\n",
      "> hello , my name is one and a little case . (i|he|the|it|there)\n",
      "> hello , my name is one and a few (of|,|.|which|in)\n",
      "> hello , my name is one and a few of (the|his|a|them|it)\n",
      "> hello , my name is one and a few of the (room|man|door|house|case)\n",
      "> hello , my name is one and a few of his (eyes|heart|hand|own|hands)\n",
      "> hello , my name is one and a few of a (few|little|man|great|dull)\n",
      "> hello , my name is one and a few of them (,|.|of|in|and)\n",
      "> hello , my name is one and a few of it (,|.|to|in|was)\n",
      "> hello , my name is one and a few , (and|which|but|he|of)\n",
      "> hello , my name is one and a few , and (the|i|then|he|we)\n",
      "> hello , my name is one and a few , which (was|is|had|i|he)\n",
      "> hello , my name is one and a few , but (he|the|i|a|that)\n",
      "> hello , my name is one and a few , he (said|answered|had|was|cried)\n",
      "> hello , my name is one and a few , of (the|a|two|one|his)\n",
      "> hello , my name is one and a few . (i|the|he|it|there)\n",
      "> hello , my name is one and a few . i (have|had|was|am|don)\n",
      "> hello , my name is one and a few . the (man|whole|other|door|latter)\n",
      "> hello , my name is one and a few . he (was|had|is|would|has)\n",
      "> hello , my name is one and a few . it (was|is|will|s|had)\n",
      "> hello , my name is one and a few . there (was|is|s|were|had)\n",
      "> hello , my name is one and a few which (was|had|is|would|could)\n",
      "> hello , my name is one and a few which was (a|to|so|not|in)\n",
      "> hello , my name is one and a few which had (been|not|a|be|have)\n",
      "> hello , my name is one and a few which is (a|the|no|that|very)\n",
      "> hello , my name is one and a few which would (have|be|not|come|had)\n",
      "> hello , my name is one and a few which could (have|be|not|been|see)\n",
      "> hello , my name is one and a few in (the|a|his|him|my)\n",
      "> hello , my name is one and a few in the (room|house|door|man|case)\n",
      "> hello , my name is one and a few in a (few|little|way|great|man)\n",
      "> hello , my name is one and a few in his (eyes|heart|own|hand|life)\n",
      "> hello , my name is one and a few in him (.|,|to|and|in)\n",
      "> hello , my name is one and a few in my (companion|own|eyes|way|hand)\n",
      "> hello , my name is one and a man (of|was|,|had|and)\n",
      "> hello , my name is one and a man of (the|a|his|an|them)\n",
      "> hello , my name is one and a man of the (man|room|case|road|floor)\n",
      "> hello , my name is one and a man of a (few|man|little|great|dull)\n",
      "> hello , my name is one and a man of his (own|companion|eyes|hands|face)\n",
      "> hello , my name is one and a man of an (,|of|.|eyes|way)\n",
      "> hello , my name is one and a man of them (,|.|of|in|to)\n",
      "> hello , my name is one and a man was (a|no|to|the|not)\n",
      "> hello , my name is one and a man was a (little|few|very|small|man)\n",
      "> hello , my name is one and a man was no (doubt|time|eyes|life|idea)\n",
      "> hello , my name is one and a man was to (be|find|tell|have|do)\n",
      "> hello , my name is one and a man was the (other|two|door|time|house)\n",
      "> hello , my name is one and a man was not (a|to|,|.|in)\n",
      "> hello , my name is one and a man , (and|he|i|said|but)\n",
      "> hello , my name is one and a man , and (the|i|he|then|a)\n",
      "> hello , my name is one and a man , he (said|answered|cried|had|continued)\n",
      "> hello , my name is one and a man , i (said|have|am|asked|answered)\n",
      "> hello , my name is one and a man , said (the|i|a|he|it)\n",
      "> hello , my name is one and a man , but (the|he|i|a|that)\n",
      "> hello , my name is one and a man had (been|to|not|a|have)\n",
      "> hello , my name is one and a man had been (to|a|,|been|in)\n",
      "> hello , my name is one and a man had to (be|do|tell|have|find)\n",
      "> hello , my name is one and a man had not (be|been|have|find|come)\n",
      "> hello , my name is one and a man had a (little|few|small|very|way)\n",
      "> hello , my name is one and a man had have (been|have|be|not|come)\n",
      "> hello , my name is one and a man and (the|a|have|in|which)\n",
      "> hello , my name is one and a man and the (man|door|other|same|case)\n",
      "> hello , my name is one and a man and a (little|man|few|great|moment)\n",
      "> hello , my name is one and a man and have (have|be|had|do|see)\n",
      "> hello , my name is one and a man and in (the|a|his|him|my)\n",
      "> hello , my name is one and a man and which (was|is|had|would|has)\n",
      "> hello , my name is one and a very (man|of|thing|case|room)\n",
      "> hello , my name is one and a very man (,|.|of|and|in)\n",
      "> hello , my name is one and a very man , (and|he|but|i|said)\n",
      "> hello , my name is one and a very man . (i|he|it|the|there)\n",
      "> hello , my name is one and a very man of (the|a|his|them|an)\n",
      "> hello , my name is one and a very man and (the|a|in|of|which)\n",
      "> hello , my name is one and a very man in (the|a|his|my|him)\n",
      "> hello , my name is one and a very of (the|a|his|an|one)\n",
      "> hello , my name is one and a very of the (room|man|street|case|other)\n",
      "> hello , my name is one and a very of a (man|few|little|great|small)\n",
      "> hello , my name is one and a very of his (own|eyes|hands|face|hand)\n",
      "> hello , my name is one and a very of an (way|eyes|hand|time|face)\n",
      "> hello , my name is one and a very of one (of|,|.|and|which)\n",
      "> hello , my name is one and a very thing (in|,|to|that|the)\n",
      "> hello , my name is one and a very thing in (the|a|his|my|an)\n",
      "> hello , my name is one and a very thing , (and|he|i|but|the)\n",
      "> hello , my name is one and a very thing to (the|me|his|him|be)\n",
      "> hello , my name is one and a very thing that (he|i|is|was|,)\n",
      "> hello , my name is one and a very thing the (other|two|last|whole|time)\n",
      "> hello , my name is one and a very case (,|of|and|in|to)\n",
      "> hello , my name is one and a very case , (he|and|i|said|but)\n",
      "> hello , my name is one and a very case of (the|a|his|my|it)\n",
      "> hello , my name is one and a very case and (have|a|the|had|then)\n",
      "> hello , my name is one and a very case in (the|a|his|my|him)\n",
      "> hello , my name is one and a very case to (be|the|me|do|him)\n",
      "> hello , my name is one and a very room (,|.|in|and|of)\n",
      "> hello , my name is one and a very room , (and|he|said|i|but)\n",
      "> hello , my name is one and a very room . (i|he|the|it|there)\n",
      "> hello , my name is one and a very room in (the|his|a|my|him)\n",
      "> hello , my name is one and a very room and (the|a|have|had|i)\n",
      "> hello , my name is one and a very room of (the|a|his|my|it)\n",
      "> hello , my name is one and a great (of|way|,|idea|room)\n",
      "> hello , my name is one and a great of (the|a|one|which|two)\n",
      "> hello , my name is one and a great of the (room|man|door|wall|case)\n",
      "> hello , my name is one and a great of a (few|little|man|great|dull)\n",
      "> hello , my name is one and a great of one (of|,|which|and|in)\n",
      "> hello , my name is one and a great of which (was|had|is|would|s)\n",
      "> hello , my name is one and a great of two (,|a|to|be|of)\n",
      "> hello , my name is one and a great way (,|.|of|to|in)\n",
      "> hello , my name is one and a great way , (and|said|he|but|i)\n",
      "> hello , my name is one and a great way . (i|he|there|you|it)\n",
      "> hello , my name is one and a great way of (the|a|his|two|one)\n",
      "> hello , my name is one and a great way to (be|have|the|do|see)\n",
      "> hello , my name is one and a great way in (a|the|an|all|his)\n",
      "> hello , my name is one and a great , (and|but|which|said|for)\n",
      "> hello , my name is one and a great , and (the|i|he|then|we)\n",
      "> hello , my name is one and a great , but (he|i|the|a|we)\n",
      "> hello , my name is one and a great , which (was|had|is|would|were)\n",
      "> hello , my name is one and a great , said (the|i|he|a|that)\n",
      "> hello , my name is one and a great , for (the|i|he|a|you)\n",
      "> hello , my name is one and a great idea (of|,|and|in|which)\n",
      "> hello , my name is one and a great idea of (the|his|a|my|it)\n",
      "> hello , my name is one and a great idea , (and|he|i|said|but)\n",
      "> hello , my name is one and a great idea and (the|a|have|it|his)\n",
      "> hello , my name is one and a great idea in (the|a|his|my|him)\n",
      "> hello , my name is one and a great idea which (was|had|is|said|has)\n",
      "> hello , my name is one and a great room (.|,|of|in|and)\n",
      "> hello , my name is one and a great room . (i|he|there|you|it)\n",
      "> hello , my name is one and a great room , (and|said|he|i|but)\n",
      "> hello , my name is one and a great room of (the|a|his|my|them)\n",
      "> hello , my name is one and a great room in (the|his|a|my|him)\n",
      "> hello , my name is one and a great room and (the|a|,|have|in)\n",
      "> hello , my name is one and have (have|see|do|not|take)\n",
      "> hello , my name is one and have have (a|the|you|no|not)\n",
      "> hello , my name is one and have have a (little|few|very|small|long)\n",
      "> hello , my name is one and have have a little (of|man|thing|girl|road)\n",
      "> hello , my name is one and have have a few (of|.|side|time|,)\n",
      "> hello , my name is one and have have a very (of|man|,|which|time)\n",
      "> hello , my name is one and have have a small (of|.|,|and|in)\n",
      "> hello , my name is one and have have a long (of|,|.|way|to)\n",
      "> hello , my name is one and have have the (man|door|other|little|whole)\n",
      "> hello , my name is one and have have the man (of|,|and|.|s)\n",
      "> hello , my name is one and have have the door (,|.|of|and|in)\n",
      "> hello , my name is one and have have the other (of|,|.|and|in)\n",
      "> hello , my name is one and have have the little (girl|man|of|thing|world)\n",
      "> hello , my name is one and have have the whole (thing|of|way|man|girl)\n",
      "> hello , my name is one and have have you (to|been|a|,|not)\n",
      "> hello , my name is one and have have you to (be|do|have|tell|find)\n",
      "> hello , my name is one and have have you been (to|a|.|,|the)\n",
      "> hello , my name is one and have have you a (little|few|way|small|very)\n",
      "> hello , my name is one and have have you , (and|he|i|said|but)\n",
      "> hello , my name is one and have have you not (been|to|be|have|a)\n",
      "> hello , my name is one and have have no (hand|eyes|doubt|way|face)\n",
      "> hello , my name is one and have have no hand (to|,|.|of|in)\n",
      "> hello , my name is one and have have no eyes (.|,|to|and|in)\n",
      "> hello , my name is one and have have no doubt (.|,|to|that|in)\n",
      "> hello , my name is one and have have no way (to|,|.|in|that)\n",
      "> hello , my name is one and have have no face (to|,|in|and|of)\n",
      "> hello , my name is one and have have not (been|not|to|be|have)\n",
      "> hello , my name is one and have have not been (to|a|,|in|.)\n",
      "> hello , my name is one and have have not not (a|to|the|not|no)\n",
      "> hello , my name is one and have have not to (be|have|do|get|tell)\n",
      "> hello , my name is one and have have not be (a|the|no|to|not)\n",
      "> hello , my name is one and have have not have (have|not|been|a|no)\n",
      "> hello , my name is one and have see (to|that|a|the|in)\n",
      "> hello , my name is one and have see to (be|the|me|do|him)\n",
      "> hello , my name is one and have see to be (to|a|,|have|of)\n",
      "> hello , my name is one and have see to the (two|other|door|time|house)\n",
      "> hello , my name is one and have see to me (.|,|to|in|that)\n",
      "> hello , my name is one and have see to do (,|to|.|in|?)\n",
      "> hello , my name is one and have see to him (.|,|to|in|and)\n",
      "> hello , my name is one and have see that (the|he|i|you|it)\n",
      "> hello , my name is one and have see that the (man|other|whole|little|case)\n",
      "> hello , my name is one and have see that he (had|was|would|could|were)\n",
      "> hello , my name is one and have see that i (have|had|am|was|could)\n",
      "> hello , my name is one and have see that you (have|had|are|do|were)\n",
      "> hello , my name is one and have see that it (was|is|had|would|were)\n",
      "> hello , my name is one and have see a (few|little|man|small|great)\n",
      "> hello , my name is one and have see a few (.|of|,|hours|side)\n",
      "> hello , my name is one and have see a little (of|man|thing|girl|road)\n",
      "> hello , my name is one and have see a man (of|.|,|and|in)\n",
      "> hello , my name is one and have see a small (of|,|.|or|which)\n",
      "> hello , my name is one and have see a great (of|way|idea|room|time)\n",
      "> hello , my name is one and have see the (man|door|little|great|case)\n",
      "> hello , my name is one and have see the man (of|,|.|and|s)\n",
      "> hello , my name is one and have see the door (,|.|of|and|which)\n",
      "> hello , my name is one and have see the little (of|girl|man|thing|case)\n",
      "> hello , my name is one and have see the great (of|,|.|which|in)\n",
      "> hello , my name is one and have see the case (,|of|.|and|in)\n",
      "> hello , my name is one and have see in (the|a|his|my|this)\n",
      "> hello , my name is one and have see in the (room|house|time|door|other)\n",
      "> hello , my name is one and have see in a (few|way|little|great|time)\n",
      "> hello , my name is one and have see in his (own|hand|eyes|face|hands)\n",
      "> hello , my name is one and have see in my (own|companion|eyes|hand|mind)\n",
      "> hello , my name is one and have see in this (man|way|case|world|room)\n",
      "> hello , my name is one and have do (to|a|not|you|that)\n",
      "> hello , my name is one and have do to (be|have|do|tell|find)\n",
      "> hello , my name is one and have do to be (a|to|not|the|you)\n",
      "> hello , my name is one and have do to have (the|a|you|no|my)\n",
      "> hello , my name is one and have do to do (to|,|.|that|in)\n",
      "> hello , my name is one and have do to tell (the|him|a|my|his)\n",
      "> hello , my name is one and have do to find (the|a|his|him|my)\n",
      "> hello , my name is one and have do a (little|man|very|small|few)\n",
      "> hello , my name is one and have do a little (of|man|thing|girl|,)\n",
      "> hello , my name is one and have do a man (of|was|,|and|s)\n",
      "> hello , my name is one and have do a very (of|man|little|time|case)\n",
      "> hello , my name is one and have do a small (of|,|.|in|to)\n",
      "> hello , my name is one and have do a few (of|,|.|time|which)\n",
      "> hello , my name is one and have do not (be|have|a|been|find)\n",
      "> hello , my name is one and have do not be (a|to|the|that|not)\n",
      "> hello , my name is one and have do not have (a|you|not|have|the)\n",
      "> hello , my name is one and have do not a (little|man|very|few|long)\n",
      "> hello , my name is one and have do not been (to|,|a|.|in)\n",
      "> hello , my name is one and have do not find (the|a|to|him|his)\n",
      "> hello , my name is one and have do you (have|know|had|are|be)\n",
      "> hello , my name is one and have do you have (a|been|to|not|you)\n",
      "> hello , my name is one and have do you know (the|to|that|.|,)\n",
      "> hello , my name is one and have do you had (been|not|a|to|no)\n",
      "> hello , my name is one and have do you are (a|to|the|that|in)\n",
      "> hello , my name is one and have do you be (to|a|not|in|the)\n",
      "> hello , my name is one and have do that (he|i|the|you|it)\n",
      "> hello , my name is one and have do that he (had|was|could|were|would)\n",
      "> hello , my name is one and have do that i (had|have|am|was|could)\n",
      "> hello , my name is one and have do that the (man|other|young|two|first)\n",
      "> hello , my name is one and have do that you (are|have|had|were|do)\n",
      "> hello , my name is one and have do that it (was|is|had|were|would)\n",
      "> hello , my name is one and have not (a|not|been|to|have)\n",
      "> hello , my name is one and have not a (little|few|small|very|man)\n",
      "> hello , my name is one and have not a little (of|or|man|which|and)\n",
      "> hello , my name is one and have not a few (.|of|hours|,|which)\n",
      "> hello , my name is one and have not a small (of|.|,|in|and)\n",
      "> hello , my name is one and have not a very (of|man|which|,|time)\n",
      "> hello , my name is one and have not a man (of|and|,|.|in)\n",
      "> hello , my name is one and have not not (a|the|to|no|not)\n",
      "> hello , my name is one and have not not a (little|man|few|very|small)\n",
      "> hello , my name is one and have not not the (other|two|door|time|man)\n",
      "> hello , my name is one and have not not to (be|have|the|do|tell)\n",
      "> hello , my name is one and have not not no (,|.|of|that|a)\n",
      "> hello , my name is one and have not not not (a|the|to|that|his)\n",
      "> hello , my name is one and have not been (to|a|,|in|.)\n",
      "> hello , my name is one and have not been to (be|have|get|say|do)\n",
      "> hello , my name is one and have not been a (few|little|man|small|great)\n",
      "> hello , my name is one and have not been , (and|but|he|said|i)\n",
      "> hello , my name is one and have not been in (the|a|his|an|my)\n",
      "> hello , my name is one and have not been . (i|he|you|it|there)\n",
      "> hello , my name is one and have not to (be|have|get|find|do)\n",
      "> hello , my name is one and have not to be (a|not|to|no|the)\n",
      "> hello , my name is one and have not to have (the|a|no|you|his)\n",
      "> hello , my name is one and have not to get (to|the|,|.|a)\n",
      "> hello , my name is one and have not to find (the|a|his|him|it)\n",
      "> hello , my name is one and have not to do (to|,|.|in|that)\n",
      "> hello , my name is one and have not have (have|been|not|a|see)\n",
      "> hello , my name is one and have not have have (a|the|you|not|no)\n",
      "> hello , my name is one and have not have been (a|to|the|that|you)\n",
      "> hello , my name is one and have not have not (been|not|to|a|have)\n",
      "> hello , my name is one and have not have a (little|very|few|small|long)\n",
      "> hello , my name is one and have not have see (to|that|the|a|in)\n",
      "> hello , my name is one and have take (the|a|to|him|you)\n",
      "> hello , my name is one and have take the (door|other|room|house|same)\n",
      "> hello , my name is one and have take the door (,|.|of|and|which)\n",
      "> hello , my name is one and have take the other (of|,|.|and|in)\n",
      "> hello , my name is one and have take the room (.|,|of|and|in)\n",
      "> hello , my name is one and have take the house (,|.|to|in|of)\n",
      "> hello , my name is one and have take the same (of|man|,|little|which)\n",
      "> hello , my name is one and have take a (few|little|way|small|great)\n",
      "> hello , my name is one and have take a few (of|.|,|side|which)\n",
      "> hello , my name is one and have take a little (of|man|,|girl|thing)\n",
      "> hello , my name is one and have take a way (,|of|.|to|in)\n",
      "> hello , my name is one and have take a small (of|.|,|in|and)\n",
      "> hello , my name is one and have take a great (of|,|.|way|which)\n",
      "> hello , my name is one and have take to (be|find|the|have|get)\n",
      "> hello , my name is one and have take to be (to|a|,|in|not)\n",
      "> hello , my name is one and have take to find (the|a|his|to|him)\n",
      "> hello , my name is one and have take to the (two|other|man|door|house)\n",
      "> hello , my name is one and have take to have (the|a|you|his|my)\n",
      "> hello , my name is one and have take to get (.|,|to|in|that)\n",
      "> hello , my name is one and have take him (to|.|,|in|that)\n",
      "> hello , my name is one and have take him to (the|be|his|a|me)\n",
      "> hello , my name is one and have take him . (i|he|it|you|the)\n",
      "> hello , my name is one and have take him , (and|he|i|but|the)\n",
      "> hello , my name is one and have take him in (the|a|his|my|an)\n",
      "> hello , my name is one and have take him that (he|i|the|they|it)\n",
      "> hello , my name is one and have take you (to|,|a|have|had)\n",
      "> hello , my name is one and have take you to (be|do|the|have|take)\n",
      "> hello , my name is one and have take you , (he|and|said|i|but)\n",
      "> hello , my name is one and have take you a (little|very|small|few|man)\n",
      "> hello , my name is one and have take you have (been|a|to|not|,)\n",
      "> hello , my name is one and have take you had (been|not|to|a|no)\n",
      "> hello , my name is one and you (have|are|had|do|were)\n",
      "> hello , my name is one and you have (been|have|not|a|you)\n",
      "> hello , my name is one and you have been (a|to|the|that|in)\n",
      "> hello , my name is one and you have been a (few|little|small|way|man)\n",
      "> hello , my name is one and you have been to (be|get|find|have|draw)\n",
      "> hello , my name is one and you have been the (door|other|man|whole|little)\n",
      "> hello , my name is one and you have been that (the|it|a|you|his)\n",
      "> hello , my name is one and you have been in (the|a|his|my|him)\n",
      "> hello , my name is one and you have have (a|not|no|you|the)\n",
      "> hello , my name is one and you have have a (little|few|very|small|long)\n",
      "> hello , my name is one and you have have not (been|not|be|have|a)\n",
      "> hello , my name is one and you have have no (hand|eyes|way|time|idea)\n",
      "> hello , my name is one and you have have you (to|a|,|been|not)\n",
      "> hello , my name is one and you have have the (man|door|whole|little|other)\n",
      "> hello , my name is one and you have not (been|have|be|not|come)\n",
      "> hello , my name is one and you have not been (to|a|,|in|.)\n",
      "> hello , my name is one and you have not have (have|been|not|a|see)\n",
      "> hello , my name is one and you have not be (a|no|not|to|the)\n",
      "> hello , my name is one and you have not not (a|not|no|to|the)\n",
      "> hello , my name is one and you have not come (to|in|,|that|.)\n",
      "> hello , my name is one and you have a (little|very|few|small|long)\n",
      "> hello , my name is one and you have a little (of|man|thing|girl|garden)\n",
      "> hello , my name is one and you have a very (of|man|which|,|time)\n",
      "> hello , my name is one and you have a few (of|time|.|side|idea)\n",
      "> hello , my name is one and you have a small (of|,|.|and|or)\n",
      "> hello , my name is one and you have a long (of|,|.|way|to)\n",
      "> hello , my name is one and you have you (been|to|,|t|have)\n",
      "> hello , my name is one and you have you been (to|a|.|,|the)\n",
      "> hello , my name is one and you have you to (be|do|tell|have|find)\n",
      "> hello , my name is one and you have you , (and|he|i|said|but)\n",
      "> hello , my name is one and you have you t (the|a|me|you|be)\n",
      "> hello , my name is one and you have you have (a|been|the|to|you)\n",
      "> hello , my name is one and you are (a|the|to|no|that)\n",
      "> hello , my name is one and you are a (little|few|small|way|very)\n",
      "> hello , my name is one and you are a little (of|,|and|or|which)\n",
      "> hello , my name is one and you are a few (.|hours|of|,|which)\n",
      "> hello , my name is one and you are a small (of|.|,|in|and)\n",
      "> hello , my name is one and you are a way (of|,|.|in|to)\n",
      "> hello , my name is one and you are a very (of|,|which|man|or)\n",
      "> hello , my name is one and you are the (man|door|room|other|case)\n",
      "> hello , my name is one and you are the man (of|,|.|and|in)\n",
      "> hello , my name is one and you are the door (,|.|of|which|and)\n",
      "> hello , my name is one and you are the room (.|,|and|of|in)\n",
      "> hello , my name is one and you are the other (of|,|.|and|which)\n",
      "> hello , my name is one and you are the case (of|,|.|and|in)\n",
      "> hello , my name is one and you are to (be|the|do|find|tell)\n",
      "> hello , my name is one and you are to be (a|to|not|have|in)\n",
      "> hello , my name is one and you are to the (two|other|very|last|door)\n",
      "> hello , my name is one and you are to do (to|.|,|in|that)\n",
      "> hello , my name is one and you are to find (the|a|his|to|him)\n",
      "> hello , my name is one and you are to tell (the|him|a|me|his)\n",
      "> hello , my name is one and you are no (eyes|hand|doubt|idea|face)\n",
      "> hello , my name is one and you are no eyes (.|,|to|and|in)\n",
      "> hello , my name is one and you are no hand (,|to|.|than|of)\n",
      "> hello , my name is one and you are no doubt (to|,|.|the|that)\n",
      "> hello , my name is one and you are no idea (that|to|,|in|the)\n",
      "> hello , my name is one and you are no face (,|and|.|in|to)\n",
      "> hello , my name is one and you are that (he|the|i|,|of)\n",
      "> hello , my name is one and you are that he (had|was|would|spoke|were)\n",
      "> hello , my name is one and you are that the (man|other|case|girl|young)\n",
      "> hello , my name is one and you are that i (had|was|have|could|am)\n",
      "> hello , my name is one and you are that , (and|he|but|said|i)\n",
      "> hello , my name is one and you are that of (the|a|his|it|them)\n",
      "> hello , my name is one and you had (been|not|a|no|you)\n",
      "> hello , my name is one and you had been (to|a|in|the|that)\n",
      "> hello , my name is one and you had been to (be|get|find|have|know)\n",
      "> hello , my name is one and you had been a (few|little|great|small|man)\n",
      "> hello , my name is one and you had been in (the|a|his|my|him)\n",
      "> hello , my name is one and you had been the (man|door|little|great|other)\n",
      "> hello , my name is one and you had been that (the|to|he|it|you)\n",
      "> hello , my name is one and you had not (be|been|have|find|come)\n",
      "> hello , my name is one and you had not be (to|a|in|,|for)\n",
      "> hello , my name is one and you had not been (to|a|.|,|that)\n",
      "> hello , my name is one and you had not have (have|been|be|not|a)\n",
      "> hello , my name is one and you had not find (to|a|the|.|him)\n",
      "> hello , my name is one and you had not come (to|that|in|.|,)\n",
      "> hello , my name is one and you had a (very|little|few|small|no)\n",
      "> hello , my name is one and you had a very (of|man|time|,|case)\n",
      "> hello , my name is one and you had a little (of|man|thing|girl|road)\n",
      "> hello , my name is one and you had a few (.|hours|,|side|white)\n",
      "> hello , my name is one and you had a small (of|.|,|or|and)\n",
      "> hello , my name is one and you had a no (of|side|that|.|time)\n",
      "> hello , my name is one and you had no (hand|way|idea|eyes|time)\n",
      "> hello , my name is one and you had no hand (,|to|.|of|and)\n",
      "> hello , my name is one and you had no way (to|,|.|in|?)\n",
      "> hello , my name is one and you had no idea (to|in|that|and|,)\n",
      "> hello , my name is one and you had no eyes (.|,|and|to|of)\n",
      "> hello , my name is one and you had no time (to|,|.|in|and)\n",
      "> hello , my name is one and you had you (been|t|to|have|not)\n",
      "> hello , my name is one and you had you been (to|a|.|the|in)\n",
      "> hello , my name is one and you had you t (be|a|the|not|been)\n",
      "> hello , my name is one and you had you to (be|do|tell|find|have)\n",
      "> hello , my name is one and you had you have (a|been|to|the|you)\n",
      "> hello , my name is one and you had you not (been|to|be|t|come)\n",
      "> hello , my name is one and you do (to|that|a|,|you)\n",
      "> hello , my name is one and you do to (be|have|do|find|tell)\n",
      "> hello , my name is one and you do to be (a|to|not|the|you)\n",
      "> hello , my name is one and you do to have (the|a|you|no|my)\n",
      "> hello , my name is one and you do to do (to|,|.|that|in)\n",
      "> hello , my name is one and you do to find (the|a|his|it|him)\n",
      "> hello , my name is one and you do to tell (the|him|a|my|you)\n",
      "> hello , my name is one and you do that (he|,|i|is|to)\n",
      "> hello , my name is one and you do that he (had|was|could|were|would)\n",
      "> hello , my name is one and you do that , (he|and|i|the|but)\n",
      "> hello , my name is one and you do that i (had|have|am|was|could)\n",
      "> hello , my name is one and you do that is (a|the|not|to|you)\n",
      "> hello , my name is one and you do that to (be|do|the|have|see)\n",
      "> hello , my name is one and you do a (little|very|man|few|small)\n",
      "> hello , my name is one and you do a little (of|man|thing|girl|matter)\n",
      "> hello , my name is one and you do a very (man|of|thing|little|case)\n",
      "> hello , my name is one and you do a man (of|was|had|s|and)\n",
      "> hello , my name is one and you do a few (of|,|.|time|man)\n",
      "> hello , my name is one and you do a small (of|,|.|or|and)\n",
      "> hello , my name is one and you do , (and|he|i|said|but)\n",
      "> hello , my name is one and you do , and (i|the|he|we|you)\n",
      "> hello , my name is one and you do , he (said|answered|was|had|cried)\n",
      "> hello , my name is one and you do , i (have|am|said|had|ll)\n",
      "> hello , my name is one and you do , said (the|i|a|it|he)\n",
      "> hello , my name is one and you do , but (the|he|i|a|it)\n",
      "> hello , my name is one and you do you (have|had|are|were|ll)\n",
      "> hello , my name is one and you do you have (a|been|not|you|to)\n",
      "> hello , my name is one and you do you had (been|not|a|no|to)\n",
      "> hello , my name is one and you do you are (a|to|the|that|you)\n",
      "> hello , my name is one and you do you were (,|of|to|a|had)\n",
      "> hello , my name is one and you do you ll (been|be|t|have|to)\n",
      "> hello , my name is one and you were (a|,|the|to|of)\n",
      "> hello , my name is one and you were a (little|few|small|man|way)\n",
      "> hello , my name is one and you were a little (of|and|,|which|or)\n",
      "> hello , my name is one and you were a few (of|.|hours|,|which)\n",
      "> hello , my name is one and you were a small (of|.|,|and|in)\n",
      "> hello , my name is one and you were a man (of|.|and|,|in)\n",
      "> hello , my name is one and you were a way (of|,|in|to|and)\n",
      "> hello , my name is one and you were , (and|he|i|but|the)\n",
      "> hello , my name is one and you were , and (i|the|he|we|then)\n",
      "> hello , my name is one and you were , he (said|answered|cried|continued|had)\n",
      "> hello , my name is one and you were , i (said|cried|answered|asked|am)\n",
      "> hello , my name is one and you were , but (he|the|i|a|we)\n",
      "> hello , my name is one and you were , the (man|other|door|little|young)\n",
      "> hello , my name is one and you were the (door|room|man|other|house)\n",
      "> hello , my name is one and you were the door (,|.|of|which|and)\n",
      "> hello , my name is one and you were the room (.|,|of|and|in)\n",
      "> hello , my name is one and you were the man (of|and|,|.|in)\n",
      "> hello , my name is one and you were the other (of|,|which|.|and)\n",
      "> hello , my name is one and you were the house (,|.|to|of|in)\n",
      "> hello , my name is one and you were to (be|the|do|have|take)\n",
      "> hello , my name is one and you were to be (a|not|have|to|be)\n",
      "> hello , my name is one and you were to the (two|other|door|house|last)\n",
      "> hello , my name is one and you were to do (to|,|.|in|at)\n",
      "> hello , my name is one and you were to have (the|a|no|you|his)\n",
      "> hello , my name is one and you were to take (to|,|.|a|him)\n",
      "> hello , my name is one and you were of (the|his|a|my|it)\n",
      "> hello , my name is one and you were of the (room|man|door|wall|house)\n",
      "> hello , my name is one and you were of his (own|hand|eyes|hands|heart)\n",
      "> hello , my name is one and you were of a (few|little|man|great|way)\n",
      "> hello , my name is one and you were of my (companion|own|eyes|mind|face)\n",
      "> hello , my name is one and you were of it (.|,|to|in|that)\n",
      "> hello , my name is one and see (the|that|a|to|it)\n",
      "> hello , my name is one and see the (man|door|whole|little|great)\n",
      "> hello , my name is one and see the man (of|,|.|and|s)\n",
      "> hello , my name is one and see the man of (the|his|a|my|it)\n",
      "> hello , my name is one and see the man , (and|he|but|i|for)\n",
      "> hello , my name is one and see the man . (i|the|he|it|you)\n",
      "> hello , my name is one and see the man and (the|a|have|of|,)\n",
      "> hello , my name is one and see the man s (own|private|hand|way|daughter)\n",
      "> hello , my name is one and see the door (,|.|of|which|and)\n",
      "> hello , my name is one and see the door , (and|but|he|for|with)\n",
      "> hello , my name is one and see the door . (i|he|there|it|the)\n",
      "> hello , my name is one and see the door of (the|his|a|him|it)\n",
      "> hello , my name is one and see the door which (was|had|i|he|is)\n",
      "> hello , my name is one and see the door and (,|the|in|of|had)\n",
      "> hello , my name is one and see the whole (of|thing|girl|world|man)\n",
      "> hello , my name is one and see the whole of (the|his|him|me|a)\n",
      "> hello , my name is one and see the whole thing (,|of|was|.|in)\n",
      "> hello , my name is one and see the whole girl (,|.|to|?|in)\n",
      "> hello , my name is one and see the whole world (.|,|to|in|of)\n",
      "> hello , my name is one and see the whole man (.|,|of|s|in)\n",
      "> hello , my name is one and see the little (of|girl|man|thing|case)\n",
      "> hello , my name is one and see the little of (the|his|a|which|it)\n",
      "> hello , my name is one and see the little girl (of|,|and|.|in)\n",
      "> hello , my name is one and see the little man (of|,|.|s|had)\n",
      "> hello , my name is one and see the little thing (the|,|in|.|of)\n",
      "> hello , my name is one and see the little case (of|,|and|.|in)\n",
      "> hello , my name is one and see the great (of|,|.|which|in)\n",
      "> hello , my name is one and see the great of (the|a|his|it|my)\n",
      "> hello , my name is one and see the great , (and|which|but|he|i)\n",
      "> hello , my name is one and see the great . (i|there|have|he|the)\n",
      "> hello , my name is one and see the great which (was|had|is|i|he)\n",
      "> hello , my name is one and see the great in (a|the|life|which|an)\n",
      "> hello , my name is one and see that (i|he|you|the|they)\n",
      "> hello , my name is one and see that i (had|have|am|was|could)\n",
      "> hello , my name is one and see that i had (been|not|no|a|you)\n",
      "> hello , my name is one and see that i have (a|not|you|been|have)\n",
      "> hello , my name is one and see that i am (a|not|no|you|the)\n",
      "> hello , my name is one and see that i was (a|not|to|been|no)\n",
      "> hello , my name is one and see that i could (have|not|see|take|be)\n",
      "> hello , my name is one and see that he (had|was|would|could|were)\n",
      "> hello , my name is one and see that he had (been|not|a|have|no)\n",
      "> hello , my name is one and see that he was (a|to|not|no|the)\n",
      "> hello , my name is one and see that he would (have|be|not|no|been)\n",
      "> hello , my name is one and see that he could (have|see|not|be|do)\n",
      "> hello , my name is one and see that he were (the|a|,|to|in)\n",
      "> hello , my name is one and see that you (have|had|are|do|ll)\n",
      "> hello , my name is one and see that you have (not|been|have|a|you)\n",
      "> hello , my name is one and see that you had (been|not|have|already|come)\n",
      "> hello , my name is one and see that you are (a|the|no|to|not)\n",
      "> hello , my name is one and see that you do (to|,|a|that|not)\n",
      "> hello , my name is one and see that you ll (been|to|have|had|not)\n",
      "> hello , my name is one and see that the (man|other|whole|case|little)\n",
      "> hello , my name is one and see that the man (was|s|is|had|of)\n",
      "> hello , my name is one and see that the other (was|of|man|is|asked)\n",
      "> hello , my name is one and see that the whole (of|man|girl|thing|case)\n",
      "> hello , my name is one and see that the case (was|of|is|man|asked)\n",
      "> hello , my name is one and see that the little (man|girl|thing|of|day)\n",
      "> hello , my name is one and see that they (had|was|were|are|must)\n",
      "> hello , my name is one and see that they had (been|not|a|have|to)\n",
      "> hello , my name is one and see that they was (a|not|no|the|to)\n",
      "> hello , my name is one and see that they were (a|the|to|,|that)\n",
      "> hello , my name is one and see that they are (a|the|not|no|you)\n",
      "> hello , my name is one and see that they must (have|be|t|had|come)\n",
      "> hello , my name is one and see a (few|little|great|man|small)\n",
      "> hello , my name is one and see a few (of|.|,|time|side)\n",
      "> hello , my name is one and see a few of (the|his|a|them|him)\n",
      "> hello , my name is one and see a few . (i|the|it|there|he)\n",
      "> hello , my name is one and see a few , (and|which|but|or|of)\n",
      "> hello , my name is one and see a few time (.|,|of|in|and)\n",
      "> hello , my name is one and see a few side (,|.|the|to|which)\n",
      "> hello , my name is one and see a little (of|man|thing|girl|garden)\n",
      "> hello , my name is one and see a little of (the|a|which|his|one)\n",
      "> hello , my name is one and see a little man (of|,|.|and|s)\n",
      "> hello , my name is one and see a little thing (,|.|the|to|in)\n",
      "> hello , my name is one and see a little girl (of|,|.|and|in)\n",
      "> hello , my name is one and see a little garden (,|of|.|in|and)\n",
      "> hello , my name is one and see a great (way|room|idea|time|of)\n",
      "> hello , my name is one and see a great way (,|.|of|to|in)\n",
      "> hello , my name is one and see a great room (.|,|of|in|and)\n",
      "> hello , my name is one and see a great idea (of|,|and|which|in)\n",
      "> hello , my name is one and see a great time (,|of|.|in|and)\n",
      "> hello , my name is one and see a great of (the|a|which|one|two)\n",
      "> hello , my name is one and see a man (of|,|.|and|in)\n",
      "> hello , my name is one and see a man of (the|a|his|an|my)\n",
      "> hello , my name is one and see a man , (and|but|said|he|i)\n",
      "> hello , my name is one and see a man . (i|the|it|you|he)\n",
      "> hello , my name is one and see a man and (have|the|a|had|which)\n",
      "> hello , my name is one and see a man in (the|his|a|him|my)\n",
      "> hello , my name is one and see a small (of|,|.|or|which)\n",
      "> hello , my name is one and see a small of (the|his|a|them|which)\n",
      "> hello , my name is one and see a small , (and|but|which|he|for)\n",
      "> hello , my name is one and see a small . (i|the|there|he|it)\n",
      "> hello , my name is one and see a small or (of|which|,|a|that)\n",
      "> hello , my name is one and see a small which (was|had|is|of|he)\n",
      "> hello , my name is one and see to (be|the|have|do|find)\n",
      "> hello , my name is one and see to be (to|,|a|have|of)\n",
      "> hello , my name is one and see to be to (be|find|tell|have|keep)\n",
      "> hello , my name is one and see to be , (and|said|but|he|i)\n",
      "> hello , my name is one and see to be a (man|little|few|way|small)\n",
      "> hello , my name is one and see to be have (a|to|been|,|the)\n",
      "> hello , my name is one and see to be of (the|a|his|it|an)\n",
      "> hello , my name is one and see to the (two|other|door|man|house)\n",
      "> hello , my name is one and see to the two (of|,|.|to|which)\n",
      "> hello , my name is one and see to the other (of|,|.|which|that)\n",
      "> hello , my name is one and see to the door (.|,|of|which|and)\n",
      "> hello , my name is one and see to the man (of|,|.|and|in)\n",
      "> hello , my name is one and see to the house (,|.|of|to|in)\n",
      "> hello , my name is one and see to have (a|the|no|you|not)\n",
      "> hello , my name is one and see to have a (little|few|very|small|way)\n",
      "> hello , my name is one and see to have the (other|door|whole|time|two)\n",
      "> hello , my name is one and see to have no (doubt|eyes|hand|life|idea)\n",
      "> hello , my name is one and see to have you (to|a|,|that|.)\n",
      "> hello , my name is one and see to have not (to|a|not|been|have)\n",
      "> hello , my name is one and see to do (,|to|.|in|?)\n",
      "> hello , my name is one and see to do , (and|said|he|i|however)\n",
      "> hello , my name is one and see to do to (be|have|find|get|do)\n",
      "> hello , my name is one and see to do . (i|it|he|you|the)\n",
      "> hello , my name is one and see to do in (the|a|his|my|him)\n",
      "> hello , my name is one and see to do ? (i|he|the|no|we)\n",
      "> hello , my name is one and see to find (the|a|his|to|him)\n",
      "> hello , my name is one and see to find the (room|door|whole|man|case)\n",
      "> hello , my name is one and see to find a (man|few|little|great|dull)\n",
      "> hello , my name is one and see to find his (hand|heart|daughter|seat|hands)\n",
      "> hello , my name is one and see to find to (be|the|last|a|his)\n",
      "> hello , my name is one and see to find him (.|,|to|and|in)\n",
      "> hello , my name is one and see it (was|is|,|.|s)\n",
      "> hello , my name is one and see it was (to|a|in|and|so)\n",
      "> hello , my name is one and see it was to (be|find|tell|do|me)\n",
      "> hello , my name is one and see it was a (little|few|small|man|very)\n",
      "> hello , my name is one and see it was in (the|his|a|my|him)\n",
      "> hello , my name is one and see it was and (the|,|to|a|in)\n",
      "> hello , my name is one and see it was so (a|that|the|to|,)\n",
      "> hello , my name is one and see it is (to|a|not|in|,)\n",
      "> hello , my name is one and see it is to (be|the|do|have|take)\n",
      "> hello , my name is one and see it is a (man|little|few|small|great)\n",
      "> hello , my name is one and see it is not (to|a|the|him|that)\n",
      "> hello , my name is one and see it is in (the|his|a|him|my)\n",
      "> hello , my name is one and see it is , (and|he|i|but|the)\n",
      "> hello , my name is one and see it , (and|he|said|i|but)\n",
      "> hello , my name is one and see it , and (i|the|then|he|we)\n",
      "> hello , my name is one and see it , he (said|answered|cried|continued|had)\n",
      "> hello , my name is one and see it , said (the|i|a|it|he)\n",
      "> hello , my name is one and see it , i (said|answered|cried|am|asked)\n",
      "> hello , my name is one and see it , but (he|i|the|a|we)\n",
      "> hello , my name is one and see it . (i|he|there|you|the)\n",
      "> hello , my name is one and see it . i (have|had|don|am|was)\n",
      "> hello , my name is one and see it . he (was|had|is|has|would)\n",
      "> hello , my name is one and see it . there (was|is|were|are|s)\n",
      "> hello , my name is one and see it . you (have|are|had|were|was)\n",
      "> hello , my name is one and see it . the (man|young|other|latter|two)\n",
      "> hello , my name is one and see it s (,|.|to|be|a)\n",
      "> hello , my name is one and see it s , (and|he|i|said|but)\n",
      "> hello , my name is one and see it s . (i|you|he|it|there)\n",
      "> hello , my name is one and see it s to (be|do|have|find|the)\n",
      "> hello , my name is one and see it s be (to|,|a|be|of)\n",
      "> hello , my name is one and see it s a (little|man|small|very|few)\n",
      "('Hello, my name is one and the other of the room', 1.0)\n"
     ]
    }
   ],
   "source": [
    "def gen_get_candidates(prompt:str, top:int):\n",
    "\n",
    "    words = TOKENIZER(prompt)\n",
    "    context = [vocabulary[word] for word in words]\n",
    "    context = torch.tensor(context).to(device)\n",
    "\n",
    "    output = gen_model(context.unsqueeze(0))\n",
    "    candidates = torch.topk(output, top).indices.squeeze()\n",
    "\n",
    "    # Make confidence scores human readable\n",
    "    output_min = output.amin(dim=1)\n",
    "    output_max = output.amax(dim=1)\n",
    "    confidence = (output - output_min) / (output_max - output_min)\n",
    "\n",
    "    candidates = { vocabulary.lookup_token(x): confidence[0, x].item() for x in candidates }\n",
    "\n",
    "    print(f\"> {\" \".join(words)} ({\"|\".join(candidates.keys())})\")\n",
    "\n",
    "    return candidates\n",
    "\n",
    "gen_get_candidates(\"Once upon a time, there was\", top=5)\n",
    "\n",
    "def gen_beam_search(prompt:str, beam_width:int, beam_depth:int | None = None, branch_confidence:float = 1.0):\n",
    "\n",
    "    if beam_depth is None:\n",
    "        beam_depth = beam_width\n",
    "\n",
    "    if beam_depth < 1:\n",
    "        return prompt, branch_confidence\n",
    "\n",
    "    candidates = gen_get_candidates(prompt, beam_width)\n",
    "\n",
    "    prompt_candidates = [ gen_beam_search(prompt + ' ' + word, beam_width, beam_depth - 1, branch_confidence * confidence) for word, confidence in candidates.items() ]\n",
    "\n",
    "    return max(prompt_candidates, key=lambda x: x[1])\n",
    "    \n",
    "print(gen_beam_search(\"Hello, my name is one and\", 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
